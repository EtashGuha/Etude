<tei>
	<teiHeader>
	<fileDesc xml:id="246"/>
	</teiHeader>
<text xml:lang="en">
		<front>
		<docTitle>
			<titlePart type="main">Structured Markov Chain Monte Carlo <lb/></titlePart>
		</docTitle>
		<byline><docAuthor>by Daniel J. SARGENT 1 , James S. HODGES 2 , and Bradley P. CARLIN 2 <lb/></docAuthor></byline>
		<byline><affiliation>1 Section of Biostatistics, Mayo Clinic <lb/></affiliation></byline>
		<byline><affiliation>2 Division of Biostatistics, School of Public Health, University of Minnesota <lb/></affiliation></byline>
		<date>January 9, 1998 <lb/></date>
		<div type="abstract">Abstract <lb/>In this paper we introduce a general method for Bayesian computing in richly-parameterized <lb/>models, Structured Markov Chain Monte Carlo (SMCMC), that is based on a blocked hybrid of the <lb/>Gibbs sampling and Metropolis-Hastings algorithms. SMCMC speeds algorithm convergence by <lb/>using the structure that is present in the problem to suggest an appropriate Metropolis-Hastings <lb/>candidate distribution. While the approach is easiest to describe for hierarchical normal linear <lb/>models, we show its extension to both non-normal and nonlinear cases to be straightforward. <lb/>After describing the method in detail we compare its performance (both in terms of runtime and <lb/>autocorrelation in the samples produced) to several other existing methods, including the traditional <lb/>single-site updating Gibbs sampler available in the popular BUGS software package. Our results <lb/>suggest significant improvements in convergence for many problems using SMCMC, as well as <lb/>broad applicability of the method, including previously intractable hierarchical nonlinear model <lb/>settings. <lb/></div>
		<keywords>KEY WORDS: Blocking; Convergence acceleration; Gibbs sampling; Hierarchical model; Metropolis-Hastings algorithm. <lb/></keywords>
		<pb/>
		</front>
</text>
</tei>