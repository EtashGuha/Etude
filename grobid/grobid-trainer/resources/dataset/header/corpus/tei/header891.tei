<tei>
	<teiHeader>
	<fileDesc xml:id="unknown"/>
	</teiHeader>
<text xml:lang="en">
		<front>
		<docTitle>
			<titlePart type="main">Update rules for parameter estimation in Bayesian networks <lb/></titlePart>
		</docTitle>
		<byline><docAuthor>Eric Bauer <lb/></docAuthor></byline>
		<byline><affiliation>Stanford University <lb/></affiliation></byline>
		<email>ebauer@cs.stanford.edu <lb/></email>
		<byline><docAuthor>Daphne Koller <lb/></docAuthor></byline>
		<byline><affiliation>Stanford University <lb/></affiliation></byline>
		<email>koller@cs.stanford.edu <lb/></email>
		<byline><docAuthor>Yoram Singer <lb/></docAuthor></byline>
		<byline><affiliation>AT&amp;T Labs <lb/></affiliation></byline>
		<email>singer@research.att.com <lb/></email>
		<div type="abstract">Abstract <lb/>This paper re-examines the problem of parameter estimation in Bayesian networks with missing values and <lb/>hidden variables from the perspective of recent work in <lb/>on-line learning [12]. We provide a unified framework <lb/>for parameter estimation that encompasses both on-line <lb/>learning, where the model is continuously adapted to new <lb/>data cases as they arrive, and the more traditional batch <lb/>learning, where a pre-accumulated set of samples is used <lb/>in a one-time model selection process. In the batch case, <lb/>our framework encompasses both the gradient projection <lb/>algorithm [2, 3] and the EM algorithm [14] for Bayesian <lb/>networks. The framework also leads to new on-line and <lb/>batch parameter update schemes, including a parameterized version of EM. We provide both empirical and theoretical results indicating that parameterized EM allows <lb/>faster convergence to the maximum likelihood parame <lb/>ters than does standard EM. <lb/></div>
		<div type="intro">1 Introduction</div>
		</front>
</text>
</tei>