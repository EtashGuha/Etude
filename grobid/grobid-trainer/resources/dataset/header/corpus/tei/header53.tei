<tei>
	<teiHeader>
	<fileDesc xml:id="54"/>
	</teiHeader>
<text xml:lang="en">
		<front>
		<docTitle>
			<titlePart type="main">Temporal Difference Learning of <lb/>Position Evaluation in the Game of Go <lb/></titlePart>
		</docTitle>
		<byline><docAuthor>Nicol N. Schraudolph Peter Dayan Terrence J. Sejnowski <lb/></docAuthor></byline>
		<email>schraudo@salk.edu </email> <email>dayan@salk.edu </email> <email>terry@salk.edu <lb/></email>
		<byline><affiliation>Computational Neurobiology Laboratory <lb/> The Salk Institute for Biological Studies <lb/></affiliation></byline>
		<address>San Diego, CA 92186-5800 <lb/></address>
		<div type="abstract">Abstract <lb/>The game of Go has a high branching factor that defeats the tree <lb/>search approach used in computer chess, and long-range spa-tiotemporal interactions that make position evaluation extremely <lb/>difficult. Development of conventional Go programs is hampered <lb/>by their knowledge-intensive nature. We demonstrate a viable <lb/>alternative by training networks to evaluate Go positions via temporal difference (TD) learning. <lb/>Our approach is based on network architectures that reflect the <lb/>spatial organization of both input and reinforcement signals on <lb/>the Go board, and training protocols that provide exposure to <lb/>competent (though unlabelled) play. These techniques yield far <lb/>better performance than undifferentiated networks trained by self-play alone. A network with less than 500 weights learned within <lb/>3,000 games of 9x9 Go a position evaluation function that enables <lb/>a primitive one-ply search to defeat a commercial Go program at <lb/>a low playing level. <lb/></div>
		<div type="intro">1 INTRODUCTION</div>
		</front>
</text>
</tei>