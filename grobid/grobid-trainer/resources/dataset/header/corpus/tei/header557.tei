<tei>
	<teiHeader>
	<fileDesc xml:id="560"/>
	</teiHeader>
<text xml:lang="en">
		<front>
		<docTitle>
			<titlePart type="main">Metamorphosis Networks: <lb/>An Alternative to Constructive Methods <lb/></titlePart>
		</docTitle>
		<byline><docAuthor>Brian V. Bonnlander Michael C. Mozer <lb/></docAuthor></byline>
		<byline><affiliation>Department of Computer Science &amp; <lb/>Institute of Cognitive Science <lb/>University of Colorado <lb/></affiliation></byline>
		<address>Boulder, CO 80309-0430 <lb/></address>
		<div type="abstract">Abstract <lb/>Given a set of training examples, determining the appropriate number of free parameters is a challenging problem. Constructive <lb/>learning algorithms attempt to solve this problem automatically by <lb/>adding hidden units, and therefore free parameters, during learning. We explore an alternative class of algorithms|called metamorphosis algorithms|in which the number of units is fixed, but <lb/>the number of free parameters gradually increases during learning. <lb/>The architecture we investigate is composed of RBF units on a lattice, which imposes flexible constraints on the parameters of the <lb/>network. Virtues of this approach include variable subset selection, robust parameter selection, multiresolution processing, and <lb/>interpolation of sparse training data. <lb/></div>
		<div type="intro">1 INTRODUCTION</div>
		</front>
</text>
</tei>