<tei>
	<teiHeader>
	<fileDesc xml:id="611"/>
	</teiHeader>
<text xml:lang="en">
		<front>
		<note type="reference">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 17, NO. 6, JUNE 1995 599 <lb/></note>
		<docTitle>
			<titlePart type="main">Best-Case Results for Nearest Neighbor <lb/>Learning <lb/></titlePart>
		</docTitle>
		<byline><docAuthor>Steven Salzberg, Arthur Delcher, David Heath, and Simon Kasif <lb/></docAuthor></byline>
		<div type="abstract">Abstract| In this paper we propose a theoretical model <lb/>for analysis of classification methods, in which the teacher <lb/>knows the classification algorithm and chooses examples in <lb/>the best way possible. We apply this model using the nearest-neighbor learning algorithm, and develop upper and lower <lb/>bounds on sample complexity for several different concept <lb/>classes. For some concept classes, the sample complexity <lb/>turns out to be exponential even using this best-case model, <lb/>which implies that the concept class is inherently difficult <lb/>for the nearest-neighbor algorithm. We identify several geometric properties that make learning certain concepts relatively easy. Finally we discuss the relation of our work <lb/>to helpful teacher models, its application to decision-tree <lb/>learning algorithms, and some of its implications for current experimental work. <lb/></div>
		<keywords>Keywords| machine learning, nearest-neighbor, geometric <lb/>concepts. <lb/></keywords>
		<div type="intro">I. Introduction</div>
		</front>
</text>
</tei>