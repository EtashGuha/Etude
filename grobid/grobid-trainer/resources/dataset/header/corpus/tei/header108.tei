<tei>
	<teiHeader>
	<fileDesc xml:id="109"/>
	</teiHeader>
<text xml:lang="en">
		<front>
		<docTitle>
			<titlePart type="main">Q-Learning for Bandit Problems <lb/></titlePart>
		</docTitle>
		<byline><docAuthor>Michael O. Duff <lb/></docAuthor></byline>
		<byline><affiliation>Department of Computer Science <lb/>University of Massachusetts <lb/></affiliation></byline>
		<address>Amherst, MA 01003 <lb/></address>
		<email>duff@cs.umass.edu <lb/></email>
		<div type="abstract">Abstract <lb/>Multi-armed bandits may be viewed as <lb/>decompositionally-structured Markov decision processes (MDP&apos;s) with potentially very-large state sets. A particularly elegant <lb/>methodology for computing optimal policies <lb/>was developed over twenty ago by Gittins <lb/>[Gittins &amp; Jones, 1974]. Gittins&apos; approach <lb/>reduces the problem of finding optimal policies for the original MDP to a sequence of <lb/>low-dimensional stopping problems whose solutions determine the optimal policy through <lb/>the so-called &quot;Gittins indices.&quot; Katehakis <lb/>and Veinott [Katehakis &amp; Veinott, 1987] have <lb/>shown that the Gittins index for a process <lb/>in state i may be interpreted as a particular <lb/>component of the maximum-value function <lb/>associated with the &quot;restart-in-i&quot; process, <lb/>a simple MDP to which standard solution <lb/>methods for computing optimal policies, such <lb/>as successive approximation, apply. This paper explores the problem of learning the Git-tins indices on-line without the aid of a process model; it suggests utilizing process-state-specific Q-learning agents to solve their respective restart-in-state-i subproblems, and <lb/>includes an example in which the online reinforcement learning approach is applied to <lb/>a problem of stochastic scheduling|one instance drawn from a wide class of problems <lb/>that may be formulated as bandit problems. <lb/></div>
		<div type="intro">1 INTRODUCTION</div>
		</front>
</text>
</tei>