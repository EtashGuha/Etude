<tei>
	<teiHeader>
	<fileDesc xml:id="unknown"/>
	</teiHeader>
<text xml:lang="en">
		<front>
		<docTitle>
			<titlePart type="main">Tactile Gestures for Human/Robot Interaction <lb/></titlePart>
		</docTitle>
		<byline><docAuthor>Richard M. Voyles, Jr. Pradeep K. Khosla <lb/></docAuthor></byline>
		<byline><affiliation>Robotics Ph.D. Program <lb/>Dept. of Electrical and Computer Engineering <lb/>Carnegie Mellon University <lb/></affiliation></byline>
		<address>Pittsburgh, PA 15213 <lb/></address>
		<note type="other">7 <lb/></note>
		<div type="abstract">Abstract <lb/>Gesture-Based Programming is a new paradigm to ease <lb/>the burden of programming robots. By tapping in to the <lb/>users wealth of experience with contact transitions, <lb/>compliance, uncertainty and operations sequencing, we <lb/>hope to provide a more intuitive programming environment <lb/>for complex, real-world tasks based on the expressiveness <lb/>of non-verbal communication. A requirement for this to be <lb/>accomplished is the ability to interpret gestures to infer the <lb/>intentions behind them. As a first step toward this goal, this <lb/>paper presents an application of distributed perception for <lb/>inferring a users intentions by observing tactile gestures. <lb/>These gestures consist of sparse, inexact, physical <lb/>nudges applied to the robots end effector for the <lb/>purpose of modifying its trajectory in free space. A set of <lb/>independent agents - each with its own local, fuzzified, <lb/>heuristic model of a particular trajectory parameter - <lb/>observes data from a wrist force/torque sensor to evaluate <lb/>the gestures. The agents then independently determine the <lb/>confidence of their respective findings and distributed <lb/>arbitration resolves the interpretation through voting. <lb/></div>
		<div type="intro">1 Gesture-based programming</div>
		</front>
</text>
</tei>