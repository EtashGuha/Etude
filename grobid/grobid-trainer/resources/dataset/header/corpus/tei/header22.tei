<tei>
	<teiHeader>
	<fileDesc xml:id="23"/>
	</teiHeader>
<text xml:lang="en">
		<front>
		<docTitle>
			<titlePart type="main">Neuronal Goals: Efficient Coding and Coincidence Detection <lb/></titlePart>
		</docTitle>
		<byline><docAuthor>Nathan Intrator <lb/></docAuthor></byline>
		<byline><affiliation>School of Mathematical Sciences <lb/> Tel Aviv University <lb/></affiliation></byline>
		<email>nin@cns.brown.edu <lb/></email>
		<div type="abstract">Abstract| Barlow&apos;s seminal work on minimal entropy codes and unsupervised learning is <lb/>reiterated. In particular, the need to transmit the probability of events is put in a practical <lb/>neuronal framework for detecting suspicious events. A variant of the BCM learning rule [15] <lb/>is presented together with some mathematical results suggesting optimal minimal entropy <lb/>coding. <lb/></div>
		<keywords>Key words: Sparse coding, Non-Gaussian distributions, BCM Theory, Minimal Entropy <lb/></keywords>
		<div type="intro">1 Introduction</div>
		</front>
</text>
</tei>