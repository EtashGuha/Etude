<tei>
	<teiHeader>
	<fileDesc xml:id="unknown"/>
	</teiHeader>
<text xml:lang="en">
		<front>
		<docTitle>
			<titlePart type="main">Tolerating Latency Through Software-Controlled Prefetching <lb/>in Shared-Memory Multiprocessors <lb/></titlePart>
		</docTitle>
		<byline><docAuthor>Todd Mowry and Anoop Gupta <lb/></docAuthor></byline>
		<byline><affiliation>Computer Systems Laboratory <lb/>Stanford University, CA 94305 <lb/></affiliation></byline>
		<note type="other">To appear in the Journal of Parallel and Distributed Computing, June 1991. <lb/></note>
		<div type="abstract">Abstract <lb/>The large latency of memory accesses is a major obstacle in obtaining high processor utilization in large <lb/>scale shared-memory multiprocessors. Although the provision of coherent caches in many recent machines <lb/>has alleviated the problem somewhat, cache misses still occur frequently enough that they significantly lower <lb/>performance. In this paper we evaluate the effectiveness of non-binding software-controlled prefetching, as <lb/>proposed in the Stanford DASH Multiprocessor, to address this problem. The prefetches are non-binding in <lb/>the sense that the prefetched data is brought to a cache close to the processor, but is still available to the cache <lb/>coherence protocol to keep it consistent. Prefetching is software-controlled since the program must explicitly <lb/>issue prefetch instructions. <lb/>The paper presents results from detailed simulation studies done in the context of the Stanford DASH <lb/>multiprocessor. Our results show that for applications with regular data access patterns|we evaluate a particle-based simulator used in aeronautics and an LU-decomposition application|prefetching can be very effective. <lb/>It was easy to augment the applications to do prefetching and it increased their performance by 100-150% when <lb/>we prefetched directly into the processor&apos;s cache. However, for applications with complex data usage patterns, <lb/>prefetching was less successful. After much effort, the performance of a distributed-time logic simulation <lb/>application that made extensive use of pointers and linked lists could be increased only by 30%. The paper <lb/>also evaluates the effects of various hardware optimizations such as separate prefetch issue buffers, prefetching <lb/>with exclusive ownership, lockup-free caches, and weaker memory consistency models on the performance of <lb/>prefetching. <lb/></div>
		<div type="intro">1 Introduction</div>
		</front>
</text>
</tei>