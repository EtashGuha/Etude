<tei>
	<teiHeader>
	<fileDesc xml:id="110"/>
	</teiHeader>
<text xml:lang="en">
		<front>
		<docTitle>
			<titlePart type="main">Intra-Option Learning about Temporally Abstract Actions <lb/></titlePart>
		</docTitle>
		<byline><docAuthor>Richard S. Sutton <lb/></docAuthor></byline>
		<byline><affiliation>Department of Computer Science <lb/>University of Massachusetts <lb/></affiliation></byline>
		<address>Amherst, MA 01003-4610 <lb/></address>
		<email>rich@cs.umass.edu <lb/></email>
		<byline><docAuthor>Doina Precup <lb/></docAuthor></byline>
		<byline><affiliation>Department of Computer Science <lb/>University of Massachusetts <lb/></affiliation></byline>
		<address>Amherst, MA 01003-4610 <lb/></address>
		<email>dprecup@cs.umass.edu <lb/></email>
		<byline><docAuthor>Satinder Singh <lb/></docAuthor></byline>
		<byline><affiliation>Department of Computer Science <lb/>University of Colorado <lb/></affiliation></byline>
		<address>Boulder, CO 80309-0430 <lb/></address>
		<email>baveja@cs.colorado.edu <lb/></email>
		<div type="abstract">Abstract <lb/>Several researchers have proposed modeling <lb/>temporally abstract actions in reinforcement <lb/>learning by the combination of a policy and a termination condition, which we refer to as an option. Value functions over options and models of <lb/>options can be learned using methods designed <lb/>for semi-Markov decision processes (SMDPs). <lb/>However, all these methods require an option to <lb/>be executed to termination. In this paper we explore methods that learn about an option from <lb/>small fragments of experience consistent with <lb/>that option, even if the option itself is not executed. We call these methods intra-option learning methods because they learn from experience <lb/>within an option. Intra-option methods are sometimes much more efficient than SMDP methods because they can use off-policy temporal-difference mechanisms to learn simultaneously <lb/>about all the options consistent with an experience, not just the few that were actually executed. In this paper we present intra-option learning methods for learning value functions over options and for learning multi-time models of the <lb/>consequences of options. We present computational examples in which these new methods <lb/>learn much faster than SMDP methods and learn <lb/>effectively when SMDP methods cannot learn at <lb/>all. We also sketch a convergence proof for intra <lb/>option value learning. <lb/></div>
		<div type="intro">1 Introduction</div>
		</front>
</text>
</tei>