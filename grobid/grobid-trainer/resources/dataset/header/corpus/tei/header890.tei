<tei>
	<teiHeader>
	<fileDesc xml:id="unknown"/>
	</teiHeader>
<text xml:lang="en">
		<front>
		<docTitle>
			<titlePart type="main">On the Boosting Ability of Top-Down <lb/>Decision Tree Learning Algorithms <lb/></titlePart>
		</docTitle>
		<byline><docAuthor>Michael Kearns <lb/></docAuthor></byline>
		<byline><affiliation>AT&amp;T Research <lb/></affiliation></byline>
		<byline><docAuthor>Yishay Mansour <lb/></docAuthor></byline>
		<byline><affiliation>Tel-Aviv University <lb/></affiliation></byline>
		<date>May 1996 <lb/></date>
		<div type="abstract">Abstract <lb/>We analyze the performance of top-down algorithms for decision tree learning, such as those employed <lb/>by the widely used C4.5 and CART software packages. Our main result is a proof that such algorithms <lb/>are boosting algorithms. By this we mean that if the functions that label the internal nodes of the <lb/>decision tree can weakly approximate the unknown target function, then the top-down algorithms we <lb/>study will amplify this weak advantage to build a tree achieving any desired level of accuracy. The bounds <lb/>we obtain for this amplification show an interesting dependence on the splitting criterion used by the <lb/>top-down algorithm. More precisely, if the functions used to label the internal nodes have error 1=2 <lb/>as approximations to the target function, then for the splitting criteria used by CART and C4.5, trees <lb/>of size (1=*) O(1= 2 * 2 ) and (1=*) O(log(1=*)= 2 ) (respectively) suffice to drive the error below *. Thus (for <lb/>example), a small constant advantage over random guessing is amplified to any larger constant advantage <lb/>with trees of constant size. For a new splitting criterion suggested by our analysis, the much stronger <lb/>bound of (1=*) O(1= 2 ) (which is polynomial in 1=*) is obtained, which is provably optimal for decision <lb/>tree algorithms. The differing bounds have a natural explanation in terms of concavity properties of the <lb/>splitting criterion. <lb/>The primary contribution of this work is in proving that some popular and empirically successful <lb/>heuristics that are based on first principles meet the criteria of an independently motivated theoretical <lb/>model. <lb/></div>
		<note type="other">A preliminary version of this paper appears in </note><note type="reference">Proceedings of the Twenty-Eighth Annual ACM Symposium on the Theory of <lb/>Computing, pages 459-468, ACM Press, 1996. </note><note type="other">Authors&apos; addresses:</note>
		<byline><docAuthor>M. Kearns,</docAuthor></byline>
		<byline><affiliation>AT&amp;T Research,</affiliation></byline>
		<address>600 Mountain Avenue, Room <lb/>2A-423, Murray Hill, New Jersey 07974;</address>
		<email>electronic mail mkearns@research.att.com.</email>
		<byline><docAuthor>Y. Mansour,</docAuthor></byline>
		<byline><affiliation>Department of Computer <lb/>Science, Tel Aviv University,</affiliation></byline>
		<address>Tel Aviv, Israel;</address>
		<email>electronic mail mansour@math.tau.ac.il.</email>
		<note type="grant">Y. Mansour was supported in part by <lb/>the Israel Science Foundation, administered by the Israel Academy of Science and Humanities, and by a grant of the Israeli <lb/>Ministry of Science and Technology. <lb/></note>
		<pb/>
		</front>
</text>
</tei>