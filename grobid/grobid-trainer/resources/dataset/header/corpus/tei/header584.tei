<tei>
	<teiHeader>
	<fileDesc xml:id="586"/>
	</teiHeader>
<text xml:lang="en">
		<front>
		<docTitle>
			<titlePart type="main">A Unified Analysis of Value-Function-Based <lb/>Reinforcement-Learning Algorithms <lb/></titlePart>
		</docTitle>
		<byline><docAuthor>Csaba Szepesvari <lb/></docAuthor></byline>
		<byline><affiliation>Research Group on Artificial Intelligence <lb/>&quot;Jozsef Attila&quot; University <lb/></affiliation></byline>
		<byline><affiliation>Szeged 6720, Aradi vrt tere 1. <lb/>Hungary <lb/></affiliation></byline>
		<email>szepes@sol.cc.u-szeged.hu <lb/></email>
		<byline><docAuthor>Michael L. Littman <lb/></docAuthor></byline>
		<byline><affiliation>Department of Computer Science <lb/>Duke University <lb/></affiliation></byline>
		<byline><affiliation>Durham, NC 27708-0129 <lb/></affiliation></byline>
		<email>mlittman@cs.duke.edu <lb/></email>
		<date>October 27, 1998 <lb/></date>
		<div type="abstract">Abstract <lb/>Reinforcement learning is the problem of generating optimal behavior in a sequential decision-making environment given the opportunity of <lb/>interacting with it. Many algorithms for solving reinforcement-learning <lb/>problems work by computing improved estimates of the optimal value <lb/>function. We extend prior analyses of reinforcement-learning algorithms <lb/>and present a powerful new theorem that can provide a unified analysis of <lb/>value-function-based reinforcement-learning algorithms. The usefulness <lb/>of the theorem lies in how it allows the asynchronous convergence of a <lb/>complex reinforcement-learning algorithm to be proven by verifying that <lb/>a simpler synchronous algorithm converges. We illustrate the application <lb/>of the theorem by analyzing the convergence of Q-learning, model-based <lb/>reinforcement learning, Q-learning with multi-state updates, Q-learning <lb/>for Markov games, and risk-sensitive reinforcement learning. <lb/></div>
		<div type="intro">1 Introduction</div>
		</front>
</text>
</tei>