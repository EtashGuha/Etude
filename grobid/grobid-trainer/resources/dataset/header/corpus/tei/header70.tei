<tei>
	<teiHeader>
	<fileDesc xml:id="71"/>
	</teiHeader>
<text xml:lang="en">
		<front>
		<docTitle>
			<titlePart type="main">Markov Decision Processes in Large State Spaces <lb/></titlePart>
		</docTitle>
		<byline><docAuthor>Lawrence K. Saul and Satinder P. Singh <lb/></docAuthor></byline>
		<email>lksaul@psyche.mit.edu, </email> <email>singh@psyche.mit.edu <lb/></email>
		<byline><affiliation>Center for Biological and Computational Learning <lb/> Massachusetts Institute of Technology <lb/></affiliation></byline>
		<address>79 Amherst Street, E10-243 <lb/>Cambridge, MA 02139 <lb/></address>
		<div type="abstract">Abstract <lb/>In this paper we propose a new framework for <lb/>studying Markov decision processes (MDPs), <lb/>based on ideas from statistical mechanics. The <lb/>goal of learning in MDPs is to find a policy <lb/>that yields the maximum expected return over <lb/>time. In choosing policies, agents must therefore weigh the prospects of short-term versus <lb/>long-term gains. We study a simple MDP in <lb/>which the agent must constantly decide between exploratory jumps and local reward mining in state space. The number of policies to <lb/>choose from grows exponentially with the size <lb/>of the state space, N . We view the expected returns as defining an energy landscape over policy space. Methods from statistical mechanics <lb/>are used to analyze this landscape in the thermodynamic limit N ! 1. We calculate the <lb/>overall distribution of expected returns, as well <lb/>as the distribution of returns for policies at a <lb/>fixed Hamming distance from the optimal one. <lb/>We briefly discuss the problem of learning optimal policies from empirical estimates of the <lb/>expected return. As a first step, we relate our <lb/>findings for the entropy to the limit of high-temperature learning. Numerical simulations <lb/>support the theoretical results. <lb/></div>
		<div type="intro">1 Introduction</div>
		</front>
</text>
</tei>