<tei>
	<teiHeader>
	<fileDesc xml:id="676"/>
	</teiHeader>
<text xml:lang="en">
		<front>
		<docTitle>
			<titlePart type="main">Run-time Compilation for Parallel Sparse Matrix Computations <lb/></titlePart>
		</docTitle>
		<byline><docAuthor>Cong Fu and Tao Yang <lb/></docAuthor></byline>
		<byline><affiliation>Department of Computer Science <lb/>University of California, <lb/></affiliation></byline>
		<address>Santa Barbara, CA 93106. <lb/></address>
		<ptr type="web">http://www.cs.ucsb.edu/f~cfu,~tyangg <lb/></ptr>
		<div type="abstract">Abstract <lb/>Run-time compilation techniques have been shown effective <lb/>for automating the parallelization of loops with unstructured <lb/>indirect data accessing patterns. However, it is still an open <lb/>problem to efficiently parallelize sparse matrix factorizations <lb/>commonly used in iterative numerical problems. The difficulty is that a factorization process contains irregularly-interleaved communication and computation with varying <lb/>granularities and it is hard to obtain scalable performance <lb/>on distributed memory machines. In this paper, we present <lb/>an inspector/executor approach for parallelizing such applications by embodying automatic graph scheduling techniques to optimize interleaved communication and computation. We describe a run-time system called RAPID that <lb/>provides a set of library functions for specifying irregular <lb/>data objects and tasks that access these objects. The system <lb/>extracts a task dependence graph from data access patterns, <lb/>and executes tasks efficiently on a distributed memory machine. We discuss a set of optimization strategies used in <lb/>this system and demonstrate the application of this system <lb/>in parallelizing sparse Cholesky and LU factorizations. <lb/></div>
		<div type="intro">1 Introduction</div>
		</front>
</text>
</tei>