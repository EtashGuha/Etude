<tei>
	<teiHeader>
	<fileDesc xml:id="211"/>
	</teiHeader>
<text xml:lang="en">
		<front>
		<docTitle>
			<titlePart type="main">On Learning Soccer Strategies <lb/></titlePart>
		</docTitle>
		<byline><docAuthor>Rafal Salustowicz, Marco Wiering, Jurgen Schmidhuber <lb/></docAuthor></byline>
		<byline><affiliation>IDSIA,</affiliation></byline>
		<address>Corso Elvezia 36, 6900 Lugano, Switzerland <lb/></address>
		<email>e-mail: frafal, marco, juergeng@idsia.ch <lb/></email>
		<note type="reference">In W. Gerstner, A. Germond, M. Hasler, and J.-D. Nicoud, editors, <lb/>Proceedings of the Seventh International Conference on Artificial <lb/>Neural Networks (ICANN&apos;97), volume 1327 of Lecture Notes in Computer <lb/>Science, pages 769-774. Springer-Verlag Berlin Heidelberg, 1997. <lb/></note>
		<div type="abstract">Abstract. We use simulated soccer to study multiagent learning. Each <lb/>team&apos;s players (agents) share action set and policy but may behave differently due to position-dependent inputs. All agents making up a team <lb/>are rewarded or punished collectively in case of goals. We conduct simulations with varying team sizes, and compare two learning algorithms: <lb/>TD-Q learning with linear neural networks (TD-Q) and Probabilistic <lb/>Incremental Program Evolution (PIPE). TD-Q is based on evaluation <lb/>functions (EFs) mapping input/action pairs to expected reward, while <lb/>PIPE searches policy space directly. PIPE uses an adaptive probability <lb/>distribution to synthesize programs that calculate action probabilities <lb/>from current inputs. Our results show that TD-Q has difficulties to learn <lb/>appropriate shared EFs. PIPE, however, does not depend on EFs and <lb/>finds good policies faster and more reliably. <lb/></div>
		<div type="intro">1 Introduction</div>
		</front>
</text>
</tei>