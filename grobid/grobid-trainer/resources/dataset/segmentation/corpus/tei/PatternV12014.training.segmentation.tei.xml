<?xml version="1.0" ?>
<tei>
	<teiHeader>
		<fileDesc xml:id="_PatternV12014"/>
	</teiHeader>
	<text xml:lang="en">
			<front>Accurate 3D Action Recognition using Learning on the <lb/>Grassmann Manifold <lb/> Rim Slama  a,b  , Hazem Wannous  a,b  , Mohamed Daoudi  b,c  , Anuj Srivastava  d <lb/> a  University Lille 1, Villeneuve d&apos;Ascq, France <lb/> b  LIFL Laboratory / UMR CNRS 8022, Villeneuve d&apos;Ascq, France <lb/> c  Institut Mines-Telecom / Telecom Lille, Villeneuve d&apos;Ascq, France <lb/> d  Florida State University, Departement of Statistics, Tallahassee, USA <lb/> Abstract <lb/> In this paper we address the problem of modelling and analyzing hu-<lb/>man motion by focusing on 3D body skeletons. Particularly, our intent is <lb/>to represent skeletal motion in a geometric and e񮽙cient way, leading to an <lb/>accurate action-recognition system. Here an action is represented by a dy-<lb/>namical system whose observability matrix is characterized as an element <lb/>of a Grassmann manifold. To formulate our learning algorithm, we pro-<lb/>pose two distinct ideas: (1) In the first one we perform classification using <lb/>a Truncated Wrapped Gaussian model, one for each class in its own tangent <lb/>space. (2) In the second one we propose a novel learning algorithm that <lb/>uses a vector representation formed by concatenating local coordinates in <lb/>tangent spaces associated with di↵erent classes and training a linear SVM. <lb/>We evaluate our approaches on three public 3D action datasets: MSR-action <lb/>3D, UT-kinect and UCF-kinect datasets; these datasets represent di↵erent <lb/> 
			
			Email addresses: rim.slama@telecom-lille.fr (Rim Slama), <lb/> hazem.wannous@telecom-lille.fr (Hazem Wannous), <lb/> mohamed.daoudi@telecom-lille.fr (Mohamed Daoudi), anuj@stat.fsu.edu (Anuj <lb/>Srivastava) <lb/>
			
			Preprint submitted to Pattern Recognition <lb/>August 14, 2014 <lb/>
			
			kinds of challenges and together help provide an exhaustive evaluation. The <lb/>results show that our approaches either match or exceed state-of-the-art per-<lb/>formance reaching 91.21% on MSR-action 3D, 97.91% on UCF-kinect, and <lb/>88.5% on UT-kinect. Finally, we evaluate the latency, i.e. the ability to <lb/>recognize an action before its termination, of our approach and demonstrate <lb/>improvements relative to other published approaches. <lb/> Keywords: Human action recognition, Grassmann manifold, observational <lb/>latency, depth images, skeleton, classification. <lb/></front>

			<body> 1. Introduction <lb/> 1 <lb/> Human action and activity recognition is one of the most active research <lb/> 2 <lb/> topics in the computer vision community due to its many challenging issues. <lb/> 3 <lb/> The motivation behind the great interest granted to action recognition is <lb/> 4 <lb/> the large number of possible applications in consumer interactive entertain-<lb/> 5 <lb/> ment and gaming [1], surveillance systems [2], life-care and home systems <lb/> 6 <lb/> [3]. An extensive literature around this domain can be found in a number of <lb/> 7 <lb/> fields including pattern recognition, machine learning, and human-machine <lb/> 8 <lb/> interaction [4, 5]. <lb/> 9 <lb/> The main challenges in action recognition systems are the accuracy of <lb/> 10 <lb/> data acquisition and the dynamic modelling of the movements. The major <lb/> 11 <lb/> problems, which can alter the way actions are perceived and consequently <lb/> 12 <lb/> be recognized, are: occlusions, shadows and background extraction, lighting <lb/> 13 <lb/> condition variations and viewpoint changes. The recent release of consumer <lb/> 14 <lb/> depth cameras, like Microsoft Kinect, has significantly lighten these di񮽙-<lb/> 15 <lb/> culties that reduce the action recognition performance in 2D video. These <lb/> 16 <lb/></body>

			<page> 2 <lb/></page>

			<body> cameras provide in addition to the RGB image a depth stream allowing to <lb/> 17 <lb/> discern changes in depth in certain viewpoints. <lb/> 18 <lb/> More recently, Shotton et al. [6] have proposed a real-time approach for <lb/> 19 <lb/> estimating 3D positions of body joints using extensive training on synthetic <lb/> 20 <lb/> and real depth-streams. The accurate estimation obtained by such a low-<lb/> 21 <lb/> cost acquisition depth sensor has provided new opportunities for human-<lb/> 22 <lb/> computer-interaction applications, where popular gaming consoles involve <lb/> 23 <lb/> the player directly in interaction with the computer. While these acquisition <lb/> 24 <lb/> sensors and their accurate data are within everyone&apos;s reach, the next research <lb/> 25 <lb/> challenge is activity-driven. <lb/> 26 <lb/> In this paper we address the problem of modelling and analyzing human <lb/> 27 <lb/> motion in the 3D human joint space. Particularly, our intent is to represent <lb/> 28 <lb/> skeletal joint motion in a compact and e񮽙cient way that leads to an accurate <lb/> 29 <lb/> action recognition. Our ultimate goal is to develop an approach that avoids <lb/> 30 <lb/> an overly complex design of feature extraction and is able to recognize actions <lb/> 31 <lb/> performed by di↵erent actors in di↵erent contexts. <lb/> 32 <lb/> Additionally, we study the ability of our approach for reducing latency: <lb/> 33 <lb/> in other words, to quickly recognize human actions from the smallest number <lb/> 34 <lb/> of frames possible to permit a reliable recognition of the action occurring. <lb/> 35 <lb/> Furthermore, we analyze the impact of reducing the number of actions per <lb/> 36 <lb/> class in the training set on the classifier&apos;s accuracy. <lb/> 37 <lb/> In our approach, the spatio-temporal aspect of the action is considered <lb/> 38 <lb/> and each movement is characterized by a structure incorporating the intrinsic <lb/> 39 <lb/> nature of the data. We believe that 3D human joint motion data captures <lb/> 40 <lb/> useful knowledge to understand the intrinsic motion structure, and a manifold <lb/> 41 <lb/></body>

			<page> 3 <lb/></page>

			<body> representation of such simple features can provide discriminating structure <lb/> 42 <lb/> for action recognition. This leads to manifold-based analysis, which has <lb/> 43 <lb/> been successfully used in many computer vision applications such as visual <lb/> 44 <lb/> tracking [7] and action recognition in 2D video [8, 9, 10, 11]. <lb/> 45 <lb/> Our overall approach is sketched in Figure 1, which has the following <lb/> 46 <lb/> modules: <lb/> 47 <lb/> Split <lb/> Training <lb/>videos <lb/>Time series <lb/>extrac񮽙on <lb/>Temporal <lb/>modelling <lb/>Linear subspace <lb/>representa񮽙on <lb/>CT computa񮽙on <lb/>LTB representa񮽙on <lb/> S <lb/>V <lb/>M <lb/> Evalua񮽙on <lb/>LTB representa񮽙on <lb/>Dataset <lb/>Test <lb/>videos <lb/>Time series <lb/>extrac񮽙on <lb/>Temporal <lb/>modelling <lb/>Linear subspace <lb/>representa񮽙on <lb/> Figure 1: Overview of the approach. The illustrated pipeline is composed of two main <lb/>modules: (1) temporal modelling of time series data and manifold representation (2) <lb/>learning approach on the Control Tangent spaces on Grassman manifold, using Local <lb/>Bundle Tangent representation of data. <lb/> First, given training videos recorded from depth camera, motion trajecto-<lb/> 48 <lb/> ries from the 3D human joint in Euclidean space are extracted as time series. <lb/> 49 <lb/> Then, each motion represented by its time series is expressed as an autore-<lb/> 50 <lb/> gressive and moving average model (ARMA) in order to model its dynamic <lb/> 51 <lb/> process. The subspace spanned by columns of the observability matrix of <lb/> 52 <lb/> this model represents a point on a Grassmann manifold. <lb/> 53 <lb/> Second, using the Riemannian geometry of this manifold, we present a so-<lb/> 54 <lb/> lution for solving the classification problem. We studied statistical modelling <lb/> 55 <lb/></body>

			<page> 4 <lb/></page>

			<body> of inter-and intra-class variations in conjunction with appropriate tangent <lb/> 56 <lb/> vectors on this manifold. While class samples are presented by a Grass-<lb/> 57 <lb/> mann point cloud, we propose to learn Control Tangent (CT) spaces which <lb/> 58 <lb/> represent the mean of each class. <lb/> 59 <lb/> Third, each observation of the learning process is projected on all CTs <lb/> 60 <lb/> to form a Local Tangent Bandle (LTB) representation. This step allows <lb/> 61 <lb/> obtaining a discriminative parameterization incorporating class separation <lb/> 62 <lb/> properties and providing the input to a linear SVM classifier. <lb/> 63 <lb/> While given an unknown test video, to recognize its belonging to one of <lb/> 64 <lb/> N action classes, we apply the first step on the sequence to represent it as <lb/> 65 <lb/> a point on the Grassmann manifold. Then, this point is presented by its <lb/> 66 <lb/> LTB as done in learning step. In order to recognize the input action, SVM <lb/> 67 <lb/> classifier is performed. <lb/> 68 <lb/> The rest of the paper is organized as follows: In section 1, the state-of-<lb/> 69 <lb/> the-art is summarized and main contributions of this paper are highlighted. <lb/> 70 <lb/> In section 2, parametric subspace-based modelling of 3D joint-trajectory is <lb/> 71 <lb/> discussed. In Section 3, statistical tools developed on a Grassmann manifold <lb/> 72 <lb/> are presented and a new supervised learning algorithm is introduced. In <lb/> 73 <lb/> Section 4, the strength of the framework in term of accuracy and latency on <lb/> 74 <lb/> several datasets are demonstrated. Finally, concluding remarks are presented <lb/> 75 <lb/> in Section 5. <lb/> 76 <lb/> 2. Related works <lb/> 77 <lb/> In this section two categories of related works are reviewed from two <lb/> 78 <lb/> points of view: manifold-based approache and depth data representation. <lb/> 79 <lb/></body>

			<page> 5 <lb/></page>

			<body> We first review some related manifold based approaches for action analysis <lb/> 80 <lb/> and recognition in 2D video. Then we focus on the most recent methods of <lb/> 81 <lb/> action recognition from depth cameras. <lb/> 82 <lb/> 2.1. Manifold approaches in 2D videos <lb/> 83 <lb/> Human action modelling from 2D video is a well studied problem in the <lb/> 84 <lb/> literature. Recent surveys can be found in the work of Aggarwal et al. [12], <lb/> 85 <lb/> Weinland et al. [13], and Poppe [4]. Beside classical methods performed <lb/> 86 <lb/> in Euclidean space, a variety of techniques based on manifold analysis are <lb/> 87 <lb/> proposed in recent years. <lb/> 88 <lb/> In the first category of manifold based approaches, each frame of action <lb/> 89 <lb/> sequence (pose) is represented as an element of a manifold and the whole <lb/> 90 <lb/> action is represented as a trajectory on this manifold. These approaches give <lb/> 91 <lb/> solutions in the temporal domain to be invariant to speed and time using <lb/> 92 <lb/> techniques like Dynamic Time Warping (DTW) to align action trajectories <lb/> 93 <lb/> on the manifold. Also probabilistic grammatical models like Hidden Markov <lb/> 94 <lb/> Model (HMM) are used to classify these actions presented as trajectories. <lb/> 95 <lb/> Indeed, Veeraraghavan et al. [14] propose the use of human silhouettes ex-<lb/> 96 <lb/> tracted from video images as a representation of the pose. Silhouettes are <lb/> 97 <lb/> then characterized as points on the shape space manifold and modelled by <lb/> 98 <lb/> ARMA models in order to compare sequences using a DTW algorithm. In <lb/> 99 <lb/> another manifold shape space, Abdelkader et al. [15] represent each pose <lb/> 100 <lb/> silhouette as a point on the shape space of closed curves and each gesture is <lb/> 101 <lb/> represented as a trajectory on this space. To classify actions, two approaches <lb/> 102 <lb/> are used: a template-based approach (DTW) and a graphical model approach <lb/> 103 <lb/> (HMM). Other approaches use skeleton as a representation of each frame, as <lb/> 104 <lb/></body>

			<page> 6 <lb/></page>

			<body> works presented by Gong et al. [16]. They propose a spatio-Temporal Man-<lb/> 105 <lb/> ifold (STM) model to analyze non-linear multivariate time series with latent <lb/> 106 <lb/> spatial structure and apply it to recognize actions in the joint-trajectories <lb/> 107 <lb/> space. Based on STM, they propose a Dynamic Manifold Warping (DMW) <lb/> 108 <lb/> and a motion similarity metric to compare human action sequences both in <lb/> 109 <lb/> 2D space using a 2D tracker to extract joints from images and in 3D space <lb/> 110 <lb/> using Motion capture data. Recently, Gong et al. [17] propose a Kernelized <lb/> 111 <lb/> Temporal Cut (KTC) as an extension of their previous work [16]. They incor-<lb/> 112 <lb/> porate Hilbert space embedding of distributions to handle the non-parametric <lb/> 113 <lb/> and high dimensionality issues. <lb/> 114 <lb/> Some manifold approaches represent the entire action sequence as a point <lb/> 115 <lb/> on an other special manifold. Indeed, Turaga et al. [18] involve a study of <lb/> 116 <lb/> the geometric properties of the Grassmann and Stiefel manifolds, and give <lb/> 117 <lb/> appropriate definitions of Riemannian metrics and geodesics for the purpose <lb/> 118 <lb/> of video indexing and action recognition. Then, in order to perform the clas-<lb/> 119 <lb/> sification as a probability density function, a mean and a standard-deviation <lb/> 120 <lb/> are learnt for each class on class-specific tangent spaces. Turaga et al. [19] <lb/> 121 <lb/> use the same approach to represent complex actions by a collection of sub-<lb/> 122 <lb/> sequence. These sub-sequences correspond to a trajectory on a Grassmann <lb/> 123 <lb/> manifold. Both DTW and HMM are used for action modelling and com-<lb/> 124 <lb/> parison. Guo et al. [20] use covariance matrices of bags of low-dimensional <lb/> 125 <lb/> feature vectors to model the video sequence. These feature vectors are ex-<lb/> 126 <lb/> tracted from segments of silhouette tunnels of moving objects and coarsely <lb/> 127 <lb/> capture their shapes. <lb/> 128 <lb/> Without any extraction of human descriptor as silhouette and neither an <lb/> 129 <lb/></body>

			<page> 7 <lb/></page>

			<body> explicit learning, Lui et al. [21] introduce the notion of tangent bundle to <lb/> 130 <lb/> represent each action sequence on the Grassmann manifold. Videos are ex-<lb/> 131 <lb/> pressed as a third-order data tensor of raw pixel from action images, which <lb/> 132 <lb/> are then factorized on the Grassmann manifold. As each point on the mani-<lb/> 133 <lb/> fold has an associated tangent space, tangent vectors are computed between <lb/> 134 <lb/> elements on the manifold and obtained distances are used for action clas-<lb/> 135 <lb/> sification in a nearest neighbour fashion. In the same way, Lui et al. [22] <lb/> 136 <lb/> factorize raw pixel from images by high-order singular value decomposition <lb/> 137 <lb/> in order to represent the actions on Stiefel and Grassmann manifolds. How-<lb/> 138 <lb/> ever, in this work where raw pixels are directly factorized as manifold points, <lb/> 139 <lb/> there is no dynamic modelling of the sequence. In addition, only distances <lb/> 140 <lb/> obtained between all tangent vectors are used for action classification and <lb/> 141 <lb/> there is no training process on data. <lb/> 142 <lb/> Kernels [23, 24] are also used in order to transform subspaces of a man-<lb/> 143 <lb/> ifold onto a space where Euclidean metric can be applied. Shirazi et al. <lb/> 144 <lb/> [23] embed Grassmann manifolds upon a Hilbert space to minimize cluster-<lb/> 145 <lb/> ing distortions and then apply a locally discriminant analysis using a graph. <lb/> 146 <lb/> Video action classification is then obtained by a Nearest-Neighbour classi-<lb/> 147 <lb/> fier applied on Euclidean distances computed on the graph-embedded kernel. <lb/> 148 <lb/> Similarly, Harandi et al. [24] propose to represent the spatio-temporal as-<lb/> 149 <lb/> pect of the action by subspaces elements of a Grassmann manifold. Then, <lb/> 150 <lb/> they embed this manifold into reproducing kernel of Hilbert spaces in order <lb/> 151 <lb/> to tackle the problem of action classification on such manifolds. Gall et al. <lb/> 152 <lb/> [25] use multi-view system coupling action recognition on 2D images with <lb/> 153 <lb/> 3D pose estimation, were the action-specific manifolds are acting as a link <lb/> 154 <lb/></body>

			<page> 8 <lb/></page>

			<body> between them. <lb/> 155 <lb/> All these approaches cited above are based on features extracted from <lb/> 156 <lb/> 2D video sequences as silhouettes or raw pixels from images. However, the <lb/> 157 <lb/> recent emergence of low-cost depth sensors opens the possibility of revisiting <lb/> 158 <lb/> the problem of activity modelling and learning using depth data-driven. <lb/> 159 <lb/> 2.2. Depth data-driven approaches <lb/> 160 <lb/> Maps obtained by depth sensors are able to provide additional body shape <lb/> 161 <lb/> information to di↵erentiate actions that have similar 2D projections from a <lb/> 162 <lb/> single view. It has therefore motivated recent research works, to investigate <lb/> 163 <lb/> action recognition using the 3D information. Recent surveys [26, 27] are re-<lb/> 164 <lb/> porting works on depth videos. First methods used for activity recognition <lb/> 165 <lb/> from depth sequences have tendency to extrapolate techniques already de-<lb/> 166 <lb/> veloped for 2D video sequences. These approaches use points in depth map <lb/> 167 <lb/> sequences as a gray pixels in images to extract meaningful spatiotemporal <lb/> 168 <lb/> descriptors. In Wanqing et al. [28], depth maps are projected onto the three <lb/> 169 <lb/> orthogonal Cartesian planes (X 񮽙 Y , Z 񮽙 X, and Z 񮽙 Y planes) and the <lb/> 170 <lb/> contours of the projections are sampled for each frame. The sampled points <lb/> 171 <lb/> are used as bag-of-points to characterize a set of salient postures that corre-<lb/> 172 <lb/> spond to the nodes of an action graph used to model explicitly the dynamics <lb/> 173 <lb/> of the actions. Local feature extraction approaches like spatiotemporal inter-<lb/> 174 <lb/> est points (STIP) are also employed for action recognition on depth videos. <lb/> 175 <lb/> Bingbing et al.[29] use depth maps to extract STIP and encode Motion His-<lb/> 176 <lb/> tory Image (MHI) in a framework combining color and depth information. <lb/> 177 <lb/> Xia et al [30] propose a method to extract STIP a on depth videos (DSTIP). <lb/> 178 <lb/> Then around these points of interest they build a depth cuboid similarity <lb/> 179 <lb/></body>

			<page> 9 <lb/></page>

			<body> feature as descriptor for each action. In the work proposed by Vieira et al. <lb/> 180 <lb/> [31], each depth map sequence is represented as a 4D grid by dividing the <lb/> 181 <lb/> space and time axes into multiple segments in order to extract SpatioTempo-<lb/> 182 <lb/> ral Occupancy Pattern features (STOP). Also in Wang et al. [32], the action <lb/> 183 <lb/> sequence is considered as a 4D shape but Random Occupancy Pattern (ROP) <lb/> 184 <lb/> is used for features extraction. Yang et al.[33] employ Histograms of Oriented <lb/> 185 <lb/> Gradients features (HOG) computed from Depth Motion Maps (DMM), as <lb/> 186 <lb/> the representation of an action sequence. These histograms are then used as <lb/> 187 <lb/> input to SVM classifier. Similarly, Oreifej et al. [34] compute a 4D histogram <lb/> 188 <lb/> over depth, time, and spatial coordinates capturing the distribution of the <lb/> 189 <lb/> surface normal orientation. This histogram is created using 4D projectors <lb/> 190 <lb/> allowing quantification in 4D space. <lb/> 191 <lb/> The availability of 3D sensors has recently made possible to estimate 3D <lb/> 192 <lb/> positions of body joints. Especially thanks to the work of Shotton et al. <lb/> 193 <lb/> [6], where a real-time method is proposed to accurately predict 3D positions <lb/> 194 <lb/> of body joints. Thanks to this work, skeleton based methods have become <lb/> 195 <lb/> popular and many approaches in the literature propose to model the dynamic <lb/> 196 <lb/> of the action using these features. <lb/> 197 <lb/> Xia et al. [35] compute histograms of the locations of 12 3D joints as a <lb/> 198 <lb/> compact representation of postures and use them to construct posture visual <lb/> 199 <lb/> words of actions. The temporal evolutions of those visual words are modeled <lb/> 200 <lb/> by a discrete HMM. Yang et al. [36] extract three features, as pair-wise dif-<lb/> 201 <lb/> ferences of joint positions, for each skeleton joint. Then, principal component <lb/> 202 <lb/> analysis (PCA) is used to reduce redundancy and noise from feature, and it <lb/> 203 <lb/> is also used to obtain a compact Eigen Joints representation for each frame. <lb/> 204 <lb/></body>

			<page> 10 <lb/></page>

			<body> Finally, a na¨ ve-Bayes nearest-neighbour classifier is used for multi-class ac-<lb/> 205 <lb/> tion classification. The popular Dynamic Time Warping (DTW) technique <lb/> 206 <lb/> [37], well-known in speech recognition area, is also used for gesture and action <lb/> 207 <lb/> recognition using depth data. The classical DTW algorithm was defined to <lb/> 208 <lb/> match temporal distortions between two data trajectories, by finding an op-<lb/> 209 <lb/> timal warping path between the two time series. The feature vector of time <lb/> 210 <lb/> series is directly constructed from human body joint orientation extracted <lb/> 211 <lb/> from depth camera or 3D Motion Capture sensors. Reyes et al. [38] per-<lb/> 212 <lb/> form DTW on a feature vector defined by 15 joints on a 3D human skeleton <lb/> 213 <lb/> obtained using PrimeSense NiTE. Similarly, Sempena et al. [39], by the 3D <lb/> 214 <lb/> human skeleton model, use quaternions to form a 60-element feature vec-<lb/> 215 <lb/> tor. The obtained warping path, by classical DTW algorithm, between two <lb/> 216 <lb/> time series is mainly subjected to some constraints: (1) boundary constraint <lb/> 217 <lb/> which enforces the first elements of the sequences as well as the last one <lb/> 218 <lb/> to be aligned to each other (2) monotonicity constraint which requires that <lb/> 219 <lb/> the points in the warping path are monotonically spaced in time in the two <lb/> 220 <lb/> sequences. This technique is relatively sensitive to noise as it requires all <lb/> 221 <lb/> elements of the sequences to be matched to a corresponding elements of the <lb/> 222 <lb/> other sequence. It also has a drawback related to its computational complex-<lb/> 223 <lb/> ity incurring in quadratic cost. However, many works have been proposed to <lb/> 224 <lb/> bypass its drawbacks by means of probabilistic models [40] or incorporating <lb/> 225 <lb/> manifold learning approach [17, 16]. <lb/> 226 <lb/> Recent research has carried on more complex challenge of in-line recogni-<lb/> 227 <lb/> tion systems for di↵erent applications, in which a trade-o↵ between accuracy <lb/> 228 <lb/> and latency can be highlighted. Ellis et al. [41] study this trade-o↵ and <lb/> 229 <lb/></body>

			<page> 11 <lb/></page>

			<body> employed a Latency Aware Learning (LAL) method, reducing latency when <lb/> 230 <lb/> recognizing actions. They train a logistic regression-based classifier, on 3D <lb/> 231 <lb/> joint position sequences captured by kinect camera, to search a single canon-<lb/> 232 <lb/> ical posture for recognition. Another work is presented by Barnachon et <lb/> 233 <lb/> al. [42], where a histogram-based formulation is introduced for recognizing <lb/> 234 <lb/> streams of poses. In this representation, classical histogram is extended to <lb/> 235 <lb/> integral one to overcome the lack of temporal information in histograms. <lb/> 236 <lb/> They also prove the possibility of recognizing actions even before they are <lb/> 237 <lb/> completed using the integral histogram approach. Tests are made on both 3D <lb/> 238 <lb/> MoCap from TUM kitchen dataset [43] and RGB-D data from MSR-Action <lb/> 239 <lb/> dataset [28]. <lb/> 240 <lb/> Some hybrid approaches combining both skeleton data features and depth <lb/> 241 <lb/> information were recently introduced, trying to combine positive aspects of <lb/> 242 <lb/> both approaches. Azary et al. [44] propose spatiotemporal descriptors as <lb/> 243 <lb/> time-invariant action surfaces, combining image features extracted using ra-<lb/> 244 <lb/> dial distance measures and 3D joint tracking. Wang et al. [45] compute <lb/> 245 <lb/> local features on patches around joints for human body representation. The <lb/> 246 <lb/> temporal structure of each joint in the sequence is represented through a tem-<lb/> 247 <lb/> poral pattern representation called Fourier Temporal Pyramid. In Oreifej et <lb/> 248 <lb/> al. [34], a spatiotemporal histogram (HON4D) computed over depth, time, <lb/> 249 <lb/> and spatial coordinates is used to encode the distribution of the surface nor-<lb/> 250 <lb/> mal orientation. Similarly to Wang et al. [45], HON4D histograms [34] are <lb/> 251 <lb/> computed around joints to provide the input of an SVM classifier. Althloothi <lb/> 252 <lb/> et al. [46] represent 3D shape features based on spherical harmonics repre-<lb/> 253 <lb/> sentation and 3D motion features using kinematic structure from skeleton. <lb/> 254 <lb/></body>

			<page> 12 <lb/></page>

			<body> Both feature are then merged using multi kernel learning method. <lb/> 255 <lb/> It is important to note that, to date, few works have very recently pro-<lb/> 256 <lb/> posed to use manifold analysis for 3D action recognition. Devanne et al. [47], <lb/> 257 <lb/> propose a spatiotemporal motion representation to characterize the action as <lb/> 258 <lb/> a trajectory which corresponds to a point on Riemannian manifold of open <lb/> 259 <lb/> curves shape space. These motion trajectories are extracted from 3D joints, <lb/> 260 <lb/> and the action recognition is performed by K-Nearest-Neighbor method ap-<lb/> 261 <lb/> plied on geodesic distances obtained on open curve shape space. Azary et al. <lb/> 262 <lb/> [48] use a Grassmannian representation as an interpretation of depth motion <lb/> 263 <lb/> image (DMI) computed from depth pixel values. All DMI in the sequence <lb/> 264 <lb/> are combined to create a motion depth surface representing the action as a <lb/> 265 <lb/> spatiotemporal descriptor. <lb/> 266 <lb/> 2.3. Contributions and proposed approach <lb/> 267 <lb/> On the one hand, approaches modelling actions as elements of manifolds <lb/> 268 <lb/> [49, 50, 9] prove that it is an appropriate way to represent and compare <lb/> 269 <lb/> videos. On the other hand, very few works deal with this task using depth <lb/> 270 <lb/> images and it is still possible to improve learning step using these models. <lb/> 271 <lb/> Besides, linear dynamic systems [51] show more and more promising results <lb/> 272 <lb/> on the motion modelling since they exhibit the stationary properties in time, <lb/> 273 <lb/> so they fit for action representation. <lb/> 274 <lb/> In this paper, we propose the use of geometric structure inherent in the <lb/> 275 <lb/> Grassmann manifold for action analysis. We perform action recognition by <lb/> 276 <lb/> introducing a manifold learning algorithm in conjunction with dynamic mod-<lb/> 277 <lb/> elling process. In particular, after modelling motions as a linear dynamic sys-<lb/> 278 <lb/> tems using ARMA models, we are interested in a representation of each point <lb/> 279 <lb/></body>

			<page> 13 <lb/></page>

			<body> on the manifold incorporating class separation properties. Our representa-<lb/> 280 <lb/> tion takes benefit of statistics in the Grassmann manifold and action classes <lb/> 281 <lb/> representations on tangent spaces. From spatiotemporal point of view, each <lb/> 282 <lb/> action sequence is represented in our approach as linear dynamical system <lb/> 283 <lb/> acquiring the time series of 3D joint-trajectory. From geometrical point of <lb/> 284 <lb/> view, each action sequence is viewed as a point on the Grassmann manifold. <lb/> 285 <lb/> In terms of machine learning, a discriminative representation is provided for <lb/> 286 <lb/> each action thanks to a set of appropriate tangent vectors taking benefit <lb/> 287 <lb/> of manifold proprieties. Finally, the e񮽙ciency of the proposed approach is <lb/> 288 <lb/> demonstrated on three challenging action recognition datasets captured by <lb/> 289 <lb/> depth cameras. <lb/> 290 <lb/> 3. Spatiotemporal modelling of action <lb/> 291 <lb/> The human body can be represented as an articulated system composed <lb/> 292 <lb/> of hierarchical joints that are connected with bones, forming a skeleton. The <lb/> 293 <lb/> two best-known skeletons provided by the Microsoft Kinect sensor, are those <lb/> 294 <lb/> obtained by o񮽙cial Microsoft SDK, which contains 20 joints, and PrimeSense <lb/> 295 <lb/> NiTE which contains only 15 joints (see Figure 2). The various joint con-<lb/> 296 <lb/> figurations throughout the motion sequence produce a time series of skeletal <lb/> 297 <lb/> poses giving the skeleton movement. In our approach, an action is simply <lb/> 298 <lb/> described as a collection of time series of 3D positions of the joints in the <lb/> 299 <lb/> hierarchical configuration. <lb/> 300 <lb/> 3.1. Linear dynamic model <lb/> 301 <lb/> Let p  j <lb/> t  denote the 3D position of a joint j at a given frame t i.e., p  j  = <lb/> 302 <lb/> [x  j  , y  j  , z  j  ]  j=1:J  , with J is the number of joints. The joint position time-series <lb/> 303 <lb/></body>

			<page> 14 <lb/></page>

			<body> 1 <lb/> 2 <lb/>3 <lb/>5 <lb/>6 <lb/>7 <lb/>8 <lb/>9 <lb/>10 <lb/>12 <lb/>4 <lb/>11 <lb/>13 <lb/>14 <lb/>16 <lb/>18 <lb/>20 <lb/>15 <lb/>19 <lb/>17 <lb/> (a) <lb/> 1 <lb/> 2 <lb/>3 <lb/>5 <lb/>6 <lb/>7 <lb/>8 <lb/>12 <lb/>11 <lb/>13 <lb/>15 <lb/>19 <lb/>14 <lb/>16 <lb/>20 <lb/> (b) <lb/>Figure 2: Skeleton joint locations captured by Microsof Kinect sensor (a) using Microsoft <lb/>SDK (b) using PrimeSense NiTE. Joint signification are: (1) head (2) shoulder center (3) <lb/>spine (4) hip center (5/6) left/right hip (7/8) left/ ight knee (9/10) left/right ankle (11/12) <lb/>left/right foot (13/14) left/right shoulder (15/16) left/right elbow (17/19) left/right wrist <lb/>(19/20) left/right hand. <lb/> of joint j is p  j <lb/> t  = {x  j <lb/>t  , y  j <lb/>t  , z  j <lb/>t  }  t=1:T <lb/>j=1:J  , with T the number of frames. A motion <lb/> 304 <lb/> sequence can then be seen as a matrix collecting all time-series from J joints, <lb/> 305 <lb/> i.e., M = [p  1  p  2  · · · p  T  ], p 2 R  3⇤J  . <lb/> 306 <lb/> At this level, we could consider using DTW algorithm [37] to find optimal <lb/> 307 <lb/> non-linear warping function to match these given time-series as proposed by <lb/> 308 <lb/> [38, 39, 16]. However, we opted for a system combining a linear dynamic <lb/> 309 <lb/> modelling with statistical analysis on a manifold, avoiding the boundary and <lb/> 310 <lb/> the monotonicity constraints presented by classical DTW algorithm. Such a <lb/> 311 <lb/> system is also less sensitive to noise due to the poor estimation of the joint <lb/> 312 <lb/> locations, in addition to its reduced computational complexity. <lb/> 313 <lb/> The dynamic and the continuity of movement imply that the action can <lb/> 314 <lb/> not be resumed as a simply set of skeletal poses because of the temporal <lb/> 315 <lb/></body>

			<page> 15 <lb/></page>

			<body> information contained in the sequence. Instead of directly using original <lb/> 316 <lb/> joint position time-series data, we believe that a linear dynamic system, like <lb/> 317 <lb/> that often used for dynamic texture modelling, is essential before manifold <lb/> 318 <lb/> analysis. Therefore, to capture both the spatial and the temporal dynamics <lb/> 319 <lb/> of a motion, linear dynamical system characterized by ARMA models are <lb/> 320 <lb/> applied to the 3D joint position time-series matrix M . <lb/> 321 <lb/> The dynamic captured by the ARMA [52, 53] model during an action <lb/> 322 <lb/> sequence M can be represented as: <lb/> 323 <lb/> p(t) = Cz(t) + w(t), w(t) ⇠ N (0, R), <lb/>z(t + 1) = Az(t) + v(t), v(t) ⇠ N (0, Q) <lb/> (1) <lb/>where z 2 R  d  is a hidden state vector, A 2 R  d⇥d  is the transition matrix <lb/> 324 <lb/> and C 2 R  3⇤J⇥d  is the measurement matrix. w and v are noise components <lb/> 325 <lb/> modeled as normal with mean equal to zero and covariance matrix R 2 <lb/> 326 <lb/> R  3⇤J⇥3⇤J  and Q 2 R  d⇥d  respectively. The goal is to learn parameters of the <lb/> 327 <lb/> model (A, C) given by these equations. Let U <lb/> P V  T  be the singular value <lb/> 328 <lb/> decomposition of the matrix M . Then, the estimated model parameters A <lb/> 329 <lb/> and C are given by: ˆ <lb/> C = U and <lb/> A = <lb/> P <lb/> V  T  D  1  V (V  T  D  2  V )  񮽙1  P  񮽙1  , where <lb/> 330 <lb/> D  1  = [0 0, I  ⌧ 񮽙1  0], D  2  = [I  ⌧ 񮽙1  0, 0 0] and I  ⌧ 񮽙1  is the identity matrix of <lb/> 331 <lb/> size ⌧ 񮽙 1. <lb/> 332 <lb/> Comparing two ARMA models can be done by simply comparing their <lb/> 333 <lb/> observability matrices. The expected observation sequence generated by an <lb/> 334 <lb/> ARMA model (A,C) lies in the column space of the extended observability <lb/> 335 <lb/> matrix given by ✓  T <lb/> 1  = [C  T  , (CA)  T  , (CA  2  )  T  , ...]  T  . This can be approximated <lb/> 336 <lb/> by the finite observability matrix ✓  T <lb/>m  = [C  T  , (CA)  T  , (CA  2  )  T  , ..., (CA  2  )  m  ]  T <lb/> 337 <lb/></body>

			<page> 16 <lb/></page>

			<body> [18]. The subspace spanned by columns of this finite observability matrix <lb/> 338 <lb/> corresponds to a point on a Grassmann manifold. <lb/> 339 <lb/> 3.2. Grassmann manifold interpretation <lb/> 340 <lb/> Grassmannian analysis provides a natural way to deal with the problem of <lb/> 341 <lb/> sequence matching. Especially, this manifold allows to represent a sequence <lb/> 342 <lb/> by a point on its space and o↵ers tools to compare and to do statistics on <lb/> 343 <lb/> this manifold. The classification problem of sets of motions represented by a <lb/> 344 <lb/> collection of features can be transformed to point classification problem on <lb/> 345 <lb/> the Grassmann manifold. <lb/> 346 <lb/> In this work we are interested in Grassmann manifolds which definition <lb/> 347 <lb/> is as below. <lb/> 348 <lb/> Definition: The Grassmann manifold G  n⇥d  is a quotient space of orthogonal <lb/> 349 <lb/> group O(n) and is defined as the set of d-dimensional linear subspaces of R  n  . <lb/> 350 <lb/> Points on the Grassmann manifold are equivalent classes of n ⇥ d orthogonal <lb/> 351 <lb/> matrices, with d &lt; n, where two matrices are equivalent if their columns span <lb/> 352 <lb/> the same d-dimensional subspace. <lb/> 353 <lb/> Let µ denotes an element on G  n⇥d  , the tangent space to this element T  µ  on <lb/> 354 <lb/> G  n,d  is the tangent plane to the surface of the manifold at µ. It is possible <lb/> 355 <lb/> to map a point U , of the Grassmann manifold, to a vector in the tangent <lb/> 356 <lb/> space T  µ  using the logarithm map as defined by Turaga et al. [18]. An other <lb/> 357 <lb/> important tool in statistics is the exponential map Exp  µ  : T  µ  (G  n,d  ) ! G  n,d  , <lb/> 358 <lb/> which allows to move on the manifold. <lb/> 359 <lb/> Two points U  1  and U  2  on G  n,d  are equivalent if one can be mapped into <lb/> 360 <lb/> the other one by d ⇥ d orthogonal matrix [54]. In other words, U  1  and U  2  are <lb/> 361 <lb/> equivalent if the d columns of U  1  are rotations of U  2  . The minimum length <lb/> 362 <lb/></body>

			<page> 17 <lb/></page>

			<body> curve connecting these two points is the geodesic between them computed <lb/> 363 <lb/> as: <lb/> 364 <lb/> d  geod  (U  1  , U  2  ) =k [✓  1  , ✓  2  , · · · , ✓  i  , · · · , ✓  d  ] k  2 <lb/> (2) <lb/>where ✓  i  is the principal angle vector which can be computed through the <lb/> 365 <lb/> SVD of U  T <lb/> 1  U  2  . <lb/> 366 <lb/> 4. Learning process on the manifold <lb/> 367 <lb/> Let {U  1  , · · · U  N  } be N actions represented by points on the Grassmann <lb/> 368 <lb/> manifold. A common learning approach on manifolds is based on the use <lb/> 369 <lb/> of only one-tangent space, which usually can be obtained as the tangent <lb/> 370 <lb/> space to the mean (µ) of the entire data points {U  i  }  i=1:N  without regard <lb/> 371 <lb/> to class labels. All data points on the manifold are then projected on this <lb/> 372 <lb/> tangent space to provide the input of a classifier. This assumption provide an <lb/> 373 <lb/> accommodated solution to use a classical supervised learning on the manifold. <lb/> 374 <lb/> However, this flattening of the manifold through tangent space is not e񮽙cient <lb/> 375 <lb/> since the tangent space on the global mean can be far from other points. <lb/> 376 <lb/> A more appropriate way is to consider separate tangent spaces for each <lb/> 377 <lb/> class at the class-mean. The classification is then performed in these indi-<lb/> 378 <lb/> vidual tangent spaces as in [18]. <lb/> 379 <lb/> Some other approaches explore the idea of tangent bundle as in Lui et <lb/> 380 <lb/> al. [21, 22], in which all tangent planes of all data points on the manifold <lb/> 381 <lb/> are considered. Tangent vectors are then computed between all points on <lb/> 382 <lb/> a Grassmann manifold and action classification is performed thanks to ob-<lb/> 383 <lb/> tained distances. <lb/> 384 <lb/> We believe that using several tangent spaces, obtained for each class of <lb/> 385 <lb/></body>

			<page> 18 <lb/></page>

			<body> the training data points, is more intuitive. However, the question here is how <lb/> 386 <lb/> to learn a classifier in this case? <lb/> 387 <lb/> In the rest of the section, we present a statistical computation of the mean <lb/> 388 <lb/> in the Grassmann manifold [55]. Then, we propose two learning methods on <lb/> 389 <lb/> this manifold taking benefit from tangent space class specific and tangent <lb/> 390 <lb/> bundle [21]: Truncated Wrapped Gaussian (TWG) [56] and Local Tangent <lb/> 391 <lb/> Bundle SVM (LBTSVM). <lb/> 392 <lb/> 4.1. Mean computation on the Grassmann manifold <lb/> 393 <lb/> The Karcher mean [55] enables computation of a mean representative for <lb/> 394 <lb/> a cluster of points on the manifold. This mean should belong to the same <lb/> 395 <lb/> space as the given points. In our case, we need Karcher mean to compute <lb/> 396 <lb/> averages on the Grassman manifold and more precisely means of each action <lb/> 397 <lb/> class which represents the action at best. The algorithm exploits log and exp <lb/> 398 <lb/> maps in a predictor/corrector loop until convergence to an expected point. <lb/> 399 <lb/> The computation of a mean can be used to perform an action classification <lb/> 400 <lb/> solution. This can be done by a s simple comparison of an unknown action, <lb/> 401 <lb/> represented as a point on the manifold, to all class-means and assigning it to <lb/> 402 <lb/> the nearest one using the distance presented in Equation 2. <lb/> 403 <lb/> 4.2. Truncated Wrapped Gaussian <lb/> 404 <lb/> In addition to the mean µ computed by Karcher mean on {U  i  }  i=1:N  , we <lb/> 405 <lb/> look for the standard deviation value 񮽙 between all actions in each class of <lb/> 406 <lb/> training data. The 񮽙 must be computed on {V  i  }  i=1:N  where V = exp  񮽙1 <lb/> µ  (U  i  ) <lb/> 407 <lb/> are the projections of actions from the Grassmann manifold into the tangent <lb/> 408 <lb/></body>

			<page> 19 <lb/></page>

			<body> space defined on the mean µ. The key idea here is to use the fact that the <lb/> 409 <lb/> tangent space T  µ  (G  n,d  ) is a vector space. <lb/> 410 <lb/> Thus, we can estimate the parameters of a probability density function <lb/> 411 <lb/> such as a Gaussian and then use the exponential map to wrap these param-<lb/> 412 <lb/> eters back onto the manifold using exponential map operator [18]. However, <lb/> 413 <lb/> the exponential map is not a bijection for the Grassmann manifold. In fact, a <lb/> 414 <lb/> line on tangent space, with infinite length, can be warpped around the man-<lb/> 415 <lb/> ifold many times. Thus, some points of this line are going to have more than <lb/> 416 <lb/> one image on G  n,d  . It becomes a bijection only if the domain is restricted. <lb/> 417 <lb/> Therefore, we can restrict the tangent space by a truncation beyond a radius <lb/> 418 <lb/> of ⇡ in T  µ  (G  n,d  ). By truncation, the normalization constant changes for mul-<lb/> 419 <lb/> tivariate density in T  µ  (G  n,d  ). In fact, it gets scaled down depending on how <lb/> 420 <lb/> much of the probability mass is left out of the truncation region. <lb/> 421 <lb/> Let f (x) denotes the probability density function (pdf) defined on T  µ  (G  n,d  ) <lb/> 422 <lb/> by : <lb/> 423 <lb/> f (x) = <lb/>1 <lb/> p <lb/> 2⇡⇡  2 <lb/> e <lb/> 񮽙x 2 <lb/> 2񮽙 2 <lb/> (3) <lb/>After truncation, an approximation of f gives: <lb/> 424 <lb/> f (x) = <lb/> f (x) ⇥  1  |x|&lt;⇡ <lb/> z <lb/> (4) <lb/>where z is the normalization factor : <lb/> 425 <lb/> z = <lb/> Z  ⇡ <lb/> 񮽙⇡ <lb/> f (x) ⇥  1  |x|&lt;⇡  dx <lb/> (5) <lb/>Using Monte Carlo estimation, it can proved that the estimation of z is given <lb/> 426 <lb/></body>

			<page> 20 <lb/></page>

			<body> by: <lb/> 427 <lb/> z = <lb/>1 <lb/> N <lb/> N <lb/> X <lb/> i=1 <lb/> 1  |x  i  |&lt;⇡ <lb/> (6) <lb/>In practice, we employ wrapped Gaussians in each class-specific tangent <lb/> 428 <lb/> space. Separate tangent space is considered for each class at its mean com-<lb/> 429 <lb/> puted by Karcher mean algorithm. Predicted class of an observation point <lb/> 430 <lb/> is estimated in these individual tangent spaces. In the training step, the <lb/> 431 <lb/> mean, standard deviation and normalization factor in each class of actions <lb/> 432 <lb/> are computed. The predicted label of unknown class action is estimated as <lb/> 433 <lb/> a function of probability density in class-specific tangent spaces. <lb/> 434 <lb/> 4.3. Local Tangent Bundle <lb/> 435 <lb/> We intent here to generalize a learning algorithm to work with data points <lb/> 436 <lb/> which are geometrically lying to a Grassmann manifold. Using multiple class-<lb/> 437 <lb/> specific tangent spaces is decidedly more relevant than single one. However, <lb/> 438 <lb/> restrict the learning to only the mean and the standard-deviation in each tan-<lb/> 439 <lb/> gent space, as in TGW method, is probably insu񮽙cient to classify complex <lb/> 440 <lb/> actions with small inter-class variation. Our idea is to build a supervised clas-<lb/> 441 <lb/> sifier on the manifold but without limiting the learning process to distances <lb/> 442 <lb/> computed on the tangent spaces as in [22]. <lb/> 443 <lb/> We consider such data points to be embedded in higher dimensional rep-<lb/> 444 <lb/> resentation providing a natural and implicit separation of directions. We <lb/> 445 <lb/> use the notion of tangent bundle on the manifold to formulate our learning <lb/> 446 <lb/> algorithm. <lb/> 447 <lb/> The tangent bundle of a manifold is defined in the literature as the mani-<lb/> 448 <lb/> fold along with the set of tangent planes taken at all points on it. Each such <lb/> 449 <lb/></body>

			<page> 21 <lb/></page>

			<body> a tangent plane can be equipped with a local Euclidean coordinate system. <lb/> 450 <lb/> In our approach, we consider several &quot; local &quot; bundles, each one represents the <lb/> 451 <lb/> tangent planes taken at all points belonging to a class from training dataset <lb/> 452 <lb/> and expressed as class-specific local bundle. <lb/> 453 <lb/> We generate Control Tangents (CT) on the manifold, which represent <lb/> 454 <lb/> all class-specific local bundles of data points. Each CT can be seen as the <lb/> 455 <lb/> tangent space of the Karcher mean of all points belonging to the same class <lb/> 456 <lb/> of points from only training data. Karcher mean algorithm can be employed <lb/> 457 <lb/> here for mean computation. <lb/> 458 <lb/> We introduce an upswing of the manifold learning so-called Local Tangent <lb/> 459 <lb/> Bundle (LTB), in which proximities are required between each point on the <lb/> 460 <lb/> manifold and all CTs. The LTB can be viewed as a parameterization of <lb/> 461 <lb/> a point on the manifold which incorporates implicitly release properties in <lb/> 462 <lb/> relation to all class clusters, by mapping this point to all CTs using logarithm <lb/> 463 <lb/> map. <lb/> 464 <lb/> The LTBs can provide the input of a classifier, like the linear SVM clas-<lb/> 465 <lb/> sifier as in our case. In doing so, the learning model of the classifier is con-<lb/> 466 <lb/> structed using LTBs instead of classifying as function of the local distances <lb/> 467 <lb/> (mean and standard-deviation) of the point from LTBs as in TWG method. <lb/> 468 <lb/> We finally notice that training a linear SVM classifier on our represen-<lb/> 469 <lb/> tation of points provided by LTB is more appropriate than the use of SVM <lb/> 470 <lb/> with classical Kernel, like rbf, on original points on the manifold. <lb/> 471 <lb/> In experiments, we compare our learning approach LTBSVM to the clas-<lb/> 472 <lb/> sical one denoted as One-tangent SVM (TSVM), in which the mean is com-<lb/> 473 <lb/> puted on the entire training dataset regardless to class labels. Then, all <lb/> 474 <lb/></body>

			<page> 22 <lb/></page>

			<body> points on the manifold are projected on this later to provide the inputs of a <lb/> 475 <lb/> linear SVM. <lb/> 476 <lb/> A graphical illustration of the manifold learning by TWG and LTB can <lb/> 477 <lb/> be shown in Figure 3. <lb/> 478 <lb/> (a) TWG <lb/>(b) LTB <lb/>Figure 3: Conceptual TWG and LTB learning methods on the Grassmann manifold. <lb/>(a) Actions belonging to the same class, illustrated with same color, are projected to the <lb/>tangent space presented with their mean and then Gaussian function is computed on each <lb/>tangent space, (b) An action is projected on all CTs, and thus construct a new observation <lb/>is represented by its LTB. <lb/> 5. Experimental results <lb/> 479 <lb/> This section summarizes our empirical results and provides an analysis of <lb/> 480 <lb/> the performances of our proposed approach on several datasets compared to <lb/> 481 <lb/> the state-of-the-art approaches. <lb/> 482 <lb/></body>

			<page> 23 <lb/></page>

			<body> 5.1. Data and features <lb/> 483 <lb/>We extensively experimented our proposed approach on three public 3D <lb/> 484 <lb/> action datasets containing various challenges, including MSR-action 3D [28], <lb/> 485 <lb/> UT-kinect [35] and UCF-kinect [41]. All details about these datasets: di↵er-<lb/> 486 <lb/> ent types and number of motions, number of subjects executing these motions <lb/> 487 <lb/> and the experimental protocol used for evaluation are summarized in Table <lb/> 488 <lb/> 1. Examples of actions from these datasets are shown in Figure 4. <lb/> 489 <lb/> Dataset <lb/> Motions <lb/>Total number of ac-<lb/>tions <lb/>Experimental <lb/>protocol <lb/>MSR-action 3D <lb/>[28] <lb/>RGB + depth (320*240) + 20 <lb/>joints: high arm wave, horizontal <lb/>arm wave, hammer, hand catch, <lb/>forward punch, high throw, draw <lb/>X, draw tick, draw circle, hand <lb/>clap, two hand wave, side-<lb/>boxing, bend, forward kick, side <lb/>kick, jogging, tennis swing, ten-<lb/>nis serve, golf swing, pick up and <lb/>throw <lb/>10 subjects | 20 ac-<lb/>tions | 3 try ) To-<lb/>tal of 520 actions <lb/>50% Learn-<lb/>ing / 50% <lb/>Testing <lb/>UT-kinect [35] <lb/>RGB + depth (320*240) + 20 <lb/>joints: walk, sit down, stand up, <lb/>pick up, carry, throw, push, pull, <lb/>wave and clap hands <lb/>10 subject | 10 ac-<lb/>tions | 2 try ) To-<lb/>tal of 200 actions <lb/>leave-one-<lb/>out <lb/>cross-<lb/>validation <lb/>UCF-kinect [41] 15 joints: balance, climb up, <lb/>climb ladder, duck, hop, vault, <lb/>leap, run, kick, punch, twist left, <lb/>twist right , step forward, step <lb/>back, step left, step right <lb/>16 subjects | 16 ac-<lb/>tions | 5 try ) To-<lb/>tal of 1280 actions <lb/>70% Learn-<lb/>ing / 30% <lb/>Testing <lb/>Table 1: Overview of the datasets used in the experiments. <lb/> In all these datasets, a normalization step is performed in order to make <lb/> 490 <lb/> the skeletons scale-invariant. For each frame, the hip center joint is first <lb/> 491 <lb/> placed at the origin of the coordinate system. Then, a skeleton template is <lb/> 492 <lb/> taken as reference and all the other skeletons are normalized such that their <lb/> 493 <lb/></body>

			<page> 24 <lb/></page>

			<body> (a) MSR-action 3D <lb/>(b) UT-Kinect <lb/>(c) UCF-kinect <lb/>Figure 4: Examples of human actions from datsets used in our experiments: (a) &apos;hand <lb/>clap&apos; from MSR-action 3D , (b) &apos;walk&apos; from UT kinect and (c) &apos;climb ladder&apos; from UCF-<lb/>kinect. <lb/></body>

			<page> 25 <lb/></page>

			<body>body part lengths are equal to the corresponding lengths of the reference <lb/> 494 <lb/> skeleton. Each 3D joint sequence is represented as time series matrix of size <lb/> 495 <lb/> F ⇥ T with T the number of frames in the sequence and F the number <lb/> 496 <lb/> of features per frame. The number of features depends on the number of <lb/> 497 <lb/> estimated joints (60 values for Microsoft SDK skeleton and 45 for PrimeSense <lb/> 498 <lb/> NiTE skeleton). The dynamic of the activity is then captured using an <lb/> 499 <lb/> ARMA model. In this process, a dimensionality reduction is needed and best <lb/> 500 <lb/> subspace dimension &quot; d &quot; have been chosen using a 5-fold cross-validation on <lb/> 501 <lb/> the training dataset. The parameter giving the best accuracy on the training <lb/> 502 <lb/> set is kept for all experiments. <lb/> 503 <lb/> Each action is an element of the Grassmann manifold G  n⇥d  with n = m⇥J <lb/> 504 <lb/> where J represents the number of joints and d is the subspace dimension <lb/> 505 <lb/> learnt on the training data. We set m = d, while m represents the truncation <lb/> 506 <lb/> parameter of observation. <lb/> 507 <lb/> In our LTBSVM approach, we train a linear SVM on our LTB represen-<lb/> 508 <lb/> tations of points on the Grassmann manifold. We use a multi-class SVM <lb/> 509 <lb/> classifier from LibSVM library [57], where the penalty parameter C is tuned <lb/> 510 <lb/> using a 5-fold cross-validation on the training dataset. <lb/> 511 <lb/> We evaluate the performance of our approach for action recognition and <lb/> 512 <lb/> explore the latency on recognition by evaluating the trade-o↵ between accu-<lb/> 513 <lb/> racy and latency over varying number of actions. To allow a better evalua-<lb/> 514 <lb/> tion of our approach, we conducted experiments respecting those made in the <lb/> 515 <lb/> state-of-the-art approaches. We note here that other interesting datasets are <lb/> 516 <lb/> available, like TUM kitchen dataset [43] which presents challenging short and <lb/> 517 <lb/> complex actions. In our experiments we concentrated on three other datasets <lb/> 518 <lb/></body>

			<page> 26 <lb/></page>

			<body> from depth sensors (such as kinect), chosen according to the challenges they <lb/> 519 <lb/> contain, as occlusion, change of view and possibility to compare the latency. <lb/> 520 <lb/> Details of the experiments are presented in the following sections. <lb/> 521 <lb/> 5.2. MSR-Action 3D dataset <lb/> 522 <lb/> MSR-Action 3D [28] is a public dataset of 3D action captured by a depth <lb/> 523 <lb/> camera. It consists of a set of temporally segmented actions where subjects <lb/> 524 <lb/> are facing the camera and they are advised to use their right arm or leg if <lb/> 525 <lb/> an action is performed by a single limb. The background is pre-processed <lb/> 526 <lb/> clearing discontinuities and there is no interaction with objects in performed <lb/> 527 <lb/> actions. Despite of all of these facilities, it is also a challenging dataset <lb/> 528 <lb/> since many activities appear very similar due to small inter-class variation. <lb/> 529 <lb/> Several works have already been conducted on this dataset. Table 2 shows <lb/> 530 <lb/> the accuracy of our approach compared to the state-of-the-art methods. We <lb/> 531 <lb/> followed the same experimental setup as in Oreifej et al. [34] and Jiang et <lb/> 532 <lb/> al. [45], where first five actors are used for training and the rest for testing. <lb/> 533 <lb/> Our results obtained in this table correspond to four learning methods: <lb/> 534 <lb/> simple Karcher Mean (KM), one Tangent SVM (TSVM), Truncated Wrapped <lb/> 535 <lb/> Gaussian (TWG) and Local Tangent Bundle SVM (LTBSVM). Our approach <lb/> 536 <lb/> using LTBSVM achieves an accuracy of 91.21%, exceeding the best method <lb/> 537 <lb/> from the state-of-the-art proposed by Oreifej et al. [34]. We note that our <lb/> 538 <lb/> approach is based on only skeletal joint coordinates as motion features, com-<lb/> 539 <lb/> pared to other approaches, such as Oreifej et al. [34] and Wang et al. [32] <lb/> 540 <lb/> which use the depth map or depth information around joint locations. <lb/> 541 <lb/> To evaluate the e↵ect of the changing of the subspace dimensions, we <lb/> 542 <lb/> conduct several tests on MSR-Action 3D dataset with di↵erent dimensions <lb/> 543 <lb/></body>

			<page> 27 <lb/></page>

			<body> Method <lb/> accuracy % <lb/>Histograms of 3D Joints [58] <lb/>78.97 <lb/>Eigen Joints [36] <lb/>82.33 <lb/>DMM-HOG [33] <lb/>85.52 <lb/>HON4D [34] <lb/>85.80 <lb/>Random Occupancy patterns [32] <lb/>86.50 <lb/>Actionlet Ensemble [45] <lb/>88.20 <lb/>HOH4D + D  disc  [34] <lb/>88.89 <lb/>TSVM on one tangent space <lb/> 74.32 <lb/> KM <lb/> 77.02 <lb/> TWG <lb/> 84.45 <lb/> LTBSVM <lb/> 91.21 <lb/> Table 2: Recognition accuracy (in %) for the MSR-Action 3D dataset using our approach <lb/>compared to the previous approaches. <lb/> of subspaces. Figure 5 shows the variation of recognition performances with <lb/> 544 <lb/> the change of the subspace dimension. We remark that until dimension 12, <lb/> 545 <lb/> the recognition rate generally increase with the increase of the size of the <lb/> 546 <lb/> subspaces dimensions. This is expected, since a small dimension causes a <lb/> 547 <lb/> lack of information but also a big dimension of the subspace keeps noise and <lb/> 548 <lb/> brings confusion between inter-classes. We also compare in this figure, our <lb/> 549 <lb/> new introduced learning algorithm LBTSVM to TWG and KM. <lb/> 550 <lb/> To better understand the behavior of our approach according to the action <lb/> 551 <lb/> type, the confusion matrix is illustrated in Figure 6. For most actions, about <lb/> 552 <lb/> 11 classes of actions, video sequences are 100% correctly classified. <lb/> 553 <lb/> The classification error occurs if two actions are very similar, such as <lb/> 554 <lb/> &apos;horizontal arm wave&apos; and &apos;high arm wave&apos;. Besides, one of most problematic <lb/> 555 <lb/> action to classify is &apos;hammer&apos; action which is frequently confused with &apos;draw <lb/> 556 <lb/> X&apos;. The particularity of these two actions is that they start in the same <lb/> 557 <lb/> way but one finishes before the other. If we show only the first part of <lb/> 558 <lb/> &apos;draw X&apos; action and the whole sequence of &apos;hammer&apos; action we can see that <lb/> 559 <lb/></body>

			<page> 28 <lb/></page>

			<body> 0 <lb/> 2 <lb/>4 <lb/>6 <lb/>8 <lb/>10 <lb/>12 <lb/>14 <lb/>16 <lb/>0 <lb/>10 <lb/>20 <lb/>30 <lb/>40 <lb/>50 <lb/>60 <lb/>70 <lb/>80 <lb/>90 <lb/>100 <lb/> Dimension of subspaces <lb/>Accuracy (%) <lb/> GJD (KM) <lb/>GJD (TWG) <lb/>GJD (LTBSVM) <lb/>91.21 <lb/> Figure 5: Recognition rate variation with learning approach and subspace dimension. <lb/> they are very similar. The same for &apos;hand catch&apos; action which is confused <lb/> 560 <lb/> with &apos;draw circle&apos;. It is important to note that &apos;hammer&apos; action is completely <lb/> 561 <lb/> misclassified with the approach presented by Oreifej et al. [34] which presents <lb/> 562 <lb/> the second better recognition rate after our approach. <lb/> 563 <lb/> While the focus of this paper is mainly on action recognition and latency <lb/> 564 <lb/> reduction, some applications need to perform training step with a reduced <lb/> 565 <lb/> amount of data. To study the e↵ect of the amount of training dataset, we <lb/> 566 <lb/> measured how the accuracy changed as we iteratively reduced the number of <lb/> 567 <lb/> actions per class in the training dataset. Table 3 shows obtained accuracy <lb/> 568 <lb/> results with di↵erent size of training dataset. <lb/> 569 <lb/> These results show that, in contrast to approaches that use HMM which <lb/> 570 <lb/> require a large number of training data, our approach reveals robustness and <lb/> 571 <lb/> e񮽙ciency. This robustness is due to the fact that the Control Tangents, which <lb/> 572 <lb/></body>

			<page> 29 <lb/></page>

			<body> 93.33 33.33 <lb/>60.00 <lb/>6.67 <lb/>7.14 <lb/>40.00 <lb/>73.33 <lb/>92.86 <lb/>7.14 92.86 <lb/>6.67 <lb/>46.67 <lb/>7.14 92.86 6.67 <lb/>6.67 6.67 <lb/>93.33 <lb/>6.67 <lb/>13.33 <lb/>100 <lb/>100 <lb/>100 <lb/>100 <lb/>100 <lb/>13.33 <lb/>100 <lb/>100 <lb/>100 <lb/>6.67 <lb/>100 <lb/>100 <lb/>100 <lb/>86.67 <lb/> High arm wave <lb/> H ig h a r m <lb/> w a v e <lb/> Horizontal arm wave <lb/> H o r iz o n t a l a r m w a v e <lb/> Hammer <lb/> H a m m e r <lb/> Hand catch <lb/> H a n d c a t c h <lb/> Forward punch <lb/> F o r w a r d p u n c h <lb/> High throw <lb/> H ig h t h r o w <lb/> Draw X <lb/> D r a w X <lb/> Draw tick <lb/> D r a w t ic k <lb/> Draw circle <lb/> D r a w c ir c le <lb/> Hand clap <lb/> H a n d c la p <lb/> Two hand wave <lb/> T o w h a n d w a v e <lb/> Side−boxing <lb/> S id e  −  b o x in g <lb/> Bend <lb/> B e n d <lb/> Forward kick <lb/> F o r w a r d k ic k <lb/> Side kick <lb/> S id e k ic k <lb/> Jogging <lb/> J o g g in g <lb/> Tennis swing <lb/> T e n n is s w in g <lb/> Tennis serve <lb/> T e n n is s e r v e <lb/> Golf swing <lb/> G lo f s w in g <lb/> Pick up &amp; throw <lb/> P ic k u p &amp; t h r o w <lb/> Figure 6: The confusion matrix for the proposed approach on MSR-Action 3D dataset. <lb/>It is recommended to view the Figure on the screen. <lb/> play an important role in learning process, can be computed e񮽙ciently using <lb/> 573 <lb/> small number of action points per class on the manifold. <lb/> 574 <lb/> 5.3. UT-Kinect dataset <lb/> 575 <lb/> Sequences of this dataset are taken using one depth camera (kinect) in <lb/> 576 <lb/> indoor settings and their length vary from 5 to 120 frames. We use this <lb/> 577 <lb/> dataset because it contains several challenges: <lb/> 578 <lb/> • View change, where actions are taken from di↵erent views: right view, <lb/> 579 <lb/> frontal view or back view. <lb/> 580 <lb/></body>

			<page> 30 <lb/></page>

			<body> Actions <lb/> per class <lb/>Training dataset % Accuracy % <lb/>5 <lb/>37.17 <lb/>73.36 <lb/>6 <lb/>44.23 <lb/>77.64 <lb/>7 <lb/>51.13 <lb/>83.10 <lb/>8 <lb/>58.36 <lb/>84.79 <lb/>9 <lb/>65.54 <lb/>88.51 <lb/>10 <lb/>72.49 <lb/>89.18 <lb/>11 <lb/>79.95 <lb/>87.83 <lb/>12 <lb/>86.24 <lb/>88.85 <lb/>13 <lb/>91.07 <lb/>90.20 <lb/>14 <lb/>95.91 <lb/>90.54 <lb/>15 <lb/>100 <lb/>91.21 <lb/>Table 3: Recognition accuracy, obtained by our approach using LTBSVM on MSR-Action <lb/>3D dataset, with di↵erent size of training dataset. <lb/> • Significant variation in the realization of the same action: same action <lb/> 581 <lb/> is done with one hand or two hands can be used to describe the &apos;pick <lb/> 582 <lb/> up&apos; action. <lb/> 583 <lb/> • Variation in duration of actions: the mean and standard-deviation are <lb/> 584 <lb/> respectively for the whole actions 31.1 and 11.61 frames at 30 fps. <lb/> 585 <lb/> To compare our results with state-of-the-art approaches, we follow experi-<lb/> 586 <lb/> ment protocol proposed by Xia et al. [35]. The protocol is leave-one-out <lb/> 587 <lb/> cross-validation. In Table 4, we show comparison between the recognition <lb/> 588 <lb/> accuracy produced by our approach and the approach presented by Xia et <lb/> 589 <lb/> al. [35]. <lb/> 590 <lb/> This table shows the accuracy of the five least-recognized actions in UT-<lb/> 591 <lb/> kinect dataset and the five best-recognized actions. Our system performs <lb/> 592 <lb/> the worst when the action represents an interaction with an object: &apos;throw&apos;, <lb/> 593 <lb/> &apos;push&apos;, &apos;sit down&apos; and &apos;pick up&apos;. However, for the best five recognized actions, <lb/> 594 <lb/> our approach improves the recognition rate reaching 100%. These actions <lb/> 595 <lb/></body>

			<page> 31 <lb/></page>

			<body> Action <lb/> Acc % Xia et al. [35] Acc % LTBSVM <lb/>Walk <lb/>96.5 <lb/> 100 <lb/> Stand up <lb/>91.5 <lb/> 100 <lb/> Pick up <lb/>97.5 <lb/> 100 <lb/> Carry <lb/>97.5 <lb/> 100 <lb/> Wave <lb/>100 <lb/> 100 <lb/> Throw <lb/>59 <lb/>60 <lb/>Push <lb/>81.5 <lb/>65 <lb/>Sit down <lb/>91.5 <lb/>80 <lb/>Pull <lb/>92.5 <lb/>85 <lb/>Clap hands <lb/>100 <lb/>95 <lb/>Overall <lb/>90.92 <lb/>88.5 <lb/>Table 4: Recognition accuracy (per action) for the UT-kinect dataset obtained by our <lb/>approach using LTBSVM compared to Xia et al. [35]. <lb/> contain variations in view point and realization of the same action. This <lb/> 596 <lb/> means that our approach is view-invariant and it is robust to change in action <lb/> 597 <lb/> types thanks to the used learning approach. The overall accuracy of Xia et al. <lb/> 598 <lb/> [35] is better than our recognition rate. However on MSR Action3D database, <lb/> 599 <lb/> the recognition rate obtained by this approach gives only 78.97%. This can <lb/> 600 <lb/> be explained by the fact that this approach requires a large training dataset. <lb/> 601 <lb/> Especially for complex actions which a↵ect adversely the HMM classification <lb/> 602 <lb/> in case of small samples of training. <lb/> 603 <lb/> 5.4. UCF-kinect dataset <lb/> 604 <lb/> In this experiment, our approach is evaluated in terms of latency, i.e. <lb/> 605 <lb/> the ability for a rapid (low-latency) action recognition. The goal here is to <lb/> 606 <lb/> automatically determine when a su񮽙cient number of frames are observed to <lb/> 607 <lb/> permit a reliable recognition of the occurring action. For many applications, <lb/> 608 <lb/> a real challenge is to define a good compromise between &quot; making forced de-<lb/> 609 <lb/> cision &quot; on partial available frames (but potentially unreliable) and &quot; waiting &quot; <lb/> 610 <lb/></body>

			<page> 32 <lb/></page>

			<body> for the entire video sequence. <lb/> 611 <lb/> To evaluate the performance of our approach in reducing latency, we con-<lb/> 612 <lb/> ducted our experiments on UCF-kinect dataset [41]. The skeletal joint loca-<lb/> 613 <lb/> tions (15 joints) over sequences of this dataset are estimated using Microsoft <lb/> 614 <lb/> Kinect sensor and the PrimeSense NiTE. The same experimental setup as <lb/> 615 <lb/> in Ellis et al. [41] is followed. For a total of 1280 action samples contained <lb/> 616 <lb/> in this dataset, a 70% and 30% split is used for respectively training and <lb/> 617 <lb/> testing datasets. From the original dataset, new subsequences were created <lb/> 618 <lb/> by varying a parameter corresponding to the K first frames. Each new sub-<lb/> 619 <lb/> sequence was created by selecting only the first K frames from the video. For <lb/> 620 <lb/> videos shorter than K frames, the entire video is used. We compare the re-<lb/> 621 <lb/> sult obtained by our approach to those obtained by Latency Aware Learning <lb/> 622 <lb/> (LAL) method proposed by Ellis et al. [41] and other baseline algorithms: <lb/> 623 <lb/> Bag-of-Words (BoW) and Linear Chain Conditional Random Field (CRF), <lb/> 624 <lb/> also reported by Ellis et al. [41]. <lb/> 625 <lb/> As shown in Figure 7, our approach using LTBSVM clearly achieves im-<lb/> 626 <lb/> proved latency performance compared to all other baseline approaches. Anal-<lb/> 627 <lb/> ysis of these curves shows that, accuracy rates for all other approaches are <lb/> 628 <lb/> close when using small number of frames (less than 10) or a large number of <lb/> 629 <lb/> frames (more than 40). However, the di↵erence increases significantly in the <lb/> 630 <lb/> middle range. The table joint to Figure 7 shows numerical results at several <lb/> 631 <lb/> points along the curves in the figure. Thus, given only 20 frames of input, <lb/> 632 <lb/> our system achieves 74.37%, while BOW, CRF recognition rate below 50% <lb/> 633 <lb/> and LAL achieves 61.45%. <lb/> 634 <lb/> It is also interesting to notice the improvement of accuracy of 92.08% <lb/> 635 <lb/></body>

			<page> 33 <lb/></page>

			<body> 0 <lb/> 10 <lb/>20 <lb/>30 <lb/>40 <lb/>50 <lb/>60 <lb/>0 <lb/>10 <lb/>20 <lb/>30 <lb/>40 <lb/>50 <lb/>60 <lb/>70 <lb/>80 <lb/>90 <lb/>100 <lb/>Maximum frames <lb/>Accuracy (%) <lb/> LTBSVM <lb/>LAL <lb/>TWG <lb/>CRF <lb/>BoW <lb/> Approach/frames <lb/> 10 <lb/>15 <lb/>20 <lb/>25 <lb/>30 <lb/>40 <lb/>60 <lb/>LTBSVM <lb/> 21.87 <lb/> 49.37 <lb/>74.37 <lb/>86.87 <lb/>92.08 <lb/>97.29 <lb/>97.91 <lb/> TWG <lb/>18.95 <lb/>40.62 <lb/>61.45 <lb/>74.79 <lb/>82.7 <lb/>92.29 <lb/>95.62 <lb/>LAL [41] <lb/>13.91 <lb/>36.95 <lb/>64.77 <lb/>81.56 <lb/>90.55 <lb/>95.16 <lb/>95.94 <lb/>CRF [41] <lb/>14.53 <lb/>25.46 <lb/>46.88 <lb/>67.27 <lb/>80.70 <lb/>91.41 <lb/>94.06 <lb/>BOW [41] <lb/>10.7 <lb/>21.17 <lb/>43.52 <lb/>67.58 <lb/>83.20 <lb/>91.88 <lb/>94.06 <lb/> Figure 7: Accuracy vs. state-of-the-art approaches over videos truncated at varying maxi-<lb/>mum lengths. Each point of this curve shows the accuracy achieved by the classifier given <lb/>only the number of frames shown in the x-axis. <lb/> obtained by LTBSVM compared to 82.7% obtained by TWG, with maximum <lb/> 636 <lb/> frame number equal to 30. For a large number of frames, all of the methods <lb/> 637 <lb/> perform globally a good accuracy with an improvement of the ours (97.91% <lb/> 638 <lb/> comparing to 95.94% obtained by LAL proposed in Ellis et al. [41]). These <lb/> 639 <lb/> results show that our approach can recognize actions at the desired accuracy <lb/> 640 <lb/> with reducing latency. <lb/> 641 <lb/> Finally, the detail of recognition rates, when using the totality of frames <lb/> 642 <lb/> in the sequence, are shown through the confusion matrix in Figure 8. Unlike <lb/> 643 <lb/> what gives LAL, we can observe that the &apos;twist left&apos;, &apos;twist right&apos; actions are <lb/> 644 <lb/></body>

			<page> 34 <lb/></page>

			<body> not confused with each others. All classes of actions are classified with a rate <lb/> 645 <lb/> more than 93.33% which gives a lot of confidence to our proposed learning <lb/> 646 <lb/> approach. <lb/> 647 <lb/> 100 <lb/> 100 <lb/>93.33 <lb/>3.33 <lb/>100 3.33 <lb/>93.33 <lb/>100 <lb/>6.67 <lb/>3.33 <lb/>96.67 <lb/>100 <lb/>3.33 <lb/>100 <lb/>93.33 <lb/>3.33 100 <lb/>100 <lb/>3.33 <lb/>100 <lb/>100 3.33 <lb/>93.33 <lb/>3.33 <lb/>96.67 <lb/> balance <lb/> b a la n c e <lb/> climbladder <lb/> c li m b la d d e r <lb/> climbup <lb/> c li m b u p <lb/> duck <lb/> d u c k <lb/> hop <lb/> h o p <lb/> kick <lb/> k ic k <lb/> leap <lb/> le a p <lb/> punch <lb/> p u n c h <lb/> run <lb/> r u n <lb/> stepback <lb/> s t e p b a c k <lb/> stepfront <lb/> s t e p f r o n t <lb/> stepleft <lb/> s t e p le f t <lb/> stepright <lb/> s t e p r ig h t <lb/> twistleft <lb/> t w is t le f t <lb/> twistright <lb/> t w is t r ig h t <lb/> vault <lb/> v a u lt <lb/> Figure 8: The confusion matrix for the proposed method on UCF-kinect dataset. Overall <lb/>accuracy achieved 97.91%. It is recommended to view the figure on the screen. <lb/> 5.5. Discussion <lb/> 648 <lb/> Manifold representation and learning. Data representation is one of the most <lb/> 649 <lb/> important factors in the recognition approach, on which we must take a lot <lb/> 650 <lb/> of consideration. Our data representation, like many state-of-the-art man-<lb/> 651 <lb/> ifold techniques [19, 14, 21], consider the geometric space and incorporates <lb/> 652 <lb/> the intrinsic nature of the data. In our framework, which is 3D joint-based, <lb/> 653 <lb/> both geometric appearance and dynamic of human body are captured simul-<lb/> 654 <lb/></body>

			<page> 35 <lb/></page>

			<body> taneously. Furthermore, unlike the manifold approaches using silhouettes <lb/> 655 <lb/> [14, 15, 18], or directly raw pixels [22, 19], our approach use informative <lb/> 656 <lb/> geometric features, which capture useful knowledge to understand the in-<lb/> 657 <lb/> trinsic motion structure. Thanks to recent release of depth sensor, these <lb/> 658 <lb/> features are extracted and tracked along the action sequence, while classical <lb/> 659 <lb/> pixel-based manifold approaches relying on a good action localization, or on <lb/> 660 <lb/> tedious feature extraction from 2D videos like silhouettes. <lb/> 661 <lb/> In terms of learning method, we generalized a learning algorithm to work <lb/> 662 <lb/> with data points which are geometrically lying to a Grassmann manifold. <lb/> 663 <lb/> Other approaches are tested in the learning process on the manifold: one <lb/> 664 <lb/> tangent space (TSVM) and class-specific tangent spaces (TWG). In the first <lb/> 665 <lb/> one, recognition rate is low. In fact, the computation of the mean of all <lb/> 666 <lb/> actions from all classes can be inaccurate. Besides, projections on this plane <lb/> 667 <lb/> can lead to big deformations. A better solution is to operate on each class by <lb/> 668 <lb/> computing its proper tangent space, as in TWG [56] which improve TSVM <lb/> 669 <lb/> results (see Table 2). In our approach (LTBSVM), both Control Tangent <lb/> 670 <lb/> and statistics on the manifold are used. The purpose was to formulate our <lb/> 671 <lb/> learning algorithm using a discriminative parametrization which incorporate <lb/> 672 <lb/> class separation properties. The particularity of our learning model is the <lb/> 673 <lb/> incorporation of proximities relative to all Control Tangent spaces represent-<lb/> 674 <lb/> ing class clusters, instead of classifying using a function of local distances. <lb/> 675 <lb/> The results in Table 2 demonstrate that the proposed algorithm is more e񮽙-<lb/> 676 <lb/> cient in action recognition scenario when inter-variation classes is present as <lb/> 677 <lb/> a challenge. <lb/> 678 <lb/> Furthermore, the analysis of the impact of reducing the number of actions <lb/> 679 <lb/></body>

			<page> 36 <lb/></page>

			<body> in the training set on the accuracy of the classifier show robustness. Even <lb/> 680 <lb/> with a small number of actions in the training data recognition rates remain <lb/> 681 <lb/> good as demonstrated in Table 3. However it is a limitation especially for <lb/> 682 <lb/> approaches using an HMM learning because they require a large number of <lb/> 683 <lb/> training dataset. Such as Xia et al. approach [35], which gives only 78.97% <lb/> 684 <lb/> of recognition rate while performing cross subject test on MSR dataset. <lb/> 685 <lb/> Latency and Time computation. The evaluations in terms of latency have <lb/> 686 <lb/> clearly revealed the e񮽙ciency of our approach for a rapid recognition. It <lb/> 687 <lb/> is possible to recognize actions up to 95% using only 40 frames which is <lb/> 688 <lb/> a good performance comparing to state-of-the-art approaches presented in <lb/> 689 <lb/> [41]. Thus, our approach can be used for interactive systems. Particularly, <lb/> 690 <lb/> in entertainment applications to resolve the problem of lag and improve some <lb/> 691 <lb/> motion-based games. <lb/> 692 <lb/> Since the proposed approach is based on only skeletal joint coordinates, <lb/> 693 <lb/> it is simple to calculate and it needs only a small computation time. In fact, <lb/> 694 <lb/> with our current implementation written in C++, the whole recognition time <lb/> 695 <lb/> takes 0.26 sec to recognize a sequence of 60 frames. The joint extraction and <lb/> 696 <lb/> normalisation take 0.0001 sec, the Grassmann and the LTB representation <lb/> 697 <lb/> take 0.0108 sec and the prediction on SVM takes 0.251 sec. These computa-<lb/> 698 <lb/> tion time are reported on UCF dataset, with Grassmann manifold dimension <lb/> 699 <lb/> n = 540 and d = 12. We also reported the computation time needed to <lb/> 700 <lb/> recognize actions while incorporating latency on UCF dataset. Figure 9 il-<lb/> 701 <lb/> lustrates inline time recognition with time progression, after only 40 frames <lb/> 702 <lb/> the recognition is given at the 0.94 sec within 97.29% of correctness rate. <lb/> 703 <lb/> After 60 frames, in 1.3 sec the algorithm recognize correctly the action with <lb/> 704 <lb/></body>

			<page> 37 <lb/></page>

			<body>  97.91%. All the computation time experiments are lunched on a PC having <lb/> 705 <lb/>Intel Core i5-3350P (3.1 GHz) CPU, 4GB RAM and a PrimeSense camera <lb/> 706 <lb/> for skeleton extraction giving about 60 skeleton/sec. <lb/> 707 <lb/> 0 <lb/> 20 <lb/>40 <lb/>60 <lb/> Frame number <lb/> … <lb/>… <lb/>… <lb/>0 <lb/>0.66 <lb/>0.94 <lb/>1.31 <lb/> Inline 񮽙me recogni񮽙on <lb/>(secondes) <lb/>Recogni񮽙on 񮽙me: 0.26 sec <lb/> Figure 9: The computation time to perform 20 frames actions sequences is 0.26 sec by <lb/>using our approach. The computation time is given for each actions frames sequences (e.g. <lb/>0.94 sec for 40 frames). <lb/> Limitations. Our proposed approach is a 3D joint-based framework derives <lb/> 708 <lb/> a human action recognition from skeletal joint sequences. In the case of <lb/> 709 <lb/> presence of object interaction in human actions, our approach do not provides <lb/> 710 <lb/> any relevant information about objects and thus, action with and without <lb/> 711 <lb/> objects are confused. This limitation can be leveraged in future by the use <lb/> 712 <lb/> of additional features, which can be extracted from depth or color images <lb/> 713 <lb/> associated to 3D joint locations. <lb/> 714 <lb/></body>

			<page> 38 <lb/></page>

			<body> 6. Conclusion <lb/> 715 <lb/> In this paper, an e↵ective framework for modelling and recognizing hu-<lb/> 716 <lb/> man motion in the 3D skeletal joint space is proposed. In this framework, <lb/> 717 <lb/> sequence features are modeled temporally as subspaces lying to a Grassman-<lb/> 718 <lb/> nian manifold. A new learning algorithm on this manifold is then introduced. <lb/> 719 <lb/> It embeds each action, presented as a point on the manifold, in higher dimen-<lb/> 720 <lb/> sional representation providing natural separation directions. We formulated <lb/> 721 <lb/> our learning algorithm using the notion of local tangent bundles on class clus-<lb/> 722 <lb/> ters on the Grassmann manifold. The empirical results and the analysis of <lb/> 723 <lb/> the performance of our proposed approach show promising results with high <lb/> 724 <lb/> accuracies superior to 88% on three di↵erent datasets. The evaluation of <lb/> 725 <lb/> our approach in terms of accuracy/latency reveals an important ability for <lb/> 726 <lb/> a low-latency action recognition system. Obtained results show that with <lb/> 727 <lb/> minimum number of frames, it provides the highest recognition rate. <lb/> 728 <lb/> We would encourage future works to extend our approach to investigate <lb/> 729 <lb/> more challenging problems like human behaviour recognition. Finally, we <lb/> 730 <lb/> plan to use additional features from depth or color images associated to 3D <lb/> 731 <lb/> joint locations to solve the problem of human-object interaction. <lb/></body>

			<page> 732 <lb/></page>

			<listBibl> References <lb/> 733 <lb/> [1] S. Fothergill, H. Mentis, P. Kohli, S. Nowozin, Instructing people for <lb/> 734 <lb/> training gestural interactive systems, in: CHI Conference on Human <lb/> 735 <lb/> Factors in Computing Systems, New York, NY, USA, 2012, pp. 1737– <lb/> 736 <lb/> 1746. <lb/> 737 <lb/>

			<page> 39 <lb/></page>

			[2] W. Lao, J. Han, P. de With, Automatic video-based human motion <lb/> 738 <lb/> analyzer for consumer surveillance system, in: IEEE Transactions on <lb/> 739 <lb/> Consumer Electronics, Vol. 55, 2009, pp. 591–598. <lb/> 740 <lb/> [3] A. Jalal, M. Uddin, T. S. Kim, Depth video-based human activity recog-<lb/> 741 <lb/> nition system using translation and scaling invariant features for life log-<lb/> 742 <lb/> ging at smart home, in: IEEE Transactions on Consumer Electronics, <lb/> 743 <lb/> Vol. 58, 2012, pp. 863–871. <lb/> 744 <lb/> [4] R. Poppe, A survey on vision-based human action recognition, in: Image <lb/> 745 <lb/> and Vision Computing, Vol. 28, 2010, pp. 976–990. <lb/> 746 <lb/> [5] P. Turaga, R. Chellappa, V. S. Subrahmanian, O. Udrea, Machine recog-<lb/> 747 <lb/> nition of human activities: A survey, in: IEEE Transactions on Circuits <lb/> 748 <lb/> and Systems for Video Technology, Vol. 18, Piscataway, NJ, USA, 2008, <lb/> 749 <lb/> pp. 1473–1488. <lb/> 750 <lb/> [6] J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, <lb/> 751 <lb/> A. Kipman, A. Blake, Real-time human pose recognition in parts from <lb/> 752 <lb/> single depth images, in: Machine Learning for Computer Vision, Vol. <lb/> 753 <lb/> 411, 2013, pp. 119–135. <lb/> 754 <lb/> [7] C.-S. Lee, A. M. Elgammal, Modeling view and posture manifolds for <lb/> 755 <lb/> tracking, in: IEEE International Conference on Computer Vision, 2007, <lb/> 756 <lb/> pp. 1–8. <lb/> 757 <lb/> [8] Y. M. Lui, Advances in matrix manifolds for computer vision, in: Image <lb/> 758 <lb/> and Vision Computing, Vol. 30, 2012, pp. 380 – 388. <lb/> 759 <lb/>

			<page> 40 <lb/></page>

			[9] M. T. Harandi, C. Sanderson, S. Shirazi, B. C. Lovell, Kernel analysis <lb/> 760 <lb/> on grassmann manifolds for action recognition, in: Pattern Recognition <lb/> 761 <lb/> Letters, Vol. 34, 2013, pp. 1906 – 1915. <lb/> 762 <lb/> [10] M. Bregonzio, T. Xiang, S. Gong, Fusing appearance and distribution <lb/> 763 <lb/> information of interest points for action recognition, in: Pattern Recog-<lb/> 764 <lb/> nition, Vol. 45, 2012, pp. 1220 – 1234. <lb/> 765 <lb/> [11] S. O&apos;Hara, Y. M. Lui, B. A. Draper, Using a product manifold distance <lb/> 766 <lb/> for unsupervised action recognition, in: Image and Vision Computing, <lb/> 767 <lb/> Vol. 30, 2012, pp. 206 – 216. <lb/> 768 <lb/> [12] J. Aggarwal, M. Ryoo, Human activity analysis: A review, in: ACM <lb/> 769 <lb/> Computing Surveys, Vol. 43, 2011, pp. 1–43. <lb/> 770 <lb/> [13] D. Weinland, R. Ronfard, E. Boyer, A survey of vision-based methods <lb/> 771 <lb/> for action representation, segmentation and recognition, in: Computer <lb/> 772 <lb/> Vision and Image Understanding, Vol. 115, 2011, pp. 224–241. <lb/> 773 <lb/> [14] A. Veeraraghavan, A. Roy-Chowdhury, R. Chellappa, Matching shape <lb/> 774 <lb/> sequences in video with applications in human movement analysis, <lb/> 775 <lb/> in: IEEE Transactions on Pattern Analysis and Machine Intelligence, <lb/> 776 <lb/> Vol. 27, 2005, pp. 1896–1909. <lb/> 777 <lb/> [15] M. F. Abdelkader, W. Abd-Almageed, A. Srivastava, R. Chellappa, <lb/> 778 <lb/> Silhouette-based gesture and action recognition via modeling trajecto-<lb/> 779 <lb/> ries on riemannian shape manifolds, in: Computer Vision and Image <lb/> 780 <lb/> Understanding, Vol. 115, 2011, pp. 439 – 455. <lb/> 781 <lb/>

			<page> 41 <lb/></page>

			[16] D. Gong, G. Medioni, Dynamic manifold warping for view invariant <lb/> 782 <lb/> action recognition, in: IEEE International Conference on Computer Vi-<lb/> 783 <lb/> sion, Barcelona, Spain, 2011, pp. 571–578. <lb/> 784 <lb/> [17] D. Gong, G. Medioni, X. Zhao, Structured time series analysis for human <lb/> 785 <lb/> action segmentation and recognition, in: IEEE Transactions on Pattern <lb/> 786 <lb/> Analysis and Machine Intelligence, Vol. PP, 2014, pp. 1–1. <lb/> 787 <lb/> [18] P. Turaga, A. Veeraraghavan, A. Srivastava, R. Chellappa, Statistical <lb/> 788 <lb/> computations on grassmann and stiefel manifolds for image and video-<lb/> 789 <lb/> based recognition, in: IEEE Transactions on Pattern Analysis and Ma-<lb/> 790 <lb/> chine Intelligence, Vol. 33, 2011, pp. 2273–2286. <lb/> 791 <lb/> [19] P. Turaga, R. Chellappa, Locally time-invariant models of human ac-<lb/> 792 <lb/> tivities using trajectories on the grassmannian, in: IEEE Conference on <lb/> 793 <lb/> Computer Vision and Pattern Recognition, 2009, pp. 2435–2441. <lb/> 794 <lb/> [20] K. Guo, P. Ishwar, J. Konrad, Action recognition in video by sparse <lb/> 795 <lb/> representation on covariance manifolds of silhouette tunnels, in: Recog-<lb/> 796 <lb/> nizing Patterns in Signals, Speech, Images and Videos, Vol. 6388, 2010, <lb/> 797 <lb/> pp. 294–305. <lb/> 798 <lb/> [21] Y. M. Lui, J. R. Beveridge, Tangent bundle for human action recogni-<lb/> 799 <lb/> tion, in: IEEE International Conference on Automatic Face and Gesture <lb/> 800 <lb/> Recognition, 2011, pp. 97–102. <lb/> 801 <lb/> [22] Y. M. Lui, Tangent bundles on special manifolds for action recognition, <lb/> 802 <lb/> in: IEEE Transactions on Circuits and Systems for Video Technology, <lb/> 803 <lb/> Vol. 22, 2012, pp. 930–942. <lb/> 804 <lb/>

			<page> 42 <lb/></page>

			[23] S. Shirazi, M. T. Har, C. S, A. Alavi, B. C. Lovell, Clustering on grass-<lb/> 805 <lb/> mann manifolds via kernel embedding with application to action anal-<lb/> 806 <lb/> ysis, in: International Conference on Image Processing, 2012, pp. 781– <lb/> 807 <lb/> 784. <lb/> 808 <lb/> [24] M. T. Harandi, C. Sanderson, S. Shirazi, B. C. Lovell, Kernel analysis <lb/> 809 <lb/> on grassmann manifolds for action recognition, in: Pattern Recognition <lb/> 810 <lb/> Letters, Vol. 34, 2013, pp. 1906 – 1915. <lb/> 811 <lb/> [25] J. Gall, A. Yao, L. Van Gool, 2D action recognition serves 3d human <lb/> 812 <lb/> pose estimation, in: European Conference on Computer Vision, Vol. <lb/> 813 <lb/> 6313, 2010, pp. 425–438. <lb/> 814 <lb/> [26] L. Chen, H. Wei, J. Ferryman, A survey of human motion analysis using <lb/> 815 <lb/> depth imagery, in: Pattern Recognition Letters, Vol. 34, 2013, pp. 1995 <lb/> 816 <lb/> – 2006. <lb/> 817 <lb/> [27] M. Ye, Q. Zhang, L. Wang, J. Zhu, R. Yang, J. Gall, A survey on human <lb/> 818 <lb/> motion analysis from depth data, in: Time-of-Flight and Depth Imaging. <lb/> 819 <lb/> Sensors, Algorithms, and Applications, Vol. 8200, 2013, pp. 149–187. <lb/> 820 <lb/> [28] W. Li, Z. Zhang, Z. Liu, Action recognition based on a bag of 3D <lb/> 821 <lb/> points, in: IEEE Conference on Computer Vision and Pattern Recogni-<lb/> 822 <lb/> tion Workshops, 2010, pp. 9–14. <lb/> 823 <lb/> [29] B. Ni, G. Wang, P. Moulin, Rgbd-hudaact: A color-depth video database <lb/> 824 <lb/> for human daily activity recognition, in: International Conference on <lb/> 825 <lb/> Computer Vision Workshops, 2011, pp. 1147–1153. <lb/> 826 <lb/>

			<page> 43 <lb/></page>

			[30] L. Xia, J. Aggarwal, Spatio-temporal depth cuboid similarity feature <lb/> 827 <lb/> for activity recognition using depth camera, in: IEEE Conference on <lb/> 828 <lb/> Computer Vision and Pattern Recognition, 2013, pp. 2834–2841. <lb/> 829 <lb/> [31] A. W. Vieira, E. R. Nascimento, G. L. Oliveira, Z. Liu, M. F. Campos, <lb/> 830 <lb/> STOP: Space-time occupancy patterns for 3D action recognition from <lb/> 831 <lb/> depth map sequences, in: Progress in Pattern Recognition, Image Anal-<lb/> 832 <lb/> ysis, Computer Vision, and Applications, Vol. 7441, 2012, pp. 252–259. <lb/> 833 <lb/> [32] J. Wang, Z. Liu, J. Chorowski, Z. Chen, Y. Wu, Robust 3D action <lb/> 834 <lb/> recognition with random occupancy patterns, in: European Conference <lb/> 835 <lb/> on Computer Vision, 2012, pp. 872–885. <lb/> 836 <lb/> [33] X. Yang, C. Zhang, Y. Tian, Recognizing actions using depth motion <lb/> 837 <lb/> maps-based histograms of oriented gradients, in: international confer-<lb/> 838 <lb/> ence on ACM Multimedia, New York, NY, USA, 2012, pp. 1057–1060. <lb/> 839 <lb/> [34] O. Oreifej, Z. Liu, Hon4d: Histogram of oriented 4d normals for activity <lb/> 840 <lb/> recognition from depth sequences, in: IEEE Conference on Computer <lb/> 841 <lb/> Vision and Pattern Recognition, Washington, DC, USA, 2013, pp. 716– <lb/> 842 <lb/> 723. <lb/> 843 <lb/> [35] L. Xia, C.-C. Chen, J. K. Aggarwal, View invariant human action recog-<lb/> 844 <lb/> nition using histograms of 3D joints, in: Computer Vision and Pattern <lb/> 845 <lb/> Recognition Workshops, 2012, pp. 20–27. <lb/> 846 <lb/> [36] X. Yang, Y. Tian, Eigenjoints based action recognition using naive bayes <lb/> 847 <lb/> nearest neighbor, in: Computer Vision and Pattern Recognition Work-<lb/> 848 <lb/> shops, 2012, pp. 14–19. <lb/> 849 <lb/>

			<page> 44 <lb/></page>

			[37] T. Giorgino, Computing and visualizing dynamic time warping align-<lb/> 850 <lb/> ments in R: The dtw package, in: Journal of Statistical Softwar, Vol. 31, <lb/> 851 <lb/> 2009, p. 1–24. <lb/> 852 <lb/> [38] M. Reyes, G. Dominguez, S. Escalera, Featureweighting in dynamic <lb/> 853 <lb/> timewarping for gesture recognition in depth data, in: IEEE Interna-<lb/> 854 <lb/> tional Conference on Computer Vision Workshops, 2011, pp. 1182–1188. <lb/> 855 <lb/> [39] S. Sempena, N. Maulidevi, P. Aryan, Human action recognition using <lb/> 856 <lb/> Dynamic Time Warping, in: International Conference on Electrical En-<lb/> 857 <lb/> gineering and Informatics, 2011, pp. 1–5. <lb/> 858 <lb/> [40] M. Bautista, A. Hernndez-Vela, V. Ponce, X. Perez-Sala, X. Bar, O. Pu-<lb/> 859 <lb/> jol, C. Angulo, S. Escalera, Probability-based dynamic time warping for <lb/> 860 <lb/> gesture recognition on RGB-D data, in: Advances in Depth Image Anal-<lb/> 861 <lb/> ysis and Applications, Vol. 7854, 2013, pp. 126–135. <lb/> 862 <lb/> [41] C. Ellis, S. Z. Masood, M. F. Tappen, J. J. Laviola, Jr., R. Sukthankar, <lb/> 863 <lb/> Exploring the trade-o↵ between accuracy and observational latency in <lb/> 864 <lb/> action recognition, in: International Journal of Computer Vision, Vol. <lb/> 865 <lb/> 101, 2013, pp. 420–436. <lb/> 866 <lb/> [42] M. Barnachon, S. Bouakaz, B. Boufama, E. Guillou, Ongoing human ac-<lb/> 867 <lb/> tion recognition with motion capture, in: Pattern Recognition, Vol. 47, <lb/> 868 <lb/> 2014, pp. 238 – 247. <lb/> 869 <lb/> [43] M. Tenorth, J. Bandouch, M. Beetz, The TUM kitchen data set of every-<lb/> 870 <lb/> day manipulation activities for motion tracking and action recognition, <lb/> 871 <lb/>

			<page> 45 <lb/></page>

			in: International Conference on Computer Vision Workshops, 2009, pp. <lb/> 872 <lb/> 1089–1096. <lb/> 873 <lb/> [44] S. Azary, A. Savakis, A spatiotemporal descriptor based on radial dis-<lb/> 874 <lb/> tances and 3D joint tracking for action classification, in: IEEE Interna-<lb/> 875 <lb/> tional Conference on Image Processing, 2012, pp. 769–772. <lb/> 876 <lb/> [45] J. Wang, Z. Liu, Y. Wu, J. Yuan, Mining actionlet ensemble for action <lb/> 877 <lb/> recognition with depth cameras, in: IEEE Conference on Computer <lb/> 878 <lb/> Vision and Pattern Recognition, 2012, pp. 1290–1297. <lb/> 879 <lb/> [46] S. Althloothi, M. H. Mahoor, X. Zhang, R. M. Voyles, Human activity <lb/> 880 <lb/> recognition using multi-features and multiple kernel learning, in: Pat-<lb/> 881 <lb/> tern Recognition, Vol. 47, 2014, pp. 1800 – 1812. <lb/> 882 <lb/> [47] M. Devanne, H. Wannous, S. Berretti, P. Pala, M. Daoudi, <lb/> 883 <lb/> A. Del Bimbo, Space-time pose representation for 3D human action <lb/> 884 <lb/> recognition, in: Workshop on Social Behaviour Analysis ICIAP, Vol. <lb/> 885 <lb/> 8158, 2013, pp. 456–464. <lb/> 886 <lb/> [48] S. Azary, A. Savakis, Grassmannian sparse representations and motion <lb/> 887 <lb/> depth surfaces for 3D action recognition, in: IEEE Conference on Com-<lb/> 888 <lb/> puter Vision and Pattern Recognition Workshops, 2013, pp. 492–499. <lb/> 889 <lb/> [49] X. Zhang, Y. Yang, L. Jiao, F. Dong, Manifold-constrained coding and <lb/> 890 <lb/> sparse representation for human action recognition, in: Pattern Recog-<lb/> 891 <lb/> nition, Vol. 46, 2013, pp. 1819 – 1831. <lb/> 892 <lb/> [50] R. Li, P. Turaga, A. Srivastava, R. Chellappa, Di↵erential geometric <lb/> 893 <lb/>

			<page> 46 <lb/></page>

			representations and algorithms for some pattern recognition and com-<lb/> 894 <lb/> puter vision problems, in: Pattern Recognition Letters, Vol. 43, 2014, <lb/> 895 <lb/> pp. 3 – 16. <lb/> 896 <lb/> [51] H. Wang, C. Yuan, G. Luo, W. Hu, C. Sun, Action recognition using <lb/> 897 <lb/> linear dynamic systems, in: Pattern Recognition, Vol. 46, 2013, pp. 1710 <lb/> 898 <lb/> – 1718. <lb/> 899 <lb/> [52] G. Doretto, A. Chiuso, Y. N. Wu, S. Soatto, Dynamic textures, in: <lb/> 900 <lb/> International Journal of Computer Vision, Vol. 51, 2003, pp. 91–109. <lb/> 901 <lb/> [53] A. Bissacco, A. Chiuso, Y. Ma, S. Soatto, Recognition of human gaits, <lb/> 902 <lb/> in: IEEE Conference on Computer Vision and Pattern Recognition, <lb/> 903 <lb/> Vol. 2, 2001, pp. 52–57. <lb/> 904 <lb/> [54] A. Edelman, T. A. Arias, S. T. Smith, The geometry of algorithms with <lb/> 905 <lb/> orthogonality constraints, in: SIAM Journal on Matrix Analysis and <lb/> 906 <lb/> Applications, Vol. 20, 1998, pp. 303–353. <lb/> 907 <lb/> [55] A. Srivastava, E. Klassen, S. Joshi, I. Jermyn, Shape analysis of elastic <lb/> 908 <lb/> curves in euclidean spaces, in: IEEE Transactions on Pattern Analysis <lb/> 909 <lb/> and Machine Intelligence, Vol. 33, 2011, pp. 1415–1428. <lb/> 910 <lb/> [56] S. Kurtek, A. Srivastava, E. Klassen, Z. Ding, Statistical modeling of <lb/> 911 <lb/> curves using shapes and related features, in: Journal of the American <lb/> 912 <lb/> Statistical Association, Vol. 107, 2012, pp. 1152–1165. <lb/> 913 <lb/> [57] C.-C. Chang, C.-J. Lin, LIBSVM: A library for support vector machines, <lb/> 914 <lb/> in: ACM Transactions on Intelligent Systems and Technology, Vol. 2, <lb/> 915 <lb/> 2011, pp. 1–27. <lb/> 916 <lb/>

			<page> 47 <lb/></page>

			[58] L. Xia, C.-C. Chen, J. K. Aggarwal, View invariant human action recog-<lb/> 917 <lb/> nition using histograms of 3D joints., in: IEEE Conference on Computer <lb/> 918 <lb/> Vision and Pattern Recognition Workshops, 2012, pp. 20–27. <lb/> 919 <lb/></listBibl>

			<page> 48 </page>


	</text>
</tei>
