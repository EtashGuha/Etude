## Project reference

To cite this work, you can simply refer to the github project:

```
GROBID (2008-2019) <https://github.com/kermitt2/grobid>
```

(please do not include a particular person name to emphasize the project and tool!)

Here's a BibTeX entry:

```
@misc{GROBID, 
    title = {GROBID}, 
    howpublished = {\url{https://github.com/kermitt2/grobid}}, 
    publisher = {GitHub},
    year = {2008 --- 2019},
    note = {[Online; accessed 12-February-2019]},
    commit = {6d487aae44dff4d2d8e5e2b15ad876561e9dceda}
}
```

## Presentations on Grobid

[GROBID in 30 slides](grobid-04-2015.pdf) (2015).

[GROBID in 20 slides](GROBID.pdf) (2012).

## Papers on Grobid

P. Lopez. [GROBID: Combining Automatic Bibliographic Data Recognition and Term Extraction for Scholarship Publications](https://lekythos.library.ucy.ac.cy/bitstream/handle/10797/14013/ECDL069.pdf?sequence=1). Proceedings of the 13th European Conference on Digital Library (ECDL), Corfu, Greece, 2009.

P. Lopez. Automatic Extraction and Resolution of Bibliographical References in Patent Documents. P. Lopez. First Information Retrieval Facility Conference (IRFC), Vienna, May 2010. LNCS 6107, pp. 120-135. Springer, Heidelberg (2010).

Joseph Boyd. [Automatic Metadata Extraction The High Energy Physics Use Case](https://preprints.cern.ch/record/2039361/files/CERN-THESIS-2015-105.pdf). Master Thesis, EPFL, Switzerland, 2015. 

## Evalution

M. Lipinski, K. Yao, C. Breitinger, J. Beel, and B. Gipp, [Evaluation of Header Metadata Extraction Approaches and Tools for Scientific PDF Documents](http://docear.org/papers/Evaluation_of_Header_Metadata_Extraction_Approaches_and_Tools_for_Scientific_PDF_Documents.pdf), in Proceedings of the 13th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL), Indianapolis, IN, USA, 2013. 

Phil Gooch and Kris Jack, [How well does Mendeleyâ€™s Metadata Extraction Work?](https://krisjack.wordpress.com/2015/03/12/how-well-does-mendeleys-metadata-extraction-work/), 2015

[Meta-eval](https://github.com/allenai/meta-eval)

Tkaczyk, D., Collins, A., Sheridan, P., & Beel, J. Evaluation and Comparison of Open Source Bibliographic Reference Parsers: A Business Use Case. [arXiv:1802.01168](https://arxiv.org/pdf/1802.01168), 2018.

## Articles on CRF for bibliographical extraction

Accurate Information Extraction from Research Papers using Conditional Random Fields. Fuchun Peng and Andrew McCallum. Proceedings of Human Language Technology Conference and North American Chapter of the Association for Computational Linguistics (HLT-NAACL), 2004.

Isaac G. Councill, C. Lee Giles, Min-Yen Kan. (2008) ParsCit: An open-source CRF reference string parsing package. In Proceedings of the Language Resources and Evaluation Conference (LREC), Marrakesh, Morrocco. 

## Other similar Open Source tools

CiteSeerX page on [Scholarly Information Extraction](http://csxstatic.ist.psu.edu/about/scholarly-information-extraction) which list many tools and related information. 

[parsCit](http://wing.comp.nus.edu.sg/parsCit)

[CERMINE](https://github.com/CeON/CERMINE)

[Metatagger](https://github.com/iesl/rexa1-metatagger)

[BILBO](https://github.com/OpenEdition/bilbo)