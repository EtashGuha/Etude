                  Pr eface iv     Matrices and Gaussian Elimination  . Introduction  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . The Geometry of Linear Equations  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . An Example of Gaussian Elimination    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . Matrix Notation and Matrix Multiplication .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . Triangular Factors and Row Exchanges    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . Inverses and Transposes .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . Special Matrices and Applications   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  Review Exercises   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .      Vector Spaces  . Vector Spaces and Subspaces  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . Solving Ax   and Ax  b .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . Linear Independence Basis and Dimension  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    . The Four Fundamental Subspaces    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    . Graphs and Networks  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    . Linear Transformations  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    Review Exercises   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .        Orthogonality  . Orthogonal Vectors and Subspaces  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    . Cosines and Projections onto Lines .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    . Projections and Least Squares    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    . Orthogonal Bases and GramSchmidt    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    . The Fast Fourier Transform  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    Review Exercises   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .          Determinants  . Introduction  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    . Properties of the Determinant .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    . Formulas for the Determinant .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    . Applications of Determinants .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    Review Exercises   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .        Eigenvalues and Eigenvectors  . Introduction  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    . Diagonalization of a Matrix .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    . Difference Equations and Powers A  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    . Differential Equations and e  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    . Complex Matrices .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    . Similarity Transformations  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    Review Exercises   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .        Positive Definite Matrices  . Minima Maxima and Saddle Points  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    . Tests for Positive Definiteness   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    . Singular Value Decomposition   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    . Minimum Principles    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    . The Finite Element Method .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .        Computations with Matrices  . Introduction  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    . Matrix Norm and Condition Number  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    . Computation of Eigenvalues   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    . Iterative Methods for Ax  b .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .        Linear Programming and Game Theory  . Linear Inequalities    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    . The Simplex Method   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    . The Dual Problem .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    . Network Models    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    . Game Theory   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    A   Intersection Sum and Product of Spaces  A.   The Intersection of Two Vector Spaces  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    A.   The Sum of Two Vector Spaces .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    A.   The Cartesian Product of Two Vector Spaces .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    A.   The Tensor Product of Two Vector Spaces  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    A.   The Kronecker Product A  B of Two Matrices    .  .  .  .  .  .  .  .  .  .  .  .  .     B The Jordan Form  C   Matrix Factorizations  D   Glossary A Dictionary for Linear Algebra  E   MATLAB Teaching Codes  F    Linear Algebra in a Nutshell                                                                            Revising this textbook has been a special challenge for a very nice reason.  So many people have read this book and taught from it and even loved it. The spirit of the book could never change. This text was written to help our teaching of linear algebra keep up with the enormous importance of this subjectwhich just continues to grow. One step was certainly possible and desirable to add new problems . Teaching for all these years required hundreds of new exam questions especially with quizzes going onto the web. I think you will approve of the extended choice of problems. The questions are still a mixture of explain and compute the two complementary approaches to learning this beautiful subject. I personally believe that many more people need linear algebra than calculus.  Isaac Newton might not agree!  But he isnt teaching mathematics in the st century and maybe he wasnt a great teacher but we will give him the benefit of the doubt.  Cer tainly the laws of physics are well expressed by differential equations.  Newton needed calculusquite right.  But the scope of science and engineering and management and life is now so much wider and linear algebra has moved into a central place. May I say a little more because many universities have not yet adjusted the balance toward linear algebra.  Working with curved lines and curved surfaces the first step is always to linearize .   Replace the curve by its tangent line  fit the surface by a plane and the problem becomes linear.  The power of this subject comes when you have ten variables or  variables instead of two. You might think I am exaggerating to use the word beautiful for a basic course in mathematics.  Not at all.  This subject begins with two vectors v and w  pointing in different directions.  The key step is to take their linear combinations .  We multiply to get  v and  w  and we add to get the particular combination  v   w .  That new vector is in the same plane as v and w .  When we take all combinations we are filling in the whole plane.  If I draw v and w on this page their combinations cv  dw fill the page and beyond but they dont go up from the page. In the language of linear equations I can solve cv  dw  b exactly when the vector b lies in the same plane as v and w .  Matrices I will keep going a little more to convert combinations of threedimensional vectors into linear algebra. If the vectors are v         and w          put them into the columns of a matrix  matrix                    . To find combinations of those columns  multiply  the matrix by a vector  c  d   Linear combinations cv  dw                    c d   c           d          . Those combinations fill a vector space . We call it the column space of the matrix. For these two columns that space is a plane.  To decide if b         is on that plane we have three components to get right. So we have three equations to solve                    c d            means c  d    c   d    c   d   . I leave the solution to you.  The vector b         does lie in the plane of v and w . If the  changes to any other number  then b wont lie in the planeit will not be a combination of v and w  and the three equations will have no solution. Now I can describe the first part of the book  about linear equations Ax  b .  The matrix A has n columns and m rows. Linear algebra moves steadily to n vectors in m dimensional space .  We still want combinations of the columns in the column space. We still get m equations to produce b one for each row.  Those equations may or may not have a solution. They always have a leastsquares solution. The interplay of columns and rows is the heart of linear algebra. Its not totally easy but its not too hard. Here are four of the central ideas .  The column space all combinations of the columns. .  The row space all combinations of the rows. .  The rank the number of independent columns or rows. . Elimination the good way to find the rank of a matrix. I will stop here so you can start the course.  Web Pages It may be helpful to mention the web pages connected to this book. So many messages come back  with suggestions  and  encouragement  and I  hope you  will  make free  use of everything.  You can directly access httpweb.mit.edu.   which is continually updated for the course that is taught every semester.  Linear algebra is also on MITs OpenCourseWare site httpocw.mit.edu  where . became exceptional by including videos of the lectures which you definitely dont have to watch....  Here is a part of what is available on the web .  Lecture schedule and current homeworks and exams with solutions. .  The goals of the course and conceptual questions. .  Interactive Java demos audio is now included for eigenvalues. .  Linear Algebra Teaching Codes and MATLAB problems. .  Videos of the complete course taught in a real classroom. The course page has become a valuable link to the class and a resource for the students. I am very optimistic about the potential for graphics with sound.  The bandwidth for voiceover is low and FlashPlayer is freely available.  This offers a quick review with active  experiment  and  the  full  lectures  can  be  downloaded.   I  hope  professors  and students worldwide will find these web pages helpful.  My goal is to make this book as useful as possible with all the course material I can provide. Other Supporting Materials Student Solutions Manual  The Student Solutions Manual provides solutions to the oddnumbered problems in the text. Instructors Solutions Manual  The Instructors Solutions Man ual has teaching notes for each chapter and solutions to all of the problems in the text. Structure of the Course The two fundamental problems are Ax  b and Ax   x for square matrices A . The first problem Ax  b has a solution when A has independent columns .  The second problem Ax   x looks for independent eigenvectors .  A crucial part of this course is to learn what independence means. I believe that most of us learn first from examples. You can see that A                             does not have independent columns. Column  plus column  equals column .  A wonderful theorem of linear algebra says that the three rows are not independent either. The third row must lie in the same plane as the first two rows. Some combination of rows  and  will produce row . You might find that combination quickly I didnt.  In the end I had to use elimination to discover that the right combination uses  times row  minus row . Elimination is the simple and natural way to understand a matrix by producing a lot of zero entries. So the course starts there. But dont stay there too long! You have to get from combinations of the rows to independence of the rows to dimension of the row space. That is a key goal to see whole spaces of vectors the row space and the column space and the nullspace . A further goal is to understand how the matrix acts . When A multiplies x it produces the new vector Ax . The whole space of vectors movesit is transformed by A . Special transformations come from particular matrices and those are the foundation stones of linear algebra  diagonal matrices orthogonal matrices triangular matrices symmetric matrices. The eigenvalues of those matrices are special too.  I think  by  matrices provide terrific examples of the information that eigenvalues  can give.  Sections . and . are worth careful reading to see how Ax   x is useful.  Here is a case in which small matrices allow tremendous insight. Ov erall the beauty of linear algebra is seen in so many different ways .  Visualization. Combinations of vectors. Spaces of vectors. Rotation and reflection and projection of vectors. Perpendicular vectors. Four fundamental subspaces. .  Abstraction. Independence  of  vectors.   Basis  and  dimension  of  a  vector  space. Linear transformations. Singular value decomposition and the best basis. .  Computation. Elimination  to  produce  zero  entries.   GramSchmidt  to  produce orthogonal vectors. Eigenvalues to solve differential and difference equations. .  Applications. Leastsquares solution when Ax  b has too many equations.  Dif ference equations approximating differential equations. Markov probability matrices the basis for Google!. Orthogonal eigenvectors as principal axes and more.... To go further with those applications may I mention the books published by Wellesley Cambridge Press.  They are all linear algebra in disguise applied to signal processing and partial differential equations and scientific computing and even GPS. If you look at httpwww.wellesleycambridge.com  you will see part of the reason that linear algebra is so widely used. After this preface the book will speak for itself.  You will see the spirit right away. The emphasis is on understanding I try to explain rather than to deduce .   This is a book about real mathematics not endless drill.  In class I am constantly working with examples to teach what students need.  Acknowledgments I enjoyed writing this book and I certainly hope you enjoy reading it. A big part of the pleasure comes from working with friends.  I had wonderful help from Brett Coonley and Cordula Robinson and Erin Maneri.  They created the L  T E X files and drew all the figures. Without Bretts steady support I would never have completed this new edition. Earlier help with the Teaching Codes came from Steven Lee and Cleve Moler. Those follow the steps described in the book MATLAB and Maple and Mathematica are faster for  large  matrices.   All  can  be  used   optionally   in  this  course.   I  could  have  added Factorization to that list above as a fifth avenue to the understanding of matrices L U P  luA for linear equations Q R  qrA to make the columns orthogonal S E  eigA to find eigenvectors and eigenvalues. In giving thanks I never forget the first dedication of this textbook years ago.  That was a special chance to thank my parents for so many unselfish gifts.  Their example is an inspiration for my life. And I thank the reader too hoping you like this book. Gilbert Strang    .    Introduction This book begins with the central problem of linear algebra solving linear equations . The most important ease and the simplest is when the number of unknowns equals the number of equations. We have n equations in n unknowns  starting with n   Two equations  x   y   Two unknowns  x   y   .  The unknowns are x and y .  I want to describe two ways elimination and determinants  to solve these equations.  Certainly x and y are determined by the numbers      . The question is how to use those six numbers to solve the system. . Elimination Subtract  times the first equation from the second equation.  This eliminates x from the second equation. and it leaves one equation for y   equation      equation     y    .  Immediately we know y  . Then x comes from the first equation  x   y   Backsubstitution  x           gives x    .  Proceeding carefully we cheek that x and y also solve the second equation.  This should work and it does  times  x    plus  times  y   equals . . Determinants The solution y   depends completely on those six numbers in the equations. There most be a formula for y and also x  It is a ratio of determinants and I hope you will allow me to write it down directly y                                                          .   That could seem a little mysterious unless you already know about  by  determi nants. They gave the same answer y   coming from the same ratio of   to  . If we stay with determinants which we dont plan to do there will be a similar formula to compute the other unknown x  x                                                          .  Let me compare those two approaches  looking ahead to real problems when n is much larger  n   is a very moderate size in scientific computing. The truth is that direct use of the determinant formula for  equations would be a total disaster.  It would use the million numbers on the left sides correctly but not efficiently.  We will find that formula Cramers Rule in Chapter  but we want a good method to solve  equations in Chapter . That good method is Gaussian Elimination .  This is the algorithm that is constantly used to solve large systems of equations.  From the examples in a textbook  n   is close to the upper limit on the patience of the author and reader too might not see much difference. Equations  and  used essentially the same steps to find y  . Certainly x came faster by the backsubstitution in equation  than the ratio in .  For larger n there is absolutely no question.  Elimination wins and this is even the best way to compute determinants. The idea of elimination is deceptively simpleyou will master it after a few exam ples.  It will become the basis for half of this book simplifying a matrix so that we can understand it.  Together with the mechanics of the algorithm we want to explain four deeper aspects in this chapter. They are .  Linear equations lead to geometry of planes .   It is not easy to visualize a nine dimensional plane in tendimensional space. It is harder to see ten of those planes intersecting at the solution to ten equationsbut somehow this is almost possible. Our  example  has  two  lines  in  Figure  .  meeting  at  the  point  x  y         . Linear algebra moves that picture into ten dimensions where the intuition has to imagine the geometry and gets it right .  We move to matrix notation  writing the n unknowns as a vector x and the n equa tions as Ax  b .  We multiply A by elimination matrices to reach an upper trian gular matrix U .  Those steps factor A into L times U  where L is lower triangular. I will write down A and its factors for our example and explain them at the right time Factorization A                              L times U .                                                         First we have to introduce matrices and vectors and the rules for multiplication. Every matrix has a transpose A  . This matrix has an inverse A   . .  In  most  cases  elimination  goes  forward  without  difficulties.   The  matrix  has  an inverse and the system Ax  b has one solution.  In exceptional cases the method will break down either the equations were written in the wrong order which is easily fixed by exchanging them or the equations dont have a unique solution. That singular case will appear if  replaces  in our example Singular case Two parallel lines  x   y    x   y   .  Elimination still innocently subtracts  times the first equation from the second. But look at the result!  equation      equation       . This singular case has no solution . Other singular cases have infinitely many solu tions .  Change  to  in the example and elimination will lead to   .  Now y can have any value  When elimination breaks down we want to find every possible solution. .  We need a rough count of the number of elimination steps required to solve a sys tem of size n .  The computing cost often determines the accuracy in the model.  A hundred equations require a third of a million steps multiplications and subtrac tions.  The computer can do those quickly  but not many trillions.  And already after a million steps roundoff error could be significant.  Some problems are sen sitive others are not.  Without trying for full detail we want to see large systems that arise in practice and how they are actually solved. The final result of this chapter will be an elimination algorithm that is about as effi cient as possible.  It is essentially the algorithm that is in constant use in a tremendous variety of applications. And at the same time understanding it in terms of matrices the coefficient matrix A  the matrices E for elimination and P for row exchanges and the  final factors L and U is an essential foundation for the theory.  I hope you will enjoy this book and this course. . The Geometry of Linear Equations The way to understand this subject is by example. We begin with two extremely humble equations  recognizing  that  you  could  solve  them  without  a  course  in  linear  algebra. Nevertheless I hope you will give Gauss a chance  x  y   x  y   . We can look at that system by rows or by columns . We want to see them both. The  first  approach  concentrates  on  the  separate  equations  the rows .   That  is  the most familiar and in two dimensions we can do it quickly.  The equation  x  y   is represented by a straight line in the x  y plane.  The line goes through the points x   y   and x     y   and also through      and all intermediate points. The second equation x  y   produces a second line Figure .a. Its slope is dy  dx    and it crosses the first line at the solution. The point of intersection lies on both lines.  It is the only solution to both equations. That point x   and y   will soon be found by elimination.                                                                    The second approach looks at the columns of the linear system.  The two separate equations are really one vector equation  Column form x      y           .   The  problem  is to  find  the  combination  of  the  column  vectors  on  the  left  side  that produces the vector on the right side .  Those vectors      and       are represented by the bold lines in Figure .b.  The unknowns are the numbers x and y that multiply the column vectors.  The whole idea can be seen in that figure where  times column  is added to  times column .  Geometrically this produces a famous parallelogram. Algebraically it produces the correct vector       on the right side of our equations. The column picture confirms that x   and y  . More time could be spent on that example but I would rather move forward to n  . Three equations are still manageable and they have much more variety Three planes  u  v  w    u   v      u   v   w   .  Again we can study the rows or the columns and we start with the rows. Each equation describes a plane in three dimensions. The first plane is  u  v  w   and it is sketched in Figure ..  It contains the points         and        and        . It is determined by any three of its pointsprovided they do not lie on a line.                               Changing  to   the plane  u  v  w   would be parallel to this one . It contains        and        and         twice as far from the originwhich is the center point u   v   w  . Changing the right side moves the plane parallel to itself and the plane  u  v  w   goes through the origin.  The second plane is  u   v   .  It is drawn vertically because w can take any value.  The coefficient of w is zero but this remains a plane in space.  The equation  u   or even the extreme case u   would still describe a plane. The figure shows the intersection of the second plane with the first.  That intersection is a line. In three dimensions a line requires two equations  in n dimensions it will require n  . Finally the third plane intersects this line in a point. The plane not drawn represents the third equation   u   v   w   and it crosses the line at u   v   w  . That triple intersection point        solves the linear system. How  does  this  row  picture  extend  into n dimensions?   The n equations  will  con tain n unknowns.  The first equation still determines a plane.  It is no longer a two dimensional plane in space somehow it has dimension n  .  It must be flat and extremely thin within n dimensional space although it would look solid to us. If time is the fourth dimension then the plane t   cuts through fourdimensional space and produces the threedimensional universe we live in or rather the universe as it was at t  . Another plane is z   which is also threedimensional it is the ordinary x  y plane taken over all time. Those threedimensional planes will intersect! They share the ordinary x  y plane at t  .  We are down to two dimensions  and the next plane leaves a line. Finally a fourth plane leaves a single point. It is the intersection point of  planes in  dimensions and it solves the  underlying equations. I will be in trouble if that example from relativity goes any further.  The point is that linear algebra can operate with any number of equations. The first equation produces an  n    dimensional plane in n dimensions The second plane intersects it we hope in a smaller set of dimension n  . Assuming all goes well every new plane every new equation reduces the dimension by one.  At the end when all n planes are accounted for the intersection has dimension zero.  It is a point  it lies on all the planes and its coordinates satisfy all n equations. It is the solution! Column Vectors and Linear Combinations We turn to the columns. This time the vector equation the same equation as  is Column form u            v            w                      b .  Those are threedimensional column vectors . The vector b is identified with the point whose coordinates are      . Every point in threedimensional space is matched to a vector and vice versa. That was the idea of Descartes who turned geometry into algebra by working with the coordinates of the point.  We can write the vector in a column or we can list its components as b           or we can represent it geometrically by an arrow from the origin. You can choose the arrow  or the point  or the three numbers . In six dimensions it is probably easiest to choose the six numbers.   We use parentheses and commas when the components are listed horizontally and square brackets with no commas when a column vector is printed vertically.   What really matters is addition of vectors and multiplication by a scalar a number. In Figure .a you see a vector addition component by component Vector addition                                          . In the righthand figure there is a multiplication by  and if it had been   the vector                                                                               would have gone in the reverse direction Multiplication by scalars                                             . Also in the righthand figure is one of the central ideas of linear algebra.  It uses both of the basic operations vectors are multiplied by numbers and then added . The result is called a linear combination  and this combination solves our equation Linear combination                                              . Equation  asked for multipliers u  v  w that produce the right side b .  Those numbers are u    v    w   .  They give the correct combination of the columns.  They also gave the point        in the row picture where the three planes intersect.  Our true goal is to look beyond two or three dimensions into n dimensions.  With n equations in n unknowns there are n planes in the row picture.  There are n vectors in the column picture plus a vector b on the right side. The equations ask for a linear com bination of the n columns that equals b .  For certain equations that will be impossible. Paradoxically the way to understand the good case is to study the bad one.  Therefore we look at the geometry exactly when it breaks down in the singular case . Ro w picture  Intersection of planes Column picture  Combination of columns The Singular Case Suppose we are again in three dimensions and the three planes in the row picture do not intersect .  What can go wrong?  One possibility is that two planes may be parallel.  The equations  u  v  w   and  u   v   w   are inconsistentand parallel planes give no solution Figure .a shows an end view.   In two dimensions  parallel lines are the only possibility for breakdown.  But three planes in three dimensions can be in trouble without being parallel.            The most common difficulty is shown in Figure .b.  From the end view the planes form a triangle. Every pair of planes intersects in a line and those lines are parallel. The third plane is not parallel to the other planes but it is parallel to their line of intersection. This corresponds to a singular system with b          No solution as in Figure .b u  v  w    u   w    u  v   w   .  The first two left sides add up to the third. On the right side that fails      . Equation  plus equation  minus equation  is the impossible statement   . Thus the equations are inconsistent  as Gaussian elimination will systematically discover.   Another singular system close to this one has an infinity of solutions .  When the  in the last equation becomes  the three equations combine to give   .  Now the third equation is the sum of the first two. In that case the three planes have a whole line in common Figure .c.  Changing the right sides will move the planes in Figure .b parallel to themselves and for b         the figure is suddenly different.  The lowest plane moved up to meet the others and there is a line of solutions. Problem .c is still singular but now it suffers from too many solutions instead of too few. The extreme case is three parallel planes.  For most right sides there is no solution Figure  .d.   For  special  right  sides  like b         !   there  is  a  whole  plane  of solutionsbecause the three parallel planes move over to become the same. What happens to the column picture when the system is singular? it has to go wrong the question is how There are still three columns on the left side of the equations and we try to combine them to produce b . Stay with equation  Singular case Column picture Three columns in the same plane Solvable only for b in that plane u           v           w           b .  For b         this was possible for b         it was not.  The reason is that those three columns lie in a plane .  Then every combination is also in the plane which goes through the origin.  If the vector b is not in that plane no solution is possible Figure ..  That is by far the most likely event a singular system generally has no solution. But there is a chance that b does lie in the plane of the columns. In that case there are too many solutions the three columns can be combined in infinitely many ways to produce b . That column picture in Figure .b corresponds to the row picture in Figure .c.                      How do we know that the three columns lie in the same plane? One answer is to find a combination of the columns that adds to zero. After some calculation it is u   v   w   .  Three times column  equals column  plus twice column .  Column  is in  the plane of columns  and . Only two columns are independent. The vector b         is in that plane of the columnsit is column  plus column so    is a solution. We can add an multiple of the combination          that gives b  . So there is a whole line of solutionsas we know from the row picture. The truth is that we knew the columns would combine to give zero because the rows did. That is a fact of mathematics not of computationand it remains true in dimension n . If the n planes have no point in common  or infinitely many points  then the n columns lie in the same plane . If  the  row  picture  breaks  down  so  does  the  column  picture.   That  brings  out  the difference between Chapter  and Chapter .  This chapter studies the most important problemthe nonsingular casewhere there is one solution and it has to be found. Chapter    studies  the  general  case  where  there  may  be  many  solutions  or  none.   In both cases we cannot continue without a decent notation  matrix notation  and a decent algorithm  elimination . After the exercises we start with elimination. Pr oblem Set . . For  the  equations x  y     x   y    draw  the  row  picture  two  intersecting lines  and  the  column  picture  combination  of  two  columns  equal  to  the  column vector      on the right side. . Solve to find a combination of the columns that equals b  Triangular system u  v  w  b  v  w  b  w  b  . . Recommended Describe the intersection of the three planes u  v  w  z   and u  w  z   and u  w   all in fourdimensional space. Is it a line or a point or an empty set? What is the intersection if the fourth plane u    is included? Find a fourth equation that leaves us with no solution. . Sketch these three lines and decide if the equations are solvable  by  system x   y   x  y   y   . What happens if all righthand sides are zero?  Is there any nonzero choice of right hand sides that allows the three lines to intersect at the same point? . Find two points on the line of intersection of the three planes t   and z   and x  y  z  t   in fourdimensional space.   . When b          find a solution  u  v  w  to equation  different from the solution        mentioned in the text. . Give two more righthand sides in addition to b         for which equation  can be solved. Give two more righthand sides in addition to b         for which it cannot be solved. . Explain why the system u  v  w   u   v   w   v   w   is singular by finding a combination of the three equations that adds up to   . What value should replace the last zero on the right side to allow the equations to have solutionsand what is one of the solutions? . The column picture for the previous exercise singular system is u           v           w           b . Show that the three columns on the left lie in the same plane by expressing the third column as a combination of the first two.  What are all the solutions  u  v  w  if b is the zero vector        ? . Recommended Under what condition on y   y   y  do the points    y       y       y   lie on a straight line? . These equations are certain to have the solution x  y  . For which values of a is there a whole line of solutions? ax   y    x  ay   . Starting with x   y   find the equation for the parallel line through x   y  . Find the equation of another line that meets the first at x   y  . Problems  are a review of the row and column pictures. . Draw the two pictures in two planes for the equations x   y   x  y  . . For two linear equations in three unknowns x  y  z  the row picture will show  or  lines or planes in two or threedimensional space. The column picture is in two or threedimensional space. The solutions normally lie on a .  . For  four  linear  equations  in  two  unknowns x and y   the  row  picture  shows  four . The column picture is in dimensional space. The equations have no solution unless the vector on the righthand side is a combination of . . Find a point with z   on the intersection line of the planes x  y   z   and x  y  z  . Find the point with z   and a third point halfway between. . The first of these equations plus the second equals the third x  y  z   x   y  z    x   y   z   . The first two planes meet along a line.  The third plane contains that line because if x  y  z satisfy the first two equations then they also . The equations have infinitely many solutions the whole line L . Find three solutions. . Move the third plane in Problem  to a parallel plane  x   y   z  .  Now the three equations have no solution why not ? The first two planes meet along the line L  but the third plane doesnt that line. . In Problem  the columns are        and        and        . This is a singular case because the third column is . Find two combinations of the columns that give b         . This is only possible for b       c  if c  . . Normally  planes in fourdimensional space meet at a . Normally  col umn vectors in fourdimensional space can combine to produce b .  What combina tion of                                        produces b           ? What  equations for x  y  z  t are you solving? . When equation  is added to equation  which of these are changed  the planes in the row picture the column picture the coefficient matrix the solution? . If  a  b  is a multiple of  c  d  with abcd    show that  a  c  is a multiple of  b  d  . This is surprisingly important  call it a challenge question.  You could use numbers first to see how a  b  c  and d are related. The question will lead to If A      has dependent rows then it has dependent columns. . In these equations the third column multiplying w  is the same as the right side b . The column form of the equations immediately gives what solution for  u  v  w  ?  u   v   w    u   v   w    u   v   w   .   .    An Example of Gaussian Elimination The way to understand elimination is by example. We begin in three dimensions Original system  u  v  w    u   v      u   v   w   .  The problem is to find the unknown values of u  v  and w  and we shall apply Gaussian elimination. Gauss is recognized as the greatest of all mathematicians but certainly not because of this invention which probably took him ten minutes. Ironically it is the most frequently used of all the ideas that bear his name.  The method starts by subtracting multiples of the first equation from the other equations . The goal is to eliminate u from the last two equations. This requires that we a  subtract  times the first equation from the second b  subtract   times the first equation from the third. Equivalent system  u  v  w     v   w     v   w   .  The coefficient  is the first pivot .  Elimination is constantly dividing the pivot into the numbers underneath it to find out the right multipliers. The pivot for the second stage of elimination is  . We now ignore the first equation. A multiple of the second equation will be subtracted from the remaining equations in this case there is only the third one so as to eliminate v . We add the second equation to the third or in other words we c  subtract   times the second equation from the third. The elimination process is now complete at least in the forward direction Triangular system  u  v  w     v   w     w   .  This system is solved backward bottom to top.  The last equation gives w  .  Sub stituting into the second equation we find v  .  Then the first equation gives u  . This process is called backsubstitution . To repeat Forward elimination produced the pivots    . It subtracted multiples of  each  row  from  the  rows  beneath  It  reached  the  triangular  system    which  is solved in reverse order  Substitute each newly computed value into the equations that are waiting.  Remark . One good way to write down the forward elimination steps is to include the righthand side as an extra column.  There is no need to copy u and v and w and  at every step so we are left with the bare minimum                                                                    . At the end is the triangular system  ready for backsubstitution.  You may prefer this arrangement which guarantees that operations on the lefthand side of the equations are also done on the righthand sidebecause both sides are there together . In a larger problem forward elimination takes most of the effort.  We use multiples of the first equation to produce zeros below the first pivot.  Then the second column is cleared out below the second pivot.  The forward step is finished when the system is triangular equation n contains only the last unknown multiplied by the last pivot. Back substitution yields the complete solution in the opposite orderbeginning with the last unknown then solving for the next to last and eventually for the first. By definition pivots cannot be zero . We need to divide by them. The Breakdown of Elimination Under what circumstances could the process break down? Something must go wrong in the singular case and something might go wrong in the nonsingular case.  This may seem a little prematureafter all we have barely got the algorithm working.  But the possibility of breakdown sheds light on the method itself. The answer is  With a full set of n pivots there is only one solution.  The system is non singular and it is solved by forward elimination and backsubstitution. But if a zero appears in a pivot position elimination has to stopeither temporarily or permanently. The system might or might not be singular. If the first coefficient is zero in the upper left corner the elimination of u from the other equations will be impossible. The same is true at every intermediate stage. Notice that a zero can appear in a pivot position even if the original coefficient in that place was not zero.  Roughly speaking we do not know whether a zero will appear until we try  by actually going through the elimination process. In many cases this problem can be cured and elimination can proceed. Such a system still counts as nonsingular it is only the algorithm that needs repair.  In other cases a breakdown is unavoidable. Those incurable systems are singular they have no solution or else infinitely many and a full set of pivots cannot be found.   Example . Nonsingular cured by exchanging equations  and  u  v  w   u   v   w   u   v   w   u  v  w   w   v   w   u  v  w   v   w   w  The system is now triangular and backsubstitution will solve it. Example . Singular incurable u  v  w   u   v   w   u   v   w    u  v  w   w   w  There is no exchange of equations that can avoid zero in the second pivot position. The equations themselves may be solvable or unsolvable. If the last two equations are  w   and  w   there is no solution.  If those two equations happen to be consistentas in  w   and  w  then this singular case has an infinity of solutions.  We know that w   but the first equation cannot decide both u and v . Section . will discuss row exchanges when the system is not singular.  Then the exchanges produce a full set of pivots.  Chapter  admits the singular case and limps forward  with  elimination.   The   w can  still  eliminate  the   w   and  we  will  call    the second pivot. There wont be a third pivot. For the present we trust all n pivot entries to be nonzero without changing the order of the equations.  That is the best case with which we continue. The Cost of Elimination Our other question is very practical. How many separate arithmetical operations does elimination require for n equations in n unknowns? If n is large a computer is going to take our place in carrying out the elimination. Since all the steps are known we should be able to predict the number of operations. For  the  moment  ignore  the  righthand  sides  of  the  equations  and  count  only  the operations on the left.  These operations are of two kinds.  We divide by the pivot to find out what multiple say   of the pivot equation is to be subtracted.  When we do this subtraction we continually meet a multiplysubtract combination  the terms in the pivot equation are multiplied by   and then subtracted from another equation. Suppose we call each division and each multiplicationsubtraction one operation. In column  it takes n operations for every zero we achieve one to find the multiple   and the other to find the new entries along the row. There are n   rows underneath the first one so the first stage of elimination needs n  n     n   n operations.  Another approach to n   n is this All n  entries need to be changed except the n in the first row . Later stages are faster because the equations are shorter.  When the elimination is down to k equations only k   k operations are needed to clear out the column below the pivotby the same reasoning that applied to the first stage when k equaled n . Altogether the total number of operations is the sum of k   k over all values of k from  to n  Left side       n         n   n  n     n      n  n      n   n  . Those are standard formulas for the sums of the first n numbers and the first n squares. Substituting n   and n   and n   into the formula    n   n   forward elimination can take no steps or two steps or about a third of a million steps If n is at all large a good estimate for the number of operations is   n  . If the size is doubled and few of the coefficients are zero the cost is multiplied by . Backsubstitution is considerably faster. The last unknown is found in only one oper ation a division by the last pivot. The second to last unknown requires two operations and so on. Then the total for backsubstitution is       n . Forward elimination also acts on the righthand side subtracting the same multiples as on the left to maintain correct equations.  This starts with n   subtractions of the first equation.  Altogether the righthand side is responsible for n  operations much less than the n    on the left. The total for forward and back is Right side  n    n              n   n  . Thirty years ago almost every mathematician would have guessed that a general sys tem of order n could not be solved with much fewer than n    multiplications.  There were even theorems to demonstrate it but they did not allow for all possible methods. Astonishingly that guess has been proved wrong. There now exists a method that re quires only Cn    multiplications! It depends on a simple fact  Two combinations of two vectors in twodimensional space would seem to take  multiplications but they can be done in .  That lowered the exponent from log   which is  to log     . .  This discovery produced tremendous activity to find the smallest possible power of n .  The exponent finally fell at IBM below ..  Fortunately for elimination the constant C is so large and the coding is so awkward that the new method is largely or entirely of theoretical interest. The newest problem is the cost with many processors in parallel . Pr oblem Set . Problems  are about elimination on  by  systems.   . What multiple  of equation  should be subtracted from equation ?  x   y    x   y   . After this elimination step write down the upper triangular system and circle the two pivots. The numbers  and  have no influence on those pivots. . Solve the triangular system of Problem  by backsubstitution y before x .  Verify that x times      plus y times      equals      .  If the righthand side changes to       what is the new solution? . What multiple of equation  should be subtracted from equation ?  x   y    x   y   . After this elimination step solve the triangular system. If the righthand side changes to        what is the new solution? . What multiple  of equation  should be subtracted from equation ? ax  by  f cx  dy  g . The first pivot is a assumed nonzero.  Elimination produces what formula for the second pivot? What is y ? The second pivot is missing when ad  bc . . Choose a righthand side which gives no solution and another righthand side which gives infinitely many solutions. What are two of those solutions?  x   y    x   y  . . Choose a coefficient b that makes this system singular.  Then choose a righthand side g that makes it solvable. Find two solutions in that singular case.  x  by    x   y  g . . For which numbers a does elimination break down a permanently  and b tem porarily? ax   y     x   y   . Solve for x and y after fixing the second breakdown by a row exchange.  . For which three numbers k does elimination break down?  Which is fixed by a row exchange? In each case is the number of solutions  or  or  ? kx   y    x  ky    . . What test on b  and b  decides whether these two equations allow a solution?  How many solutions will they have? Draw the column picture.  x   y  b   x   y  b  . Problems  study elimination on  by  systems and possible failure. . Reduce this system to upper triangular form by two row operations  x   y  z    x   y   z     y   z   . Circle the pivots. Solve by backsubstitution for z  y  x . . Apply elimination circle the pivots and backsubstitution to solve  x   y    x   y  z    x  y   z   . List the three row operations Subtract times row from row . . Which number d forces a row exchange and what is the triangular system not sin gular for that d ? Which d makes this system singular no third pivot?  x   y  z    x  dy  z   y  z   . . Which number b leads later to a row exchange?  Which b leads to a missing pivot? In that singular case find a nonzero solution x  y  z . x  by   x   y  z   y  z   . . a  Construct a  by  system that needs two row exchanges to reach a triangular form and a solution. b  Construct a  by  system that needs a row exchange to keep going but breaks down later.   . If rows  and  are the same how far can you get with elimination allowing row exchange? If columns  and  are the same which pivot is missing?  x  y  z    x  y  z    x  y  z    x   y  z    x   y  z    x   y  z   . . Construct a  by  example that has  different coefficients on the lefthand side but rows  and  become zero in elimination. How many solutions to your system with b         and how many with b         ? . Which number q makes this system singular and which righthand side t gives it infinitely many solutions? Find the solution that has z  . x   y   z   x   y   z    y  qz  t . . Recommended It is impossible for a system of linear equations to have exactly two solutions. Explain why . a  If  x  y  z  and  X  Y  Z  are two solutions what is another one? b  If  planes meet at two points where else do they meet? . Three planes can fail to have an intersection point when no two planes are parallel. The system is singular if row  of A is a of the first two rows.  Find a third equation that cant be solved if x  y  z   and x   y  z  . Problems  move up to  by  and n by n . . Find the pivots and the solution for these four equations  x  y   x   y  z   y   z  t   z   t   . . If you extend Problem  following the    pattern or the      pattern what is the fifth pivot? What is the n th pivot? . Apply elimination and backsubstitution to solve  u   v    u   v  w    u  v   w   . What are the pivots?   List the three operations in which a multiple of one row is subtracted from another.  . For the system u  v  w   u   v   w   u   v   w    what is the triangular system after forward elimination and what is the solution? . Solve the system and find the pivots when  u  v    u   v  w    v   w  z    w   z   . You may carry the righthand side as a fifth column and omit writing u  v  w  z until the solution at the end. . Apply elimination to the system u  v  w     u   v  w   u  v  w    . When a zero arises in the pivot position exchange that equation for the one below it and proceed. What coefficient of v in the third equation in place of the present   would make it impossible to proceedand force elimination to break down? . Solve by elimination the system of two equations x  y    x   y   . Draw a graph representing each equation as a straight line in the x  y plane the lines intersect  at  the  solution.   Also  add  one  more  linethe  graph  of  the  new second equation which arises after elimination. . Find three values of a for which elimination breaks down  temporarily or perma nently in au  u    u  av   . Breakdown at the first step can be fixed by exchanging rowsbut not breakdown at the last step. . True or false a  If the third equation starts  with a zero coefficient it begins with   u  then no multiple of equation  will be subtracted from equation .   b  If the third equation has zero as its second coefficient it contains  v  then no multiple of equation  will be subtracted from equation . c  If the third equation contains  u and  v  then no multiple of equation  or equa tion  will be subtracted from equation . . Very optional Normally the multiplication of two complex numbers  a  ib  c  id    ac  bd  i  bc  ad  involves the four separate multiplications ac  bd  be  ad . Ignoring i  can you compute ac  bd and bc  ad with only three multiplications? You may do additions such as forming a  b before multiplying without any penalty. . Use elimination to solve u  v  w   u   v   w    u   v   w   and u  v  w   u   v   w    u   v   w   . . For which three numbers a will elimination fail to give three pivots? ax   y   z  b  ax  ay   z  b  ax  ay  az  b  . . Find experimentally the average size absolute value of the first and second and third pivots for MATLAB s lurand     . The average of the first pivot from absA     should be .. . Matrix Notation and Matrix Multiplication With our  by  example we are able to write out all the equations in full.  We can list the elimination steps which subtract a multiple of one equation from another and reach a triangular matrix.  For a large system this way of keeping track of elimination would be hopeless a much more concise record is needed. We now introduce matrix notation to describe the original system and matrix mul tiplication to describe the operations that make it simpler.  Notice that three different types of quantities appear in our example Nine coefficients Three unknowns Three righthand sides  u  v  w    u   v      u   v   w     On the righthand side is the column vector b . On the lefthand side are the unknowns u  v  w . Also on the lefthand side are nine coefficients one of which happens to be zero. It is natural to represent the three unknowns by a vector The unknown is x     u v w    The solution is x           . The nine coefficients fall into three rows and three columns producing a  by  matrix  Coefficient matrix A                     . A is a square matrix because the number of equations equals the number of unknowns. If there are n equations in n unknowns we have a square n by n matrix. More generally we might have m equations and n unknowns. Then A is rectangular  with m rows and n columns. It will be an  m by n matrix. Matrices are added to each other  or multiplied by numerical constants  exactly as vectors  areone  entry  at  a  time.   In  fact  we  may  regard  vectors  as  special  cases  of matrices they are matrices with only one column . As with vectors two matrices can be added only if they have the same shape Addition A  B Multiplication  A                                                                                            . Multiplication of a Matrix and a Vector We want to rewrite the three equations with three unknowns u  v  w in the simplified matrix form Ax  b . Written out in full matrix times vector equals vector Matrix form Ax  b                       u v w               .  The righthand side b is the column vector of inhomogeneous terms. The lefthand side is A times x .   This multiplication will be defined exactly so as to reproduce the original system . The first component of Ax comes from multiplying the first row of A into the column vector x  Row times column             u v w       u  v  w      .    The second component of the product Ax is  u   v   w  from the second row of A . The matrix equation Ax  b is equivalent to the three simultaneous equations in equation . Row times column is fundamental to all matrix multiplications.  From two vectors it produces a single number.  This number is called the inner product of the two vectors. In other words the product of a  by n matrix a row vector  and an n by  matrix a column vector  is a  by  matrix Inner product                                     . This confirms that the proposed solution x         does satisfy the first equation. There are two ways to multiply a matrix A and a vector x .  One way is a row at a time  Each row of A combines with x to give a component of Ax .  There are three inner products when A has three rows Ax by rows                                                                                       .  That is how Ax is usually explained but the second way is equally important. In fact it is more important! It does the multiplication a column at a time . The product Ax is found all at once as a combination of the three columns of A  Ax by columns                                           .  The answer is twice column  plus  times column .  It corresponds to the column picture of linear equations.  If the righthand side b has components    then the solution has components   .  Of course the row picture agrees with that and we eventually have to do the same multiplications. The column rule will be used over and over and we repeat it for emphasis A Every product Ax can be found using whole columns as in equation . Therefore Ax is a combination of the columns of A .  The coefficients are the components of x . To multiply A times x in n dimensions we need a notation for the individual entries in A . The entry in the ith row and  jth column is always denoted by a  . The first subscript gives the row number and the second subscript indicates the column.  In equation  a  is  and a  is . If A is an m by n matrix then the index i goes from  to m there are m rowsand the index j goes from  to n . Altogether the matrix has mn entries and a  is in the lower right corner.  One subscript is enough for a vector. The j th component of x is denoted by x  . The multiplication above had x    x    x   .  Normally x is written as a column vectorlike an n by  matrix.  But sometimes it is printed on a line as in x         . The parentheses and commas emphasize that it is not a  by  matrix.  It is a column vector and it is just temporarily lying down. To describe the product Ax  we use the  sigma  symbol  for summation Sigma notation The i th component of Ax is      a  x  . This sum takes us along the i th row of A .  The column index j takes each value from  to n and we add up the resultsthe sum is a   x   a   x     a  x  . We see again that the length of the rows the number of columns in A  must match the length of x . An m by n matrix multiplies an n dimensional vector and produces an m dimensional vector.  Summations are simpler than writing everything out in full but matrix notation is better. Einstein used tensor notation in which a repeated index automatically means summation. He wrote a  x  or even a   x   without the  . Not being Einstein we keep the  . The Matrix Form of One Elimination Step So  far  we  have  a  convenient  shorthand Ax  b for  the  original  system  of  equations. What about the operations that are carried out during elimination?  In our example the first step subtracted  times the first equation from the second.  On the righthand side  times the first component of b was subtracted from the second component. The same result is achieved if we multiply b by this elementary matrix or elimination matrix  Elementary matrix E                        . This is verified just by obeying the rule for multiplying a matrix and a vector Eb                                             . The components  and  stay the same because of the    and    in the rows of E . The new second component   appeared after the first elimination step. It is easy to describe the matrices like E  which carry out the separate elimination steps. We also notice the identity matrix which does nothing at all. B The identity matrix I  with s on the diagonal and s everywhere else leaves every vector unchanged.  The elementary matrix E  subtracts  times   row j from row i . This E  includes   in row i  column j . I                             has Ib  b E                         has E  b     b  b  b    b     . Ib  b is the matrix analogue of multiplying by .  A typical elimination step multiplies by E  .  The important question is  What happens to A on the left hand side? To maintain equality we must apply the same operation to both sides of Ax  b .  In other words we must also multiply the vector Ax by the matrix E .  Our original matrix E subtracts  times the first component from the second After this step the new and simpler system equivalent to the old is just E  Ax   Eb .  It is simpler because of the zero that was created below the first pivot.  It is equivalent because we can recover the original system by adding  times the first equation back to the second.  So the two systems have exactly the same solution x . Matrix Multiplication Now we come to the most important question How do we multiply two matrices ? There is a partial clue from Gaussian elimination We know the original coefficient matrix A  we know the elimination matrix E  and we know the result EA after the elimination step. We hope and expect that E                          times A                     gives EA                    . Twice the first row of A has been subtracted from the second row . Matrix multiplication is consistent with the row operations of elimination.  We can write the result either as E  Ax   Eb  applying E to both sides of our equation or as  EA  x  Eb .  The matrix EA is constructed exactly so that these equations agree and we dont need parentheses Matrix multiplication  EA times x  equals  E times Ax . We just write EAx . This is the whole point of an associative law like               .  The law seems so obvious that it is hard to imagine it could be false. But the same could be said of the commutative law       and for matrices EA is not AE . There is another requirement on matrix multiplication. We know how to multiply Ax  a matrix and a vector.  The new definition should be consistent with that one.  When a matrix B contains only a single column x   the matrixmatrix product AB should be identical with the matrixvector product Ax . More than that   When B contains several  columns b   b   b   the columns of AB should be Ab   Ab   Ab  ! Multiplication by columns AB  A    b  b  b         Ab  Ab  Ab     . Our first requirement had to do with rows and this one is concerned with columns. A third approach is to describe each individual entry in AB and hope for the best.  In fact there is only one possible rule and I am not sure who discovered it. It makes everything work.  It does not allow us to multiply every pair of matrices.  If they are square they must have the same size.  If they are rectangular they must not have the same shape the number of columns in A has to equal the number of rows in B .  Then A can be multiplied into each column of B . If A is m by n  and B is n by p  then multiplication is possible. The product AB will be m by p . We now find the entry in row i and column j of AB . C The i  j entry of AB is the inner product of the i th row of A and the j th column of B . In Figure . the   entry of AB comes from row  and column   AB    a  b   a  b   a  b   a  b  .           Note . We write AB when the matrices have nothing special to do with elimination. Our earlier example was EA  because of the elementary matrix E . Later we have PA  or LU  or even LDU . The rule for matrix multiplication stays the same. Example . AB                                     . The entry  is           the inner product of the first row of A and first column of B . The entry  is            from the second row and second column. The third column is zero in B  so it is zero in AB . B consists of three columns side by side and A multiplies each column separately. Every column of AB is a combination of the columns of A .  Just as in a matrixvector multiplication  the columns of A are multiplied by the entries in B .   Example . Row exchange matrix                               . Example . The s in the identity matrix I leave every matrix unchanged Identity matrix IA  A and BI  B . Important The multiplication AB can also be done a row at a time . In Example  the first row of AB uses the numbers  and  from the first row of A .  Those numbers give   row      row           .  Exactly as in elimination where all this started each row of AB is a combination of the rows of B . We summarize these three different ways to look at matrix multiplication. D i  Each entry of AB is the product of a row and a column   AB    row i of A  times column j of B  ii  Each column of AB is the product of a matrix and a column column j of AB  A times column j of B  iii  Each row of AB is the product of a row and a matrix row i of AB  row i of A  times B . This leads hack to a key property of matrix multiplication.  Suppose the shapes of three matrices A  B  C possibly rectangular permit them to be multiplied. The rows in A and B multiply the columns in B and C . Then the key property is this E Matrix multiplication is associative  AB  C  A  BC  . Just write ABC . AB times C equals A times BC . If C happens to be just a vector a matrix with only one column this is the requirement  EA  x  E  Ax  mentioned earlier. It is the whole basis for the laws of matrix multiplication.  And if C has several columns we have only to think of them placed side by side and apply the same rule several times.  Parentheses are not needed when we multiply several matrices. There are two more properties to mentionone property that matrix multiplication has and another which it does not have . The property that it does possess is F Matrix operations are distributive A  B  C   AB  AC and  B  C  D  BD  CD .  Of course the shapes of these matrices must match properly B and C have the same shape so they can be added and A and D are the right size for premultiplication and postmultiplication. The proof of this law is too boring for words. The property that fails to hold is a little more interesting G Matrix multiplication is not commutative Usually F E   EF . Example . Suppose E subtracts twice the first equation from the second. Suppose F is the matrix for the next step to add row  to row  E                        and F                           . These two matrices do commute and the product does both steps at once EF                         F E . In either order EF or F E  this changes rows  and  using row . Example . Suppose E is the same but G adds row  to row . Now the order makes a difference. When we apply E and then G  the second row is altered before it affects the third. If E comes after G  then the third equation feels no effect from the first. You will see a zero in the      entry of EG  where there is a   in GE  GE                                                                         but EG                          . Thus EG   GE .  A random example would show the same thingmost matrices dont commute.  Here the matrices have meaning.  There was a reason for EF  F E  and a reason for EG   GE .  It is worth taking one more step to see what happens with all three elimination matrices at once  GF E                           and EF G                           . The product GF E is the true order of elimination. It is the matrix that takes the original A to the upper triangular U . We will see it again in the next section. The other matrix EF G is nicer.  In that order the numbers   from E and  from F and G were not disturbed. They went straight into the product. It is the wrong order for elimination.  But fortunately it is the right order for reversing the elimination steps  which also comes in the next section. Notice that the product of lower triangular matrices is again lower triangular.   Problem Set . . Compute the products                                     and                                      and              . For the third one draw the column vectors      and      .  Multiplying by      just adds the vectors do it graphically. . Working a column at a time compute the products                       and                                     and                         . . Find two inner products and a matrix product                   and                  and                    . The first gives the length of the vector squared. . If  an m by n matrix A multiplies  an n dimensional  vector x   how  many  separate multiplications are involved? What if A multiplies an n by p matrix B ? . Multiply Ax to find a solution vector x to the system Ax  zero vector . Can you find more solutions to Ax  ? Ax                              . . Write down the  by  matrices A and B that have entries a   i  j and b          . Multiply them to find AB and BA . . Give  by  examples not just the zero matrix of a  a diagonal matrix a    if i   j . b  a symmetric matrix a   a  for all i and j . c  an upper triangular matrix a    if i  j . d  a skewsymmetric matrix a    a  for all i and j . . Do these subroutines multiply Ax by rows or columns ? Start with B  I     DO  I   N DO  J   N DO  J   N DO  I   N     BI  BI  AIJ  XJ     BI  BI  AIJ  XJ The outputs Bx  Ax are the same.   The second code is slightly more efficient in FORTRAN and much more efficient on a vector machine the first changes single entries B  I   the second can update whole vectors. . If the entries of A are a   use subscript notation to write a  the first pivot. b  the multiplier    of row  to be subtracted from row i . c  the new entry that replaces a  after that subtraction. d  the second pivot. . True or false? Give a specific counterexample when false. a  If columns  and  of B are the same so are columns  and  of AB . b  If rows  and  of B are the same so are rows  and  of AB . c  If rows  and  of A are the same so are rows  and  of AB . d  AB    A  B  . . The first row of AB is a linear combination of all the rows of B . What are the coeffi cients in this combination and what is the first row of AB  if A             and B                    ? . The product of two lower triangular matrices is again lower triangular all its entries above the main diagonal are zero.  Confirm this with a  by  example and then explain how it follows from the laws of matrix multiplication. . By trial and error find examples of  by  matrices such that a A    I  A having only real entries. b B    although B   . c CD   DC  not allowing the case CD  . d EF   although no entries of E or F are zero. . Describe the rows of EA and the columns of AE if E            .   . Suppose A commutes with every  by  matrix  AB  BA  and in particular A   a   b c   d  commutes with B             and B             . Show that a  d and b  c  .  If AB  BA for all matrices B  then A is a multiple of the identity. . Let x be the column vector     ...   . Show that the rule  AB  x  A  Bx  forces the first column of AB to equal A times the first column of B . . Which of the following matrices are guaranteed to equal  A  B   ? A    AB  B   A  A  B  B  A  B    A  B  B  A   A   AB  BA  B  . . If A and B are n by n matrices with all entries equal to  find  AB   .  Summation notation turns the product AB  and the law  AB  C  A  BC   into  AB      a  b       a  b   c     a     b  c   . Compute both sides if C is also n by n  with every c   . . A fourth way to multiply matrices is columns of A times rows of B  AB   column   row     column n  row n   sum of simple matrices . Give a  by  example of this important rule for matrix multiplication. . The matrix that rotates the x  y plane by an angle  is A      cos   sin  sin  cos   . V erify that A     A      A        from the identities for cos        and sin        . What is A    times A     ? . Find the powers A   A   A  times A  and B   B   C   C  . What are A   B   and C  ? A            and B         and C  AB              Pr oblems  are about elimination matrices. . Write down the  by  matrices that produce these elimination steps a E  subtracts  times row  from row . b E  subtracts   times row  from row .  c P exchanges rows  and  then rows  and . . In Problem  applying E  and then E  to the column b         gives E  E  b  . Applying E  before E  gives E  E  b  . When E  comes first row feels no effect from row . . Which three matrices E   E   E  put A into triangular form U ? A                          and E  E  E  A  U . Multiply those E s to get one matrix M that does elimination MA  U . . Suppose a    and the third pivot is .  If you change a  to  the third pivot is . If you change a  to  there is zero in the pivot position. . If every column of A is a multiple of         then Ax is always a multiple of        . Do a  by  example. How many pivots are produced by elimination? . What matrix E  subtracts  times row  from row ? To reverse that step R  should  times row to row . Multiply E  by R  . . a E  subtracts row  from row  and then P  exchanges rows  and .   What matrix M  P  E  does both steps at once? b P  exchanges rows  and  and then E  subtracts row I from row .   What matrix M  E  P  does both steps at once?  Explain why the M s are the same but the E s are different. . a  What  by  matrix E  will add row  to row ? b  What matrix adds row  to row  and at the same time adds row  to row ? c  What matrix adds row  to row  and then adds row  to row ? . Multiply these matrices                                                                                  and                                                       . . This  by  matrix needs which elimination matrices E  and E  and E  ? A                                  . Problems  are about creating and multiplying matrices   . Write these ancient problems in a  by  matrix form Ax  b and solve them a X is twice as old as Y and their ages add to  b  x  y        and      lie on the line y  mx  c . Find m and c . . The parabola y  a  bx  cx  goes through the points  x  y        and      and      . Find and solve a matrix equation for the unknowns  a  b  c  . . Multiply these matrices in the orders EF and F E and E   E            a     b        F                    c     . . a  Suppose all columns of B are the same.  Then all columns of EB are the same because each one is E times . b  Suppose all rows of B are        . Show by example that all rows of EB are not        . It is true that those rows are . . If E adds row  to row  and F adds row  to row  does EF equal F E ? . The first component of Ax is  a   x   a  x     a   x  .  Write formulas for the third component of Ax and the      entry of A  . . If AB  I and BC  I  use the associative law to prove A  C . . A is  by  B is  by  C is  by  and D is  by . All entries are . Which of these matrix operations are allowed and what are the results? BA AB ABD DBA A  B  C  . . What rows or columns or matrices do you multiply to find a  the third column of AB ? b  the first row of AB ? c  the entry in row  column  of AB ? d  the entry in row  column  of CDE ? .  by  matrices Choose the only B so that for every matrix A  a BA   A . b BA   B . c BA has rows  and  of A reversed and row  unchanged. d  All rows of BA are the same as row  of A . . True or false?  a  If A  is defined then A is necessarily square. b  If AB and BA are defined then A and B are square. c  If AB and BA are defined then AB and BA are square. d  If AB  B then A  I . . If A is m by n  how many separate multiplications are involved when a A multiplies a vector x with n components? b A multiplies an n by p matrix B ? Then AB is m by p . c A multiplies itself to produce A  ? Here m  n . . To prove that  AB  C  A  BC   use the column vectors b  ... b  of B . First suppose that C has only one column c with entries c  ... c   AB has columns Ab  ... Ab   and Bc has one column c  b     c  b  . Then  AB  c  c  Ab     c  Ab   equals A  c  b     c  b    A  Bc  . Linearity gives equality of those two sums and  AB  c  A  Bc  . The same is true for all other of C . Therefore  AB  C  A  BC  . Problems  use columnrow multiplication and block multiplication. . Multiply AB using columns times rows AB                                                         . . Bloc k multiplication separates matrices into blocks submatrices.  If their shapes make block multiplication possible then it is allowed. Replace these x s by numbers and confirm that block multiplication succeeds.  A   B   C D    AC  BD  and    x   x x x x x x x x       x x x x x x x x x    . . Dra w the cuts in A and B and AB to show how each of the four multiplication rules is really a block multiplication to find AB  a  Matrix A times columns of B . b  Rows of A times matrix B . c  Rows of A times columns of B . d  Columns of A times rows of B .   . Block multiplication says that elimination on column  produces EA        a   I  a   D    a    . . Elimination for a  by  block matrix  When A   A  I  multiply the first block row by CA   and subtract from the second row to find the  Schur complement  S   I   CA   I  A   B C   D    A   B  S  . . With i     the product  A  iB  x  iy  is Ax  iBx  iAy  By .  Use blocks to separate the real part from the imaginary part that multiplies i   A  B ? ?  x y    Ax  By ?  real part imaginary part . Suppose you solve Ax  b for three special righthand sides b  Ax            and Ax            and Ax            . If the solutions x   x   x  are the columns of a matrix X  what is AX ? . If the three solutions in Question  are x          and x          and x           solve Ax  b when b         . Challenge problem What is A ? . Find all matrices A   a   b c   d  that satisfy A                      A . . If you multiply a northwest matrix A and a southeast matrix B  what type of matri ces are AB and BA ?  Northwest and southeast mean zeros below and above the antidiagonal going from    n  to  n    . . Write  x   y  z   t   as a matrix A how many rows? multiplying the column vector  x  y  z  t  to produce b .  The solutions fill a plane in fourdimensional space. The plane is threedimensional with no D volume . . What  by  matrix P  projects the vector  x  y  onto the x axis to produce  x    ? What matrix P  projects onto the y axis to produce    y  ?  If you multiply      by P  and then multiply by P   you get   and  . . Write the inner product of        and  x  y  z  as a matrix multiplication Ax . A has one row. The solutions to Ax   lie on a perpendicular to the vector . The columns of A are only in dimensional space.  . In MATLAB notation write the commands that define the matrix A and the column vectors x and b . What command would test whether or not Ax  b ? A            x       b      . The MATLAB commands A  eye and v   produce the  by  identity matrix and the column vector        . What are the outputs from A  v and v  v ? Computer not needed! If you ask for v  A  what happens? . If  you  multiply  the    by    allones  matrix A    ones and  the  column v   ones   what is A  v ?   Computer not needed.   If you multiply B  eye  ones times w  zeros    ones  what is B  w ? . Invent a  by  magic matrix M with entries    ... .  All rows and columns and diagonals add to . The first row could be   . What is M times        ? What is the row vector          times M ? . Triangular Factors and Row Exchanges We want to look again at elimination to see what it means in terms of matrices.  The starting point was the model system Ax  b  Ax                        u v w                b .  Then there were three elimination steps with multipliers      Step . Subtract  times the first equation from the second Step . Subtract   times the first equation from the third Step . Subtract   times the second equation from the third. The result was an equivalent system U x  c  with a new coefficient matrix U  Upper triangular U x                      u v w                c .  This matrix U is upper triangular all entries below the diagonal are zero. The new right side c was derived from the original vector b by the same steps that took A into U . Forward elimination amounted to three row operations   Start with A and b  Apply steps    in that order End with U and c . U x  c is solved by backsubstitution. Here we concentrate on connecting A to U . The  matrices E for  step   F for  step    and G for  step    were  introduced  in  the previous section.  They are called elementary matrices  and it is easy to see how they work.  To subtract a multiple  of equation j from equation i  put the number   into the  i  j  position .  Otherwise keep the identity matrix with s on the diagonal and s elsewhere. Then matrix multiplication executes the row operation. The result of all three steps is GF EA  U .  Note that E is the first to multiply A  then F  then G .  We could multiply GF E together to find the single matrix that takes A to U and also takes b to c . It is lower triangular zeros are omitted From A to U GF E                                                          .  This is good but the most important question is exactly the opposite  How would we get from U back to A ? How can we undo the steps of Gaussian elimination? To undo step  is not hard.  Instead of subtracting we add twice the first row to the second. Not twice the second row to the first! The result of doing both the subtraction and the addition is to bring back the identity matrix Inverse of subtraction is addition                                                                                .  One operation cancels the other. In matrix terms one matrix is the inverse of the other. If the elementary matrix E has the number   in the  i  j  position then its inverse E   has   in that position. Thus E   E  I  which is equation . We can invert each step of elimination by using E   and F   and G   .  I think its not bad to see these inverses now before the next section. The final problem is to undo the whole process at once and see what matrix takes U back to A . Since step  was last in going from A to U   its matrix G must be the first to be inverted  in  the  reverse  direction .   Inverses  come  in  the  opposite  order!   The  second reverse step is F   and the last is E    From U back to A E   F   G   U  A is LU  A .  You can substitute GF EA for U  to see how the inverses knock out the original steps. Now we recognize the matrix L that takes U back to A .  It is called L  because it is lower triangular . And it has a special property that can be seen only by multiplying the  three inverse matrices in the right order E   F   G                                                    L .  The special thing is that the entries below the diagonal are the multipliers      and  .  When matrices are multiplied there is usually no direct way to read off the answer.   Here  the  matrices  come  in  just  the  right  order  so  that  their  product  can  be written down immediately.  If the computer stores each multiplier   the number that multiplies the pivot row j when it is subtracted from row i  and produces a zero in the i  j positionthen these multipliers give a complete record of elimination. The numbers   fit right into the matrix L that takes U back to A . H Triangular factorization A  LU with no exchanges of rows . L is lower triangular with s on the diagonal. The multipliers   taken from elimination are below the diagonal. U is the upper triangular matrix which appears after forward elimination The diagonal entries of U are the pivots. Example . A            goes to U            with L            . Then LU  A . Example . which needs a row exchange A            cannot be factored into A  LU . Example . with all pivots and multipliers equal to  A                                                                                     LU . From A to U there are subtractions of rows. From U to A there are additions of rows. Example . when U is the identity and L is the same as A  Lower triangular case A                    . The elimination steps on this A are easy i E subtracts   times row  from row  ii F subtracts   times row  from row  and iii G subtracts   times row  from row . The result is the identity matrix U  I . The inverses of E  F  and G will bring back A    E   applied to F   applied to G   applied to I produces A .            times            times            equals                   . The  order  is  right  for  the  s  to  fall  into  position.   This  always  happens!   Note  that parentheses in E   F   G   were not necessary because of the associative law. A  LU  The n by n case The factorization A  LU is so important that we must say more.  It used to be missing in linear algebra courses when they concentrated on the abstract side.  Or maybe it was thought to be too hardbut you have got it. If the last Example  allows any U instead of the particular U  I  we can see how the rule works in general. The matrix L  applied to U  brings back A  A  LU                      row  of U row  of U row  of U     original A .  The proof is to apply the steps of elimination .  On the righthand side they take A to U . On the lefthand side they reduce L to I  as in Example .  The first step subtracts   times        from the second row which removes   . Both sides of  end up equal to the same matrix U  and the steps to get there are all reversible. Therefore  is correct and A  LU . A  LU is  so  crucial  and  so  beautiful  that  Problem    at  the  end  of  this  section suggests a second approach. We are writing down  by  matrices but you can see how the arguments apply to larger matrices.  Here we give one more example and then put A  LU to use. Example .  A  LU  with zeros in the empty spaces A                                                                       . That shows how a matrix A with three diagonals has factors L and U with two diagonals. This example comes from an important problem in differential equations Section .. The second difference in A is a backward difference L times a forward difference U .  One Linear System  Two Triangular Systems There is a serious practical point about A  LU .  It is more than just a record of elimi nation steps L and U are the right matrices to solve Ax  b .  In fact A could be thrown away! We go from b to c by forward elimination this uses L  and we go from c to x by backsubstitution that uses U . We can and should do it without A  Splitting of Ax  b First    Lc  b    and then    U x  c .  Multiply the second equation by L to give LU x  Lc which is Ax  b .  Each triangular system is quickly solved. That is exactly what a good elimination code will do . Factor from A find its factors L and U . . Solve from L and U and b find the solution x . The separation into Factor and Solve means that a series of b s can be processed. The Solve subroutine obeys equation   two triangular systems in n    steps each. The solution for any new righthand side b can be found in only n  operations .  That is far below the n    steps needed to factor A on the lefthand side. Example . This is the previous matrix A with a righthand side b           . Ax  b x   x     x    x   x     x    x   x     x    x    splits into Lc  b and U x  c . Lc  b c     c   c     c   c     c   c    gives                 . U x  c x   x    x   x    x   x    x    gives                 . For these special tridiagonal matrices the operation count drops from n  to  n .  You see how Lc  b is solved forward  c  comes before c  . This is precisely what happens during forward elimination. Then U x  c is solved backward  x  before x  . Remark  . The LU form is unsymmetric on the diagonal L has s where U has the   pivots. This is easy to correct. Divide out of U a diagonal pivot matrix D  Factor out D U       d  d  . . . d              u   d  u   d  . . .  u   d  . . . . . . . . .        .  In the last example all pivots were d   .  In that case D  I .  But that was very excep tional and normally LU is different from LDU also written LDV . The triangular factorization can be written A  LDU  where L and U have  s on the diagonal and D is the diagonal matrix of pivots. Whene ver you see LDU or LDV  it is understood that U or V has is on the diagonal each row was divided by the pivot in D .  Then L and U are treated evenly.  An example of LU splitting into LDU is A                                            LDU . That has the s on the diagonals of L and U  and the pivots  and   in D . Remark  . We may have given the impression in describing each elimination step that the calculations must be done in that order. This is wrong. There is some freedom and there is a Crout algorithm that arranges the calculations in a slightly different way. There is no freedom in the final L D and U . That is our main point I If A  L  D  U  and also A  L  D  U   where the L s are lower triangular with unit diagonal  the U s are upper triangular with unit diagonal  and the D s are diagonal matrices with no zeros on the diagonal then L   L   D   D   U   U  .  The LDU factorization and the LU factorization are uniquely determined by A . The proof is a good exercise with inverse matrices in the next section. Row Exchanges and Permutation Matrices We now have to face a problem that has so far been avoided The number we expect to use as a pivot might be zero.  This could occur in the middle of a calculation.  It will happen at the very beginning if a   . A simple example is Zero in the pivot position         u v    b  b   . The difficulty is clear no multiple of the first equation will remove the coefficient .  The remedy is equally clear. Exchange the two equations  moving the entry  up into the pivot. In this example the matrix would become upper triangular Exchange rows  u   v  b   v  b  To express this in matrix terms we need the permutation matrix P that produces the row exchange. It comes from exchanging the rows of I  Permutation P            and PA                                . P has the same effect on b  exchanging b  and b  . The new system is PAx  Pb .  The unknowns u and v are not reversed in a row exchange. A permutation matrix P has the same rows as the identity in some order. There is a single  in every row and column. The most common permutation matrix is P  I it exchanges nothing. The product of two permutation matrices is another permutation the rows of I get reordered twice. After P  I  the simplest permutations exchange two rows.  Other permutations ex change more rows. There are n !   n  n        permutations of size n . Row  has n choices then row  has n   choices and finally the last row has only one choice. We can display all  by  permutations there are !           matrices I           P            P  P            P            P            P  P            . There will be  permutation matrices of order n  .  There are only two permutation matrices of order  namely           and           . When we know about inverses and transposes the next section defines A   and A   we discover an important fact P   is always the same as P  . A zero in the pivot location raises two possibilities The trouble may be easy to fix or it may be serious .  This is decided by looking below the zero .  If there is a nonzero entry lower down in the same column then a row exchange is carried out. The nonzero entry becomes the needed pivot and elimination can get going again A      a   b     c d   e    f    d     no first pivot a     no second pivot c     no third pivot .   If d   the problem is incurable and this matrix is singular .  There is no hope for a unique solution to Ax  b .  If d is not zero an exchange P  of rows  and  will move d into the pivot. However the next pivot position also contains a zero. The number a is now below it the e above it is useless. If a is not zero then another row exchange P  is called for P                              and P                              and P  P  A      e    f   b         One more point The permutation P  P  will do both row exchanges at once P  acts first P  P                                                                                      P . If we had known we could have multiplied A by P in the first place.  With the rows in the right order PA  any nonsingular matrix is ready for elimination. Elimination in a Nutshell PA  LU The main point is this If elimination can be completed with the help of row exchanges then we can imagine that those exchanges are done first by P . The matrix PA will not need row exchanges .  In other words PA allows the standard factorization into L times U . The theory of Gaussian elimination can be summarized in a few lines J In the nonsingular case  there is a permutation matrix P that reorders the rows of A to avoid zeros in the pivot positions.  Then Ax  b has a unique solution  With the rows reordered in advance PA can be factored into LU . In the singular case no P can produce a full set of pivots elimination fails. In practice we also consider a row exchange when the original pivot is near zero even if it is not exactly zero. Choosing a larger pivot reduces the roundoff error. You  have to  be  careful  with L .   Suppose  elimination  subtracts  row   from  row  creating    .  Then suppose it exchanges rows  and .  If that exchange is done in advance the multiplier will change to     in PA  LU . Example . A                                                                                  U .   That row exchange recovers LU but now     and     P                             and L                             and PA  LU .  In MATLAB  Ar k  exchanges row k with row r below it where the k th pivot has been found. We update the matrices L and P the same way. At the start P  I and sign    Ar k   Ak r  Lr k k  Lk r k Pr k   Pk r  sign  sign The  sign  of P tells whether the number of row exchanges is even sign    or odd sign   . A row exchange reverses sign. The final value of sign is the determinant of P and it does not depend on the order of the row exchanges. To summarize  A good elimination code saves L and U and P .  Those matrices carry the information that originally came in A and they carry it in a more usable form. Ax  b reduces to two triangular systems.  This is the practical equivalent of the calculation we do next to find the inverse matrix A   and the solution x  A   b . Pr oblem Set . . When is an upper triangular matrix nonsingular a full set of pivots? . What multiple   of row  of A will elimination subtract from row  of A ? Use the factored form A                                                        . What will be the pivots? Will a row exchange be required? . Multiply the matrix L  E   F   G   in equation  by GF E in equation                     times                            . Multiply also in the opposite order. Why are the answers what they are?   . Apply elimination to produce the factors L and U for A            and A                             and A                             . . Factor A into LU  and write down the upper triangular system U x  c which appears after elimination for Ax                                u v w              . . Find E  and E  and E   if E            . . Find the products F GH and HGF if with upper triangular zeros omitted F                                  G                                  H                                  . .  Second proof of A  LU  The third row of U comes from the third row of A by subtracting multiples of rows  and   of U ! row  of U  row  of A     row  of U      row  of U  . a  Why are rows of U subtracted off and not rows of A ?  Answer  Because by the time a pivot row is used . b  The equation above is the same as row  of A     row  of U     row  of U    row  of U  . Which rule for matrix multiplication makes this row  of L times U ? The other rows of LU agree similarly with the rows of A . . a  Under what conditions is the following product nonsingular? A                        d  d  d                      . b  Solve the system Ax  b starting with Lc  b                        c  c  c                b .  . a  Why does it take approximately n    multiplicationsubtraction steps to solve each of Lc  b and U x  c ? b  How many steps does elimination use in solving  systems with the same  by  coefficient matrix A ? . Solve as two triangular systems without multiplying LU to find A  LU x                                                           u v w              . . How could you factor A into a product U L  upper triangular times lower triangular? Would they be the same factors as in A  LU ? . Solve by elimination exchanging rows when necessary u   v   w      u   v   w   v  w   and v  w   u  v   u  v  w   . Which permutation matrices are required? . Write down all six of the  by  permutation matrices including P  I . Identify their inverses which are also permutation matrices. The inverses satisfy PP    I and are on the same list. . Find the PA  LDU factorizations and check them for A                             and A                             . . Find a  by  permutation matrix that requires three row exchanges to reach the end of elimination which is U  I . . The less familiar form A  LPU exchanges rows only at the end A                              L   A                              PU                                                        . What is L is this case? Comparing with PA  LU in Box J the multipliers now stay in place    is  and   is  when A  LPU . . Decide whether the following systems are singular or nonsingular and whether they have no solution one solution or infinitely many solutions v  w   u  v   u  w   and v  w   u  v   u  w   and v  w   u  v   u  w   .   . Which numbers a  b  c lead to row exchanges? Which make the matrix singular? A            a      b     and A   c       . Problems  compute the factorization A  LU and also A  LDU . . Forward elimination changes     x  b to a triangular     x  c  x  y   x   y    x  y   y                            . That step subtracted    times row  from row .  The reverse step adds   times row  to row .  The matrix for that reverse step is L  . Multiply this L times the triangular system     x      to get  . In letters L multiplies U x  c to give . . Mo ve to  by  Forward elimination changes Ax  b to a triangular U x  c  x  y  z   x   y   z   x   y   z   x  y  z   y   z    y   z   x  y  z   y   z   z   . The equation z   in U x  c comes from the original x   y   z   in Ax  b by subtracting    times equation  and    times the final equation . Reverse that to recover          in  A  b  from the final          and          and          in  U  c   Row  of  A   b      Row     Row    Row   of  U   c  . In matrix notation this is multiplication by L . So A  LU and b  Lc . . What are the  by  triangular systems Lc  b and U x  c from Problem ? Check that c         solves the first one. Which x solves the second one? . What two elimination matrices E  and E  put A into upper triangular form E  E  A  U ? Multiply by E    and E    to factor A into LU  E    E    U  A                             . . What  three  elimination  matrices E   E   E  put A into  upper  triangular  form E  E  E  A  U ?  Multiply by E     E    and E    to factor A into LU where L  E    E    E    . Find L and U  A                             .  . When zero appears in a pivot position A  LU  is not possible !  We need nonzero pivots d  f  i in U . Show directly why these are both impossible                    d    e  f                                    m   n        d    e   g f    h i    . . Which  number c leads  to  zero  in  the  second  pivot  position?   A  row  exchange  is needed and A  LU is not possible. Which c produces zero in the third pivot position? Then a row exchange cant help and elimination fails A      c                   . . What are L and D for this matrix A ? What is U in A  LU and what is the new U in A  LDU ? A                             . . A and B are symmetric across the diagonal because   . Find their triple factor izations LDU and say how U is related to L for these symmetric matrices A          and B                     . . Recommended Compute L and U for the symmetric matrix A       a   a   a   a a   b   b   b a   b   c   c a   b   c   d      . Find four conditions on a  b  c  d to get A  LU with four pivots. . Find L and U for the nonsymmetric matrix A       a   r   r    r a   b   s    s a   b   c   t a   b   c   d      . Find the four conditions on a  b  c  d  r  s  t to get A  LU with four pivots.   . Tridiagonal  matrices have  zero  entries  except  on  the  main  diagonal  and  the  two adjacent diagonals. Factor these into A  LU and A  LDV  A                             and A     a a  a   a  b b  b b  c    . . Solve the triangular system Lc  b to find c . Then solve U x  c to find x  L            and U            and b      . For safety find A  LU and solve Ax  b as usual. Circle c when you see it. . Solve Lc  b to find c . Then solve U x  c to find x . What was A ? L                             and U                             and b           . . If A and B have nonzeros in the positions marked by x  which zeros are still zero in their factors L and U ? A       x   x   x   x x   x   x   x   x   x     x   x      and B       x   x   x  x   x  x x  x   x  x   x   x      . . Important If A has pivots    with no row exchanges what are the pivots for the upper left  by  submatrix B without row  and column ? Explain why. . Starting from a  by  matrix A with pivots    add a fourth row and column to produce M .  What are the first three pivots for M  and why?  What fourth row and column are sure to produce  as the fourth pivot? . Use cholpascal to find the triangular factors of MATLAB s pascal .  Row exchanges in L U  lupascal spoil Pascals pattern! . Review For which numbers c is A  LU impossiblewith three pivots? A             c            . . Estimate the time difference for each new righthand side b when n  . Create A  rand and b  rand and B  rand .  Compare the times from tic A  b toc and tic A  B toc which solves for  right sides. Problems  are about permutation matrices.  . There are   even  permutations of           with an even number of exchanges . Two of them are          with no exchanges and          with two exchanges. List the other ten.  Instead of writing each  by  matrix use the numbers     to give the position of the  in each row. . How many exchanges will permute            back to            ?  How many exchanges to change              to              ?  One is even and the other is odd.  For  n ...   to   ... n   show that n   and  are even n   and  are odd. . If P  and P  are permutation matrices so is P  P  . This still has the rows of I in some order. Give examples with P  P    P  P  and P  P   P  P  . . Try this question.  Which permutation makes PA upper triangular?  Which permu tations make P  AP  lower triangular? Multiplying A on the right by P  exchanges the of A . A                           . Find a  by  permutation matrix with P   I but not P  I . Find a  by  permu tation  P with  P    I . . If you take powers of a permutation why is some P  eventually equal to I ? Find a  by  permutation P so that the smallest power to equal I is P  .  This is a challenge question. Combine a  by  block with a  by  block. . The matrix P that multiplies  x  y  z  to give  z  x  y  is also a rotation matrix. Find P and P  . The rotation axis a         doesnt move it equals Pa . What is the angle of rotation from v          to Pv          ? . If P is any permutation matrix find a nonzero vector x so that  I  P  x  .  This will mean that I  P has no inverse and has determinant zero. . If P has s on the antidiagonal from    n  to  n     describe PAP . . Inverses and Transposes The inverse of an n by n matrix is another n by n matrix. The inverse of A is written A   and pronounced  A inverse. The fundamental property is simple If you multiply by A and then multiply by A    you are back where you started  Inverse matrix If b  Ax then A   b  x .   Thus A   Ax  x .  The matrix A   times A is the identity matrix. Not all matrices have inverses.  An inverse is impossible when Ax is zero and x is nonzero .  Then A   would have to get back from Ax   to x .   No matrix can multiply that zero vector Ax and produce a nonzero vector x . Our  goals  are  to  define  the  inverse  matrix  and  compute  it  and  use  it  when A   existsand then to understand which matrices dont have inverses. K The inverse of A is a matrix B such that BA  I and AB  I .  There is at most one such B  and it is denoted by A    A   A  I and AA    I .  Note  .  The inverse exists if and only if elimination produces n pivots row exchanges allowed. Elimination solves Ax  b without explicitly finding A   . Note  . The  matrix A cannot  have  two  different  inverses  Suppose BA  I and  also AC  I . Then B  C  according to this proof by parentheses B  AC    BA  C gives BI  IC which is B  C .  This shows that a leftinverse B multiplying from the left and a rightinverse C multi plying A from the right to give AC  I  must be the same matrix . Note  . If A is invertible the one and only solution to Ax  b is x  A   b  Multiply Ax  b by A   . Then x  A   Ax  A   b . Note  . Important Suppose there is a nonzero vector x such that Ax  . Then A cannot have an inverse . To repeat No matrix can bring  back to x . If A is invertible then Ax   can only have the zero solution x  . Note  . A  by  matrix is invertible if and only if ad  bc is not zero  by  inverse  a   b c   d      ad  bc  d  b  c a  .  This number ad  bc is the determinant of A .  A matrix is invertible if its determinant is not zero Chapter .  In MATLAB  the invertibility test is to find n nonzero pivots . Elimination produces those pivots before the determinant appears. Note  . A diagonal matrix has an inverse provided no diagonal entries are zero If A     d  . . . d     then A         d  . . .   d     and AA    I . When two matrices are involved not much can be done about the inverse of A  B . The sum might or might not be invertible.   Instead  it is the inverse of their product  AB which is the key formula in matrix computations.  Ordinary numbers are the same  a  b    is hard to simplify while   ab splits into   a times   b . But for matrices the order of multiplication must be correct if ABx  y then Bx  A   y and x  B   A   y . The inverses come in reverse order . L A product AB of invertible matrices is inverted by B   A    Inverse of AB  AB     B   A   .  Proof. To show that B   A   is the inverse of AB  we multiply them and use the associa tive law to remove parentheses. Notice how B sits next to B     AB  B   A     ABB   A    AIA    AA    I  B   A    AB   B   A   AB  B   IB  B   B  I . A similar rule holds with three or more matrices Inverse of ABC  ABC     C   B   A   . We saw this change of order when the elimination matrices E  F  G were inverted to come  back  from U to A .   In  the  forward  direction GF EA was U .   In  the  backward direction L  E   F   G   was the product of the inverses. Since G came last  G   comes first . Please check that A   would be U   GF E . The Calculation of A    The GaussJordan Method Consider the equation AA    I .  If it is taken a column at a time  that equation de termines each column of A   .  The first column of A   is multiplied by A  to yield the first column of the identity Ax   e  .  Similarly Ax   e  and Ax   e  the e s are the columns of I . In a  by  example A times A   is I  Ax   e                      x  x  x     e  e  e                               .  Thus we have three systems of equations or n systems. They all have the same coeffi cient matrix A .  The righthand sides e   e   e  are different but elimination is possible on all systems simultaneously .  This is the GaussJordan method .  Instead of stopping at U and switching to backsubstitution it continues by subtracting multiples of a row from the rows above . This produces zeros above the diagonal as well as below. When it reaches the identity matrix we have found A   . The example keeps all three columns e   e   e   and operates on rows of length six   Example . Using the GaussJordan Method to Find A     e  e  e                                            pivot                                       pivot                                              L    . This completes the first halfforward elimination.  The upper triangular U appears in the first three columns. The other three columns are the same as L   . This is the effect of applying the elementary operations GF E to the identity matrix. Now the second half will go from U to I multiplying by U   .  That takes L   to U   L   which is A   . Creating zeros above the pivots we reach A    Second half  U   L                                  zeros above pivots                                  divide by pivots                                               I     . At the last step we divided the rows by their pivots  and   and .  The coefficient matrix in the lefthand half became the identity. Since A went to I  the same operations on the righthand half must have carried I into A   .  Therefore we have computed the inverse. A note for the future You can see the determinant   appearing in the denominators of A   . The determinant is the product of the pivots    .  It enters at the end when the rows are divided by the pivots. Remark  . In spite of this brilliant success in computing A    I dont recommend it I admit that A   solves Ax  b in one step. Two triangular steps are better x  A   b separates into Lc  b and U x  c . We could write c  L   b and then x  U   c  U   L   b .   But note that we did not explicitly form and in actual computation should not form  these matrices L   and U   .  It would be a waste of time since we only need backsubstitution for x and forward substitution produced c . A similar remark applies to A    the multiplication A   b would still take n  steps. It is the solution that we want and not all the entries in the inverse . Remark  . Purely out of curiosity we might count the number of operations required to find A   .  The normal count for each new righthand side is n   half in the forward direction and half in backsubstitution. With n righthand sides e  ... e  this makes n  . After including the n    operations on A itself the total seems to be  n   . This result is a little too high because of the zeros in the e  .  Forward elimination changes only the zeros below the .  This part has only n  j components so the count for e  is effectively changed to  n  j    .  Summing over all j  the total for forward elimination  is n   .   This  is  to  be  combined  with  the  usual n     operations  that  are applied to A  and the n  n     backsubstitution steps that finally produce the columns x  of A   . The final count of multiplications for computing A   is n   Operation count n    n    n  n     n  . This count is remarkably low.  Since matrix multiplication already takes n  steps it requires as many operations to compute A  as it does to compute A   !  That fact seems almost unbelievable and computing A  requires twice as many as far as we can see. Nevertheless if A   is not needed it should not be computed. Remark  . In the GaussJordan calculation we went all the way forward to U  before starting backward to produce zeros above the pivots. That is like Gaussian elimination but other orders are possible. We could have used the second pivot when we were there earlier  to create a zero above it as well as below it.  This is not smart.  At that time the second row is virtually full whereas near the end it has zeros from the upward row operations that have already taken place. Invertible  Nonsingular  n pivots Ultimately  we  want  to  know  which  matrices  are  invertible  and  which  are  not.   This question is so important that it has many answers. See the last page of the book! Each of the first five chapters will give a different but equivalent test for invertibility. Sometimes the tests extend to rectangular matrices and onesided inverses  Chapter  looks for independent rows and independent columns Chapter  inverts AA  or A  A . The other chapters look for nonzero determinants or nonzero eigenvalues or nonzero pivots . This last test is the one we meet through Gaussian elimination. We want to show in a few theoretical paragraphs that the pivot test succeeds. Suppose A has a full set of n pivots. AA    I gives n separate systems Ax   e  for the columns of A   .  They can be solved by elimination or by GaussJordan.  Row exchanges may be needed but the columns of A   are determined.   Strictly speaking we have to show that the matrix A   with those columns is also a left inverse.  Solving AA    I has at the same time solved A   A  I  but why? A sided inverse of a square matrix is automatically a sided inverse .  To see why notice  that every  GaussJordan  step  is  a  multiplication  on  the  left  by  an  elementary matrix . We are allowing three types of elementary matrices . E  to subtract a multiple  of row j from row i . P  to exchange rows i and j . D or D    to divide all rows by their pivots. The GaussJordan process is really a giant sequence of matrix multiplications  D    E  P  E  A  I .  That matrix in parentheses to the left of A  is evidently a leftinverse! It exists it equals the rightinverse by Note  so every nonsingular matrix is invertible . The converse is also true If A is invertible it has n pivots .  In an extreme case that is clear A cannot have a whole column of zeros.  The inverse could never multiply a column of zeros to produce a column of I .  In a less extreme case suppose elimination starts on an invertible matrix A but breaks down at column  Breakdown No pivot in column  A        d  x x   x  d  x   x       x       x      . This matrix cannot have an inverse  no matter what the x s are.   One proof is to use column operations for the first time?  to make the whole third column zero.  By sub tracting multiples of column  and then of column  we reach a matrix that is certainly not invertible. Therefore the original A was not invertible. Elimination gives a complete test An n by n matrix is invertible if and only if it has n pivots . The Transpose Matrix We  need  one  more  matrix  and  fortunately  it  is  much  simpler  than  the  inverse.   The transpose of A is denoted by A  . Its columns are taken directly from the rows of A the i th row of A becomes the i th column of A   Transpose If A                  then A                     . At the same time the columns of A become the rows of A   If A is an m by n matrix then A  is n by m . The final effect is to flip the matrix across its main diagonal and the entry  in row i  column j of A  comes from row j  column i of A  Entries of A   A     A  .  The transpose of a lower triangular matrix is upper triangular. The transpose of A  brings us back to A . If we add two matrices and then transpose the result is the same as first transposing and then adding  A  B   is the same as A   B  . But what is the transpose of a product AB or an inverse A   ? Those are the essential formulas of this section M i  The transpose of AB is  AB    B  A   ii  The transpose of A   is  A       A     . Notice  how  the  formula  for  AB   resembles  the  one  for  AB    .   In  both  cases  we reverse the order giving B  A  and B   A   . The proof for the inverse was easy but this one requires an unnatural patience with matrix multiplication. The first row of  AB   is the first column of AB . So the columns of A are weighted by the first column of B . This amounts to the rows of A  weighted by the first row of B  . That is exactly the first row of B  A  . The other rows of  AB   and B  A  also agree. Start from AB                                            Transpose to B  A                                                  . To establish the formula for  A      start from AA    I and A   A  I and take trans poses. On one side I   I . On the other side we know from part i the transpose of a product. You see how  A     is the inverse of A   proving ii Inverse of A   Transpose of A    A     A   I .  Symmetric Matrices With  these  rules  established  we  can  introduce  a  special  class  of  matrices  probably the most important class of all. A symmetric matrix is a matrix that equals its own transpose A   A .  The matrix is necessarily square.  Each entry on one side of the diagonal equals its mirror image on the other side a   a  . Two simple examples are A and D and also A    Symmetric matrices A            and D            and A              .   A symmetric matrix need not be invertible it could even be a matrix of zeros. But if A   exists it is also symmetric .  From formula ii above the transpose of A   always equals  A      for a symmetric matrix this is just A   . A   equals its own transpose it is symmetric whenever A is. Now we show that multiplying any matrix R by R  gives a symmetric matrix . Symmetric Products R  R  RR   and LDL  Choose any matrix R  probably rectangular. Multiply R  times R . Then the product R  R is automatically a square symmetric matrix The transpose of R  R is R   R     which is R  R .  That is a quick proof of symmetry for R  R .  Its i  j entry is the inner product of row i of R  column i of R  with column j of R .  The  j  i  entry is the same inner product column j with column i . So R  R is symmetric. RR  is also symmetric but it is different from R  R . In my experience most scientific problems that start with a rectangular matrix R end up with R  R or RR  or both. Example . R       and R       produce R  R      and RR      . The product R  R is n by n . In the opposite order RR  is m by m . Even if m  n  it is not very likely that R  R  RR  . Equality can happen but its not normal. Symmetric matrices appear in every subject whose laws are fair. Each action has an equal and opposite reaction.  The entry a  that gives the action of i onto j is matched by a  .  We will see this symmetry in the next section for differential equations.  Here LU misses the symmetry but LDL  captures it perfectly. N Suppose A  A  can be factored into A  LDU without row exchanges. Then U is the transpose of L . The symmetric factorization becomes A  LDL  . The  transpose  of A  LDU gives A   U  D  L  .   Since A  A    we  now  have  two factorizations of A into lower triangular times diagonal times upper triangular.   L  is upper triangular with ones on the diagonal exactly like U .  Since the factorization is unique see Problem  L  must be identical to U . L   U and A  LDL                                          LDL  . When elimination is applied to a symmetric matrix A   A is an advantage. The smaller matrices stay symmetric as elimination proceeds and we can work with half the matrix! The lower righthand corner remains symmetric    a   b   c b   d    e c   e    f        a b c  d              f        .  The work of elimination is reduced from n    to n   . There is no need to store entries from both sides of the diagonal or to store both L and U . Pr oblem Set . . Find the inverses no special system required of A              A              A    cos   sin  sin  cos   . . a  Find the inverses of the permutation matrices P                             and P                             . b  Explain for permutations why P   is always the same as P  .  Show that the s are in the right places to give PP   I . . From AB  C find a formula for A   . Also find A   from PA  LU . . a  If A is invertible and AB  AC  prove quickly that B  C . b  If A       find an example with AB  AC but B   C . . If the inverse of A  is B   show that the inverse of A is AB .   Thus A is invertible whenever A  is invertible. . Use the GaussJordan method to invert A                               A                       A                              . . Find three  by  matrices other than A  I and A   I  that are their own inverses A   I . . Show that A      has no inverse by solving Ax   and by failing to solve           a   b c   d             . . Suppose elimination fails because there is no pivot in column  Missing pivot A                                                    .   Show that A cannot be invertible.  The third row of A    multiplying A  should give the third row          of A   A  I . Why is this impossible? . Find the inverses in any legal way of A                                                      A                                    A        a b     c   d         a   b     c   d      . . Give examples of A and B such that a A  B is not invertible although A and B are invertible. b A  B is invertible although A and B are not invertible. c  all of A  B  and A  B are invertible. d  In the last case use A    A  B  B    B    A   to show that C  B    A   is also invertibleand find a formula for C   . . If A is invertible which properties of A remain true for A   ? a A is triangular. b A is symmetric. c A is tridiagonal. d All entries are whole numbers. e All entries are fractions including numbers like   . . If A      and B       compute A  B  B  A  AB   and BA  . . If B is square show that A  B  B  is always symmetric and K  B  B  is always skewsymmetric which means that K    K .  Find these matrices A and K when B        and  write B as  the  sum  of  a  symmetric  matrix  and  a  skewsymmetric matrix. . a  How many entries can be chosen independently in a symmetric matrix of order n ? b  How  many  entries  can  be  chosen  independently  in  a  skewsymmetric  matrix  K    K  of order n ? The diagonal of K is zero! . a  If A  LDU   with s on the diagonals of L and U   what is the corresponding factorization of A  ? Note that A and A  square matrices with no row exchanges share the same pivots. b  What triangular systems will give the solution to A  y  b ? . If A  L  D  U  and A  L  D  U   prove that L   L   D   D   and U   U  . If A is invertible the factorization is unique. a  Derive the equation L    L  D   D  U  U     and explain why one side is lower triangular and the other side is upper triangular. b  Compare the main diagonals and then compare the offdiagonals.  . Under what conditions on their entries are A and B invertible? A     a   b   c d   e  f        B     a   b  c   d      e    . . Compute the symmetric LDL  factorization of A                         and A   a   b b   d  . . Find the inverse of A                                          . . Remarkable If A and B are square matrices show that I  BA is invertible if I  AB is invertible. Start from B  I  AB      BA  B . . Find the inverses directly or from the  by  formula of A  B  C  A            and B   a   b b   and C            . . Solve for the columns of A     x   t y   z             x y       and           t z       . . Show that     has no inverse by trying to solve for the column  x  y             x   t y   z             must include           x y       . . Important If A has row   row   row  show that A is not invertible a  Explain why Ax         cannot have a solution. b  Which righthand sides  b   b   b   might allow a solution to Ax  b ? c  What happens to row  in elimination? . If A has column   column   column  show that A is not invertible   a  Find a nonzero solution x to Ax  . The matrix is  by . b  Elimination keeps column   column   column .  Explain why there is no third pivot. . Suppose A is invertible and you exchange its first two rows to reach B .  Is the new matrix B invertible? How would you find B   from A   ? . If  the  product M  ABC of  three  square  matrices  is  invertible  then A  B  C are invertible. Find a formula for B   that involves M   and A and C . . Prove that a matrix with a column of zeros cannot have an inverse. . Multiply     times        . What is the inverse of each matrix if ad   bc ? . a  What matrix E has the same effect as these three steps? Subtract row  from row  subtract row  from row  then subtract row  from row . b  What single matrix L has the same effect as these three reverse steps?  Add row  to row  add row  to row  then add row  to row . . Find the numbers a and b that give the inverse of   eye  ones                                                a   b   b   b b   a   b   b b   b   a   b b   b   b   a      . What are a and b in the inverse of   eye  ones ? . Show that A    eye  ones is not invertible Multiply A  ones . . There are sixteen  by  matrices whose entries are s and s.  How many of them are invertible? Problems  are about the GaussJordan method for calculating A   . . Change I into A   as you reduce A to I by row operations  A   I                         and  A   I                         . . Follow the  by  text example but with plus signs in A . Eliminate above and below the pivots to reduce  A  I  to  I  A       I                                                   .  . Use GaussJordan elimination on  A  I  to solve AA    I      a   b     c                                               . . Invert these matrices A by the GaussJordan method starting with  A  I   A                             and A                             . . Exchange rows and continue with GaussJordan to find A     A   I                         . . True or false with a counterexample if false and a reason if true a  A  by  matrix with a row of zeros is not invertible. b  A matrix with Is down the main diagonal is invertible. c  If A is invertible then A   is invertible. d  If A  is invertible then A is invertible. . For which three numbers c is this matrix not invertible and why not? A      c   c c   c   c     c    . . Prove that A is invertible if a    and a   b find the pivots and A    A     a   b   b a   a   b a   a   a    . . This matrix has a remarkable inverse. Find A   by elimination on  A  I  . Extend to a  by  alternating matrix and guess its inverse A                                .   . If B has the columns of A in reverse order solve  A  B  x   to show that A  B is not invertible. An example will lead you to x . . Find and check the inverses assuming they exist of these block matrices  I  C   I    A  C   D     I I    D  . . Use invS to invert MATLAB s  by  symmetric matrix S  pascal .  Create Pascals lower triangular A  abspascal and test invS  invA  invA . . If A  ones and b  rand  how does MATLAB tell you that Ax  b has no solution? If b  ones  which solution to Ax  b is found by A  b ? . M   shows the change in A   useful to know when a matrix is subtracted from A . Check part  by carefully multiplying MM   to get I  . M  I  uv  and M    I  uv      v  u  . . M  A  uv  and M    A    A   uv  A       v  A   u  . . M  I  UV and M    I   U  I   V U    V . . M  A  UW   V and M    A    A   U  W  VA   U    VA   . The four identities come from the   block when inverting these matrices  I u v      A    u v      I  U V    I     A   U V   W  . Problems  are about the rules for transpose matrices. . Find A  and A   and  A     and  A     for A            and also A    c c   . . Verify that  AB   equals B  A  but those are different from A  B   A            B            AB            . In case AB  BA not generally true! how do you prove that B  A   A  B  ? . a  The matrix   AB      comes from  A     and  B     . In what order ? b  If U is upper triangular then  U     is triangular . . Show that A    is possible but A  A   is not possible unless A  zero matrix.  . a  The row vector x  times A times the column y produces what number? x  Ay                                  . b  This is the row x  A  times the column y         . c  This is the row x        times the column Ay  . . When you transpose  a block matrix M      the result is M   . Test it. Under what conditions on A  B  C  D is the block matrix symmetric? . Explain why the inner product of x and y equals the inner product of Px and Py . Then  Px    Py   x  y says that P  P  I for any permutation. With x         and y          choose P to show that  Px   y is not always equal to x   P  y  . Problems  are about symmetric matrices and their factorizations. . If A  A  and B  B   which of these matrices are certainly symmetric? a A   B  b  A  B  A  B  c ABA d ABAB . . If A  A  needs a row exchange then it also needs a column exchange to stay sym metric. In matrix language PA loses the symmetry of A but reco vers the sym metry. . a  How many entries of A can be chosen independently if A  A  is  by ? b  How do L and D  by  give the same number of choices in LDL  ? . Suppose R is rectangular  m by n  and A is symmetric  m by m . a  Transpose R  AR to show its symmetry. What shape is this matrix? b  Show why R  R has no negative numbers on its diagonal. . Factor these symmetric matrices into A  LDL  . The matrix D is diagonal A            and A    b b   c  and A                     . The next three problems are about applications of  Ax   y  x   A  y  . . Wires go between Boston Chicago and Seattle. Those cities are at voltages x   x   x  . With unit resistances between cities the three currents are in y  y  Ax is    y  y  y                           x  x  x     .   a  Find the total currents A  y out of the three cities. b  Verify that  Ax   y agrees with x   A  y  six terms in both. . Producing x  trucks and x  planes requires x    x  tons of steel  x    x  pounds of rubber and  x    x  months of labor. If the unit costs y   y   y  are  per ton  per pound and  per month what are the values of one truck and one plane? Those are the components of A  y . . Ax gives the amounts of steel rubber and labor to produce x in Problem . Find A . Then  Ax   y is the of inputs while x   A  y  is the value of . . Here is a new factorization of A into triangular times symmetric  Start from A  LDU . Then A equals L  U     times U  DU . Why is L  U     triangular? Its diagonal is all s. Why is U  DU symmetric? . A group of matrices includes AB and A   if it includes A and B .   Products and inverses stay in the group. Which of these sets are groups? Lower triangularmatri ces L with is on the diagonal symmetric matrices S  positive matrices M  diagonal invertible matrices D  permutation matrices P . Invent two more matrix groups. . If every row of a  by  matrix contains the numbers     in some order can the matrix be symmetric? Can it be invertible? . Prove that no reordering of rows and reordering of columns can transpose a typical matrix. . A square northwest matrix B is zero in the southeast corner below the antidiagonal that connects    n  to  n    .  Will B  and B  be northwest matrices?  Will B   be northwest  or  southeast?   What  is  the  shape  of BC  northwest  times  southeast ? You are allowed to combine permutations with the usual L and U southwest and northeast. . Compare tic  invA  toc for A  rand and A  rand .  The n  count says that computing time measured by tic  toc  should multiply by  when n is doubled. Do you expect these random A to be invertible? . I  eye  A  rand  B  triuA  produces a random triangular matrix B .  Compare the times for inv B  and B  I .  Backslash is engineered to use the zeros in B  while inv uses the zeros in I when reducing  B  I  by GaussJordan. Compare also with inv A  and A  I for the full matrix A . . Show that L   has entries j  i for i  j the      matrix has this L  L                                  and L                                          .  Test this pattern for L  eye  diag  diag   and inv L  . . Special Matrices and Applications This section has two goals. The first is to explain one way in which large linear systems Ax  b can arise in practice. The truth is that a large and completely realistic problem in engineering or economics would lead us far afield. But there is one natural and important application that does not require a lot of preparation. The other goal is to illustrate by this same application the special properties that co efficient matrices frequently have. Large matrices almost always have a clear pattern frequently a pattern of symmetry  and very many zero entries .   Since a sparse matrix contains far fewer than n  pieces of information the computations ought to be fast. We look at band matrices  to see how concentration near the diagonal speeds up elimination. In fact we look at one special tridiagonal matrix. The matrix itself can be seen in equation  . It comes from changing a differential equation to a matrix equation.  The continuous problem asks for u  x  at every x  and a computer cannot solve it exactly. It has to be approximated by a discrete problemthe more unknowns we keep the better will be the accuracy and the greater the expense. As a simple but still very typical continuous problem our choice falls on the differential equation  d  u d x   f  x     x   .  This is a linear equation for the unknown function u  x  .  Any combination C  Dx could be added to any solution since the second derivative of C  Dx contributes nothing. The uncertainty left by these two arbitrary constants C and D is removed by a  boundary condition  at each end of the interval u       u      .  The result is a twopoint boundaryvalue problem  describing not a transient but a steady state phenomenonthe temperature distribution in a rod for example with ends fixed at   and with a heat source f  x  . Remember that our goal is to produce a discrete problemin other words a problem in linear algebra. For that reason we can only accept a finite amount of information about f  x   say its values at n equally spaced points x  h  x   h ... x  nh .  We compute approximate values u  ... u  for the true solution u at these same points.  At the ends x   and x     n    h  the boundary values are u    and u     . The first question is How do we replace the derivative d  u  dx  ? The first derivative can be approximated by stopping  u   x at a finite stepsize and not permitting h or  x    to approach zero. The difference  u can be forward  backward  or centered       u  x  h   u  x  h or u  x   u  x  h  h or u  x  h   u  x  h   h .  The last is symmetric about x and it is the most accurate. For the second derivative there is just one combination that uses only the values at x and x  h  Second difference d  u d x          u  x  h    u  x   u  x  h  h  .  This also has the merit of being symmetric about x .  To repeat the righthand side ap proaches the true value of d  u  dx  as h   but we have to stop at a positive h . At each meshpoint x  jh  the equation  d  u  dx   f  x  is replaced by its discrete analogue . We multiplied through by h  to reach n equations Au  b  Difference equation  u      u   u     h  f  jh  for j   ... n .  The first and last equations  j   and j  n  include u    and u      which are known from the boundary conditions.  These values would be shifted to the righthand side of the equation if they were not zero. The structure of these n equations  can be better visualized in matrix form. We choose h     to get a  by  matrix A  Matrix equation                                           u  u  u  u  u          h         f  h  f   h  f   h  f   h  f   h         .  From now on we will work with equation .  It has a very regular coefficient matrix whose order n can be very large.  The matrix A possesses many special properties and three of those properties are fundamental . The matrix A is tridiagonal. All nonzero entries lie on the main diagonal and the two adjacent diagonals.  Outside this band all entries are a   .  These zeros will bring a tremendous simplification to Gaussian elimination. . The matrix is symmetric. Each entry a  equals its mirror image a   so that A   A . The upper triangular U will be the transpose of the lower triangular L   and A  LDL  .  This symmetry of A reflects the symmetry of d  u  dx  .  An odd derivative like du  dx or d  u  dx  would destroy the symmetry. . The matrix is positive definite. This extra property says that the pivots are positive . Row exchanges are unnecessary in theory and in practice. This is in contrast to the matrix B at the end of this section which is not positive definite.  Without a row exchange it is totally vulnerable to roundoff. Positive definiteness brings this whole course together in Chapter !  We return to the fact that A is tridiagonal. What effect does this have on elimination? The first stage of the elimination process produces zeros below the first pivot Elimination on A  Step                                                                         . Compared with a general  by  matrix that step displays two major simplifications .  There was only one nonzero entry below the pivot. .  The pivot row was very short . The multiplier       came from one division.  The new pivot   came from a single multiplicationsubtraction .   Furthermore the  tridiagonal  pattern  is  preserved   Every stage of elimination admits the simplifications a and b. The final result is the LDU  LDL  factorization of A . Notice the pivots! A                                                                                        . The L and U  factors of a tridiagonal matrix are bidiagonal .  The three factors together have the same band structure of three essential diagonals  n   parameters as A . Note too that L and U are transposes of one another as expected from the symmetry.  The pivots          are all positive.   Their product is the determinant of A  det A  . The pivots are obviously converging to  as n gets large. Such matrices make a computer very happy. These sparse factors L and U completely change the usual operation count. Elimina tion on each column needs only two operations as above and there are n columns. In place of n    operations we need only  n .  Tridiagonal systems Ax  b can be solved almost instantly. The cost of solving a tridiagonal system is proportional to n . A band matrix has a    except in the band  i  j   w Figure ..  The half bandwidth is w   for a diagonal matrix w   for a tridiagonal matrix and w  n for a full matrix.   For each column  elimination requires w  w    operations  a row of length w acts on w   rows below. Elimination on the n columns of a band matrix requires about w  n operations . As w approaches n  the matrix becomes full and the count is roughly n  . For an exact count the lower righthand corner has no room for bandwidth w . The precise number of divisions and multiplicationsubtractions that produce L  D  and U without assuming a                     symmetric A  is P    w  w     n   w    . For a full matrix with w  n  we recover P    n  n    n    . This is a whole number since n   n  and n   are consecutive integers and one of them is divisible by . That is our last operation count and we emphasize the main point. A finitedifference matrix like A has a full inverse.   In solving Ax  b  we are actually much worse off knowing A   than knowing L and U .  Multiplying A   by b takes n  steps whereas  n are sufficient for the forward elimination and backsubstitution that produce x  U   c  U   L   b  A   b . We hope this example reinforced the readers understanding of elimination which we now assume to be perfectly understood!. It is a genuine example of the large linear systems that are actually met in practice. The next chapter turns to the existence and the uniqueness of x  for m equations in n unknowns. Roundoff Error In theory the nonsingular case is completed.  There is a full set of pivots with row ex changes. In practice more row exchanges may be equally necessaryor the computed solution can easily become worthless.  We will devote two pages entirely optional in class to making elimination more stablewhy it is needed and how it is done. For a system of moderate size say  by  elimination involves a third of a mil lion operations    n  . With each operation we must expect a roundoff error.  Normally we keep a fixed number of significant digits say three for an extremely weak computer. Then adding two numbers of different sizes gives an error Roundoff Error .   .   .     loses the digits  and . How do all these individual errors contribute to the final error in Ax  b ? This is not an easy problem.  It was attacked by John von Neumann who was the leading mathematician at the time when computers suddenly made a million operations possible. In fact the combination of Gauss and von Neumann gives the simple elimina tion algorithm a remarkably distinguished history although even von Neumann overes  timated the final roundoff error. It was Wilkinson who found the right way to answer the question and his books are now classics. Two simple examples will illustrate three important points about roundoff error. The examples are Illconditioned A    .  .  .  .   Wellconditioned B   .     .  .  .  . A is nearly singular whereas B is far from singular.  If we slightly change the last entry of A to a    it is singular. Consider two very close righthand sides b  u  v   u   .  v   and u  v   u   .  v   .  The solution to the first is u   v  .  The solution to the second is u  v  . A change in the fifth digit of b was amplified to a change in the first digit of the solution. No numerical method can avoid this sensitivity to small perturbations . The illconditioning can be shifted from one place to another but it cannot be removed. The true solution is very sensitive and the computed solution cannot be less so. The second point is as follows. O Even a wellconditioned matrix like B can be ruined by a poor algorithm. We regret to say that for the matrix B  direct Gaussian elimination is a poor algorithm. Suppose . is accepted as the first pivot. Then  times the first row is subtracted from the second.  The lower right entry becomes   but roundoff to three places would give  . Every trace of the entry  would disappear Elimination on B with small pivot .  u  v   u  v    .  u  v     v    . Roundoff will produce     v      or v  . This is correct to three decimal places. Backsubstitution with the right v  .  would leave u   Correct result .  u  .     or u   . Instead accepting v   which is wrong only in the fourth place we obtain u   Wrong result .  u      or u   . The computed u is completely mistaken. B is wellconditioned but elimination is vio lently unstable. L  D  and U are completely out of scale with B  B            .                . The small pivot . brought instability and the remedy is clear exchange rows .   P A  small  pivot  forces  a  practical  change  in  elimination.   Normally  we compare each pivot with all possible pivots in the same column.  Exchanging rows to obtain the largest possible pivot is called partial pivoting . For B  the pivot . would be compared with the possible pivot I below it.  A row exchange would take place immediately.   In matrix terms  this is multiplication by a permutation matrix P      . The new matrix C  PB has good factors C     .          .         .            The pivots for C are  and . much better than . and   for B . The strategy of complete pivoting looks also in all later columns for the largest pos sible  pivot.   Not  only  a  row  but  also  a  column  exchange  may  be  needed.   This  is post multiplication by a permutation matrix.  The difficulty with being so conservative is the expense and partial pivoting is quite adequate. We  have  finally  arrived  at  the  fundamental  algorithm  of  numerical  linear  algebra elimination with partial pivoting .  Some further refinements such as watching to see whether a whole row or column needs to be resealed are still possible.  But essentially the reader now knows what a computer does with a system of linear equations.  Com pared with the theoretical description find A    and multiply A   b our description has consumed a lot of the readers time and patience. I wish there were an easier way to explain how x is actually found but I do not think there is. Pr oblem Set . . Write out the LDU  LDL  factors of A in equation  when n  . Find the deter minant as the product of the pivots in D . . Modify a  in equation  from a    to a    and find the LDU factors of this new tridiagonal matrix. . Find the  by  matrix A   h     that approximates  d  u d x   f  x   du d x     du d x       replacing these boundary conditions by u   u  and u   u  .  Check that your A  times the constant vector  C  C  C  C  C   yields zero A  is singular . Analogously if u  x  is a solution of the continuous problem then so is u  x  C . . Write down the  by  finitedifference matrix equation  h     for  d  u d x   u  x  u     u      .  . With h    and f  x      sin   x  the difference equation  is                       u  u  u                   . Solv e for u   u   u  and find their error in comparison with the true solution u  sin   x at x     x     and x    . . What  by  system replaces  if the boundary conditions are changed to u      u     ? Problems  are about roundoff error and row exchanges. . Compute H   in two ways for the  by  Hilbert matrix H                          first by exact computation and second by rounding off each number to three figures. This matrix H is illconditioned and row exchanges dont help. . For the same matrix H  compare the righthand sides of Hx  b when the solutions are x         and x         .   . . Solve Hx  b      ...   for the  by  Hilbert matrix with h      i  j     using any computer code for linear equations.  Then change an entry of b by . and compare the solutions. . Compare the pivots in direct elimination to those with partial pivoting for A   .      . This is actually an example that needs rescaling before elimination. . Explain why partial pivoting produces multipliers   in L that satisfy     .  Can you construct a  by  example with all  a    whose last pivot is ?  This is the worst possible since each entry is at most doubled when     . Re view Exercises . a  Write down the  by  matrices with entries a   i  j and b   i j .   b  Compute the products AB and BA and A  . . For the matrices A            and B             compute AB and BA and A   and B   and  AB    . . Find  exampes  of    by    matrices  with a     for which  a A   I . b A    A  . c A   A . . Solve by elimination and backsubstitution u  w   u  v   u  v  w   and v  w   u  w   u  v   . . Factor the preceding matrices into A  LU or PA  LU . . a  There are sixteen  by  matrices whose entries are s and s.  How many are invertible? b  Much harder!  If you put s and s at random into the entries of a  by  matrix is it more likely to be invertible or singular? . There are sixteen  by  matrices whose entries are s and  s.  How many are invertible? . How are the rows of EA related to the rows of A in the following cases? E                             or E                  or E                             . . Write down a  by  system with infinitely many solutions. . Find inverses if they exist by inspection or by GaussJordan A                             and A                             and A                    . If E is  by  and it adds the first equation to the second what are E  and E  and  E ? . True or false with reason if true or counterexample if false   If A is invertible and its rows are in reverse order in B  then B is invertible.    If A and B are symmetric then AB is symmetric.   If A and B are invertible then BA is invertible.   Every nonsingular matrix can be factored into the product A  LU of a lower triangular L and an upper triangular U . . Solve Ax  b by solving the triangular systems Lc  b and U x  c  A  LU                                                         b           . What part of A   have you found with this particular b ? . If possible find  by  matrices B such that  BA   A for every A .  BA   B for every A .  BA has the first and last rows of A reversed.  BA has the first and last columns of A reversed. . Find the value for c in the following n by n inverse if A       n        n                 n      then A     n        c     c              c      . . For which values of k does kx  y   x  ky   have no solution one solution or infinitely many solutions? . Find the symmetric factorization A  LDL  of A                         and A   a   b b   c  . . Suppose A is the  by  identity matrix except for a vector v in column  A        v       v       v       v           .   a  Factor A into LU  assuming v    . b  Find A    which has the same form as A . . Solve by elimination or show that there is no solution u  v  w   u   v   w    u   v   w   and u  v  w   u  u   w    u   v   w   . . The n by n permutation matrices are an important example of a group.  If you multiply them you stay inside the group they have inverses in the group the identity is in the group and the law P   P  P     P  P   P  is truebecause it is true for all matrices. a  How many members belong to the groups of  by  and n by n permutation matrices? b  Find a power k so that all  by  permutation matrices satisfy P   I . . Describe the rows of DA and the columns of AD if D      . . a  If A is invertible what is the inverse of A  ? b  If A is also symmetric what is the transpose of A   ? c  Illustrate both formulas when A      . . By experiment with n   and n   find                                     . . Starting with a first plane u   v  w   find the equation for a  the parallel plane through the origin. b  a second plane that also contains the points        and        . c  a third plane that meets the first and second in the point        . . What multiple of row  is subtracted from row  in forward elimination of A? A                                                        . How do you know without multiplying those factors that A is invertible  symmet ric  and tridiagonal ? What are its pivots? . a  What vector x will make Ax  column  of A  column  for a  by  matrix A ?  b  Construct  a  matrix  that  has  column    column    .   Check  that A is singular fewer than  pivots and explain why that must be the case. . True or false with reason if true and counterexample if false   If L  U   L  U  upper triangular U s with nonzero diagonal lower triangular L s with unit diagonal  then L   L  and U   U  .   The LU factorization is unique.   If A   A  I then A    A  I .   If all diagonal entries of A are zero then A is singular. . By experiment or the GaussJordan method compute                 m                          m                              m       . . Write down the  by  matrices that a  reverse the direction of every vector. b  project every vector onto the x  axis. c  turn every vector counterclockwise through   . d  reflect every vector through the   line x   x  .    .    Vector Spaces and Subspaces Elimination can simplify one entry at a time the linear system Ax  b .  Fortunately it also simplifies the theory.  The basic questions of existence and uniqueness Is there one solution  or no solution  or an infinity of solutions?are much easier to answer after elimination We need to devote one more section to those questions to find every solution for an m by n system. Then that circle of ideas will be complete. But elimination produces only one kind of understanding of Ax  b . Our chief object is to achieve a different and deeper understanding.  This chapter may be more difficult than the first one. It goes to the heart of linear algebra. For  the  concept  of  a vector  space   we  start  immediately  with  the  most  important spaces. They are denoted by R   R   R  ...  the space R  consists of all column vectors with  n  components .   We  write R because  the  components  are  real  numbers. R  is represented by the usual x  y plane the two components of the vector become the x and y coordinates of the corresponding point. The three components of a vector in R  give a point in threedimensional space. The onedimensional space R  is a line. The  valuable  thing  for  linear  algebra  is  that  the  extension  to n dimensions  is  so straightforward.   For  a  vector  in R  we  just  need  the  seven  components  even  if  the geometry is hard to visualize. Within all vector spaces two operations are possible W e can add any two vectors and we can multiply all vectors by scalars. In other words we can take linear combinations. Addition obeys the commutative law x  y  y  x  there is a zero vector satisfying   x  x  and there is a vector   x  satisfying  x  x  . Eight properties including those three are fundamental the full list is given in Problem  at the end of this section. A real vector space is a set of vectors together with rules for vector addition and mul tiplication by real numbers .  Addition and multiplication must produce vectors in the space and they must satisfy the eight conditions.  Normally our vectors belong to one of the spaces R   they are ordinary column vec tors.  If x            then  x and also x  x  has components    .  The formal definition allows other things to be vectorsprovided that addition and scalar multipli cation are all right. We give three examples . The infinitedimensional space R  . Its vectors have infinitely many components as in x          ...  . The laws for x  y and cx stay unchanged. . The space of  by  matrices .  In this case the vectors are matrices!  We can add two matrices and A  B  B  A  and there is a zero matrix and so on. This space is almost the same as R  .  The six components are arranged in a rectangle instead of a column.  Any choice of m and n would give as a similar example the vector space of all m by n matrices. . The space of functions  f  x  .   Here we admit all functions f that are defined on a fixed interval say   x  .  The space includes f  x   x   g  x   sin x  their sum  f  g  x   x   sin x  and all multiples like  x  and  sin x .  The vectors are functions and the dimension is somehow a larger infinity than for R  . Other examples are given in the exercises but the vector spaces we need most are somewhere else they are inside the standard spaces R  .  We want to describe them and explain why they are important. Geometrically think of the usual threedimensional R  and choose any plane through the origin. That plane is a vector space in its own right .  If we multiply a vector in the plane by  or   or any other scalar we get a vector in the same plane.   If we add two vectors in the plane  their sum stays in the plane. This plane through        illustrates one of the most fundamental ideas in linear algebra it is a subspace of the original space R  . Definition. A subspace of a vector space is a nonempty subset that satisfies the require ments for a vector space Linear combinations stay in the subspace . i  If we add any vectors x and y in the subspace x  y is in the subspace . ii  If we multiply any vector x in the subspace by any scalar c  cx is in the subspace . Notice our emphasis on the word space . A sub space is a subset that is closed under addition and scalar multiplication.  Those operations follow the rules of the host space keeping us inside the subspace . The eight required properties are satisfied in the larger space and will automatically be satisfied in every subspace. Notice in particular that the zero vector will belong to every subspace . That comes from rule ii Choose the scalar to be c  . The  smallest  subspace Z contains  only  one  vector  the  zero  vector.   It  is  a  zero dimensional space containing only the point at the origin. Rules i and ii are satisfied   since the sum    is in this onepoint space and so are all multiples c . This is the smallest possible vector space  the empty set is not allowed.  At the other extreme.  the largest subspace is the whole of the original space.  If the original space is R   then the possible subspaces are easy to describe R  itself any plane through the origin any line through the origin or the origin the zero vector alone. The distinction between a subset and a subspace is made clear by examples. In each case can you add vectors and multiply by scalars without leaving the space? Example . Consider all vectors in R  whose components are positive or zero.  This subset is the first quadrant of the x  y plane the coordinates satisfy x   and y  .  It is not a subspace  even though it contains zero and addition does leave us within the subset. Rule ii is violated since if the scalar is   and the vector is       the multiple cx        is in the third quadrant instead of the first. If we include the third quadrant along with the first scalar multiplication is all right. Every multiple cx will stay in this subset. However rule i is now violated since adding             gives        which is not in either quadrant.  The smallest subspace containing the first quadrant is the whole space R  . Example . Start from the vector space of  by  matrices.  One possible subspace is the set of lower triangular matrices .  Another is the set of symmetric matrices . A  B and cA are lower triangular if A and B are lower triangular and they are symmetric if A and B are symmetric. Of course the zero matrix is in both subspaces. The Column Space of A We now come to the key examples the column space and the nullspace of a matrix A . The column space contains all linear combinations of the columns of A .  It is a subspace of R  . We illustrate by a system of m   equations in n   unknowns Combination of columns equals b                    u v      b  b  b     .  With m  n we have more equations than unknownsand usually there will be no solu tion .  The system will be solvable only for a very thin subset of all possible b s.  One way of describing this thin subset is so simple that it is easy to overlook. A The system Ax  b is solvable if and only if the vector b can be expressed as a combination of the columns of A . Then b is in the column space. This description involves nothing more than a restatement of Ax  b  by columns  Combination of columns u           v              b  b  b     .                                           These are the same three equations in two unknowns. Now the problem is Find numbers u and v that multiply the first and second columns to produce b . The system is solvable exactly when such coefficients exist and the vector  u  v  is the solution x . We are saying that the attainable righthand sides b are all combinations of the columns of A .  One possible righthand side is the first column itself the weights are u   and v  .  Another possibility is the second column u   and v  .  A third is the right hand side b  . With u   and v   the vector b   will always be attainable. We can describe all combinations of the two columns geometrically Ax  b can be solved if and only if b lies in the plane that is spanned by the two column vectors Figure .. This is the thin set of attainable b . If b lies off the plane then it is not a combination of the two columns. In that case Ax  b has no solution. What is important is that this plane is not just a subset of R  it is a subspace.  It is the column space of A  consisting of all combinations of the columns . It is denoted by   A  . Requirements i and ii for a subspace of R  are easy to check i  Suppose b and b  lie in the column space so that Ax  b for some x and Ax   b  for some x  .  Then A  x  x    b  b   so that b  b  is also a combination of the columns. The column space of all attainable vectors b is closed under addition. ii  If b is  in  the  column  space   A    so  is  any  multiple cb .   If  some  combination of columns produces b say Ax  b  then multiplying that combination by c will produce cb . In other words A  cx   cb . For another matrix A  the dimensions in Figure . may be very different. The small est possible column space one vector only comes from the zero matrix A  .  The   only combination of the columns is b  .  At the other extreme suppose A is the  by  identity matrix.  Then   I  is the whole of R   the five columns of I can combine to produce any fivedimensional vector b .  This is not at all special to the identity matrix. Any  by  matrix that is nonsingular will have the whole of R  as its column space . For such a matrix we can solve Ax  b by Gaussian elimination there are five pivots. Therefore every b is in   A  for a nonsingular matrix. You can see how Chapter  is contained in this chapter.  There we studied n by n matrices whose column space is R  .  Now we allow singular matrices and rectangu lar matrices of any shape.  Then   A  can be somewhere between the zero space and the  whole  space R  .   Together  with  its  perpendicular  space  it  gives  one  of  our  two approaches to understanding Ax  b . The Nullspace of A The second approach to Ax  b is dual to the first.  We are concerned not only with attainable righthand sides b  but also with the solutions x that attain them.  The right hand side b   always allows the solution x   but there may be infinitely many other solutions.  There always are if there are more unknowns than equations n  m . The solutions to Ax   form a vector spacethe nullspace of A . The nullspace of  a  matrix  consists  of  all  vectors x such  that Ax  .   It  is denoted  by   A  .   It  is  a  subspace  of R    just  as  the  column  space  was  a subspace of R  . Requirement i holds If Ax   and Ax    then A  x  x    . Requirement ii also holds If Ax   then A  cx   . Both requirements fail if the righthand side is not zero! Only the solutions to a homogeneous equation  b   form a subspace.  The nullspace is easy to find for the example given above it is as small as possible                    u v            . The first equation gives u   and the second equation then forces v  . The nullspace contains only the vector      . This matrix has independent columnsa key idea that comes soon. The situation is changed when a third column is a combination of the first two Larger nullspace B                             . B has the same column space as A . The new column lies in the plane of Figure . it is the sum of the two column vectors we started with. But the nullspace of B contains the  vector         and automatically contains any multiple  c  c   c   Nullspace is a line                             c   c  c            . The nullspace of B is the line of all points x  c  y  c  z   c . The line goes through the origin as any subspace must.  We want to be able for any system Ax  b  to find   A  and   A   all attainable righthand sides b and all solutions to Ax  . The vectors b are in the column space and the vectors x are in the nullspace. We shall compute the dimensions of those subspaces and a convenient set of vectors to generate them. We hope to end up by understanding all four of the subspaces that are intimately related to each other and to A the column space of A  the nullspace of A  and their two perpendicular spaces. Pr oblem Set . . Construct a subset of the x  y plane R  that is a  closed under vector addition and subtraction but not scalar multiplication. b  closed under scalar multiplication but not under vector addition. Hint  Starting with u and v  add and subtract for a. Try cu and cv for b. . Which of the following subsets of R  are actually subspaces? a  The plane of vectors  b   b   b   with first component b   . b  The plane of vectors b with b   . c  The vectors b with b  b    this is the union of two subspaces the plane b    and the plane b   . d  All combinations of two given vectors        and        . e  The plane of vectors  b   b   b   that satisfy b   b    b   . . Describe the column space and the nullspace of the matrices A         and B                  and C                  . . What is the smallest subspace of  by  matrices that contains all symmetric matrices and all lower triangular matrices?  What is the largest subspace that is contained in both of those subspaces? . Addition and scalar multiplication are required to satisfy these eight rules   . x  y  y  x . . x  y  z    x  y  z . .   There is a unique zero vector such that x    x for all x . .   For each x there is a unique vector  x such that x   x   . .    x  x . .  c  c   x  c   c  x  . . c  x  y   cx  cy . .  c   c   x  c  x  c  x . a  Suppose addition in R  adds an extra  to each component so that          equals      instead of      . With scalar multiplication unchanged which rules are broken? b  Show that the set of all positive real numbers with x  y and cx redefined to equal the usual xy and x   is a vector space. What is the zero vector? c  Suppose  x   x     y   y   is  defined  to  be  x   y   x   y   .   With  the  usual cx   cx   cx    which of the eight conditions are not satisfied? . Let P be the plane in space with equation x   y  z  .  What is the equation of the plane P  through the origin parallel to P ? Are P and P  subspaces of R  ? . Which of the following are subspaces of R  ? a  All sequences like         ...  that include infinitely many zeros. b  All sequences  x   x  ...  with x    from some point onward. c  All decreasing sequences x     x  for each j . d  All convergent sequences the x  have a limit as j   . e  All arithmetic progressions x     x  is the same for all j . f  All geometric progressions  x   kx   k  x  ...  allowing all k and x  . . Which of the following descriptions are correct? The solutions x of Ax                     x  x  x          form a  a plane. b  a line. c  a point. d  a subspace.  e  the nullspace of A . f  the column space of A . . Show that the set of nonsingular  by  matrices is not a vector space. Show also that the set of singular  by  matrices is not a vector space. . The matrix A          is a vector in the space M of all  by  matrices. Write the zero vector in this space the vector   A  and the vector  A . What matrices are in the smallest subspace containing A ? . a  Describe a subspace of M that contains A      but not B        . b  If a subspace of M contains A and B  must it contain I ? c  Describe a subspace of M that contains no nonzero diagonal matrices. . The functions f  x   x  and g  x    x are vectors in the vector space F of all real functions.  The combination  f  x    g  x  is the function h  x   . Which rule is broken if multiplying f  x  by c gives the function f  cx  ? . If the sum of the vectors f  x  and g  x  in F is defined to be f  g  x   then the zero vector is g  x   x .  Keep the usual scalar multiplication c f  x   and find two rules that are broken. . Describe the smallest subspace of the  by  matrix space M that contains a           and           . b           and           . c           . d                                 . . Let P be the plane in R  with equation x  y   z  .  The origin        is not in P ! Find two vectors in P and check that their sum is not in P . .   P  is the plane through        parallel to the plane P in Problem .  What is the equation for P  ? Find two vectors in P  and check that their sum is in P  . . The four types of subspaces of R  are planes lines R  itself or Z containing only        . a  Describe the three types of subspaces of R  . b  Describe the five types of subspaces of R  . . a  The intersection of two planes through        is probably a b ut it could be a . It cant be the zero vector Z ! b  The intersection of a plane through        with a line through        is prob ably a b ut it could be a .   c  If S and T are subspaces of R    their intersection S  T vectors in both sub spaces is a subspace of R  . Check the requirements on x  y and cx . . Suppose P is a plane through        and L is a line through        . The smallest vector space containing both P and L is either or . . T rue or false for M  all  by  matrices check addition using an example? a  The skewsymmetric matrices in M with A    A  form a subspace. b  The unsymmetric matrices in M with A    A  form a subspace. c  The matrices that have        in their nullspace form a subspace. Problems  are about column spaces   A  and the equation Ax  b . . Describe the column spaces lines or planes of these particular matrices A                    and B                    and C                    . . For which righthand sides find a condition on b   b   b   are these systems solvable? a                      x  x  x         b  b  b     . b                x  x       b  b  b     . . Adding row  of A to row  produces B . Adding column  to column  produces C . A combination of the columns of is also a combination of the columns of A . Which two matrices have the same column ? A          and B            and C            . . For which vectors  b   b   b   do these systems have a solution?                               x  x  x         b  b  b     and                               x  x  x         b  b  b     . . Recommended If we add an extra column b to a matrix A  then the column space gets larger unless . Give an example in which the column space gets larger and an example in which it doesnt.  Why is Ax  b solvable exactly when the column space doesnt get larger by including b ? . The columns of AB are combinations of the columns of A . This means The column space  of  AB  is  contained  in possibly  equal  to the  column  space  of  A .   Give  an example where the column spaces of A and AB are not equal.  . If A is any  by  invertible matrix then its column space is . Why? . True or false with a counterexample if false? a  The vectors b that are not in the column space   A  form a subspace. b  If   A  contains only the zero vector then A is the zero matrix. c  The column space of  A equals the column space of A . d  The column space of A  I equals the column space of A . . Construct a  by  matrix whose column space contains        and        but not        . Construct a  by  matrix whose column space is only a line. . If the  by  system Ax  b is solvable for every b  then   A   . . Wh y isnt R  a subspace of R  ? . Solving Ax   and Ax  b Chapter  concentrated on square invertible matrices. There was one solution to Ax  b and it was x   A   b . That solution was found by elimination not by computing A   . A rectangular matrix brings new possibilities U may not have a full set of pivots. This section goes onward from U to a reduced form R  the simplest matrix that elimina tion can give . R reveals all solutions immediately. For an invertible matrix the nullspace contains only x   multiply Ax   by A   . The column space is the whole space  Ax  b has a solution for every b . The new ques tions appear when the nullspace contains more than the zero vector andor the column space contains less than all vectors  .  Any vector x  in the nullspace can be added to a particular solution x  . The solutions to all linear equations have this form x  x   x   Complete solution Ax   b and Ax    produce A  x   x    b . .  When the column space doesnt contain every b in R   we need the conditions on b that make Ax  b solvable. A  by  example will be a good size. We will write down all solutions to Ax  . We will find the conditions for b to lie in the column space so that Ax  b is solvable. The  by  system  x  b  one equation and one unknown shows two possibilities  x  b has no solution unless b  .  The column space of the  by  zero matrix contains only b  .  x   has infinitely many solutions . The nullspace contains all x . A particular solution is x    and the complete solution is x  x   x     any x  .         Simple I admit. If you move up to  by  its more interesting. The matrix     is not invertible y  z  b  and  y   z  b  usually have no solution. There is no solution unless b    b  .  The column space of A contains only those b s the multiples of      . When b    b  there are infinitely many solutions .  A particular solution to y  z   and  y   z   is x        .   The nullspace of A in Figure . contains       and all its multiples x     c  c   Complete solution y  z    y   z   is solved by x   x        c          c   c  .                                                             Echelon Form U and Row Reduced Form R We start by simplifying this  by  matrix first to U and then further to R  Basic example A                              . The pivot a    is nonzero. The usual elementary operations will produce zeros in the first column below this pivot. The bad news appears in column  No pivot in column  A                              . The candidate for the second pivot has become zero unacceptable . We look below that zero for a nonzero entryintending to carry out a row exchange.  In this case the entry below it is also zero .  If A were square this would signal that the matrix was singular. With a rectangular matrix we must expect trouble anyway and there is no reason to stop.  All we can do is to go on to the next column  where the pivot entry is . Subtracting twice the second row from the third we arrive at U  Echelon matrix U U                                . Strictly speaking we proceed to the fourth column. A zero is in the third pivot position and nothing can be done. U is upper triangular but its pivots are not on the main diago nal. The nonzero entries of U have a staircase pattern or echelon form . For the  by  case in Figure . the starred entries may or may not be zero.                                                                                           We can always reach this echelon form U  with zeros below the pivots .  The pivots are the first nonzero entries in their rows. .  Below each pivot is a column of zeros obtained by elimination. .  Each pivot lies to the right of the pivot in the row above. This produces the staircase pattern and zero rows come last. Since we started with A and ended with U  the reader is certain to ask  Do we have A  LU as before?  There is no reason why not since the elimination steps have not changed.  Each step still subtracts a multiple of one row from a row beneath it.  The inverse of each step adds back the multiple that was subtracted. These inverses come in the right order to put the multipliers directly into L  Lower triangular L                          and A  LU . Note that L is square. It has the same number of rows as A and U . The only operation not required by our example but needed in general is row ex change by a permutation matrix P .  Since we keep going to the next column when no pivots are available there is no need to assume that A is nonsingular.  Here is PA  LU for all matrices         B For any m by n matrix A there is a permutation P  a lower triangular L with unit diagonal and an m by n echelon matrix U  such that PA  LU . Now comes R .  We can go further than U  to make the matrix even simpler.  Divide the second row by its pivot  so that all pivots are . Then use the pivot row to produce zero above the pivot .  This time we subtract a row from a higher row .  The final result the best form we can get is the reduced row echelon form R                                                                                                                 R . This matrix R is the final result of elimination on A . MATLAB would use the command R  rrefA . Of course rrefR would give R again! What is the row reduced form of a square invertible matrix?  In that case R is the identity matrix .  There is a full set of pivots all equal to  with zeros above and below. So rrefA  I  when A is invertible. For a  by  matrix with four pivots Figure . shows the reduced form R . It still contains an identity matrix in the four pivot rows and four pivot columns . From R we will quickly find the nullspace of A . Rx   has the same solutions as U x   and Ax  . Pivot Variables and Free Variables Our goal is to read off all the solutions to Rx  . The pivots are crucial Nullspace of R pivot columns in boldface Rx                           v  y                . The unknowns u  v  w  y go into two groups.  One group contains the pivot variables  those that correspond to columns with pivots .  The first and third columns contain the pivots  so  and  are  the  pivot  variables.   The  other  group  is  made  up  of  the free variables  corresponding to columns without pivots .  These are the second and fourth columns so v and y are free variables. To find the most general solution to Rx   or equivalently to Ax   we may assign arbitrary values to the free variables. Suppose we call these values simply v and y . The pivot variables are completely determined in terms of v and y  Rx      v  y   yields     v  y   y   yields    y   There is a double infinity of solutions with v and y free and independent.  The com plete solution is a combination of two special solutions  Nullspace contains all combinations of special solutions x         v  y v  y y       v                 y                .  Please look again at this complete solution to Rx   and Ax  .  The special solution           has free variables v   y  .  The other special solution           has v   and y  . All solutions are linear combinations of these two . The best way to find all solutions to Ax   is from the special solutions .  After reaching Rx   identify the pivot variables and free variables. .  Give one free variable the value  set the other free variables to  and solve Rx   for the pivot variables. This x is a special solution. .  Every free variable produces its own special solution by step . The combinations of special solutions form the nullspaceall solutions to Ax  . Within the fourdimensional space of all possible vectors x  the solutions to Ax   form a twodimensional subspace the nullspace of A  In the example   A  is gener ated by the special vectors           and           . The combinations of these two vectors produce the whole nullspace. Here is a little trick. The special solutions are especially easy from R . The numbers  and  and   and  lie in the nonpivot columns of R . Reverse their signs to find the pivot variables not free in the special solutions .  I will put the two special solutions from equation  into a nullspace matrix N  so you see this neat pattern Nullspace matrix columns are special solutions N                      not free free not free free The free variables have values  and .   When the free columns moved to the right hand side of equation  their coefficients  and  and   and  switched sign.  That determined the pivot variables in the special solutions the columns of N . This is the place to recognize one extremely important theorem. Suppose a matrix has more columns than rows n  m . Since m rows can hold at most m pivots there must be at least n  m free variables .  There will be even more free variables if some rows of R reduce to zero but no matter what at least one variable must be free. This free variable can be assigned any value leading to the following conclusion C If Ax   has more unknowns than equations  n  m  it has at least one special solution There are more solutions than the trivial x  .         There must be infinitely many solutions since any multiple cx will also satisfy A  cx   .  The nullspace contains the line through x .  And if there are additional free variables the nullspace becomes more than just a line in n dimensional space. The nullspace has the same dimension as the number of free variables and special solutions . This central ideathe dimension of a subspaceis made precise in the next section. We count the free variables for the nullspace. We count the pivot variables for the column space! Solving Ax  b  U x  c  and Rx  d The case b    is quite different from b  .  The row operations on A must act also on the righthand side on b .  We begin with letters  b   b   b   to find the solvability conditionfor b to lie in the column space.  Then we choose b         and find all solutions x . For the original example Ax  b   b   b   b    apply to both sides the operations that led from A to U . The result is an upper triangular system U x  c  U x  c                                          u v w y          b  b    b  b    b    b     .  The vector c on the righthand side which appeared after the forward elimination steps is just L   b as in the previous chapter. Start now with U x  c . It is not clear that these equations have a solution.  The third equation is very much in doubt because its lefthand side is zero. The equations are inconsistent unless b    b    b   .  Even though there are more unknowns than equations there may be no solution. We know another way of answering the same question Ax  b can be solved if and only if b lies in the column space of A . This subspace comes from the four columns of A not of U ! Columns of A span the column space                                          . Even though there are four vectors  their combinations only fill out a plane in three dimensional space.  Column  is three times column .  The fourth column equals the third minus the first. These dependent columns the second and fourth are exactly the ones without pivots . The column space   A  can be described in two different ways.  On the one hand it  is the  plane  generated  by  columns    and   .   The  other  columns  lie  in  that  plane and contribute nothing new.   Equivalently  it is the plane of all vectors b that satisfy b    b    b    this is the constraint if the system is to be solvable. Every column  satisfies this constraint so it is forced on b ! Geometrically we shall see that the vector         is perpendicular to each column. If b belongs to the column space the solutions of Ax  b are easy to find.  The last equation in U x  c is   .  To the free variables v and y  we may assign any values as before.  The pivot variables u and w are still determined by backsubstitution.  For a specific example with b    b    b    choose b          Ax  b                                  u v w y                . Forward elimination produces U on the left and c on the right U x  c                                          u v w y                . The last equation is    as expected. Backsubstitution gives  w   y   or w    y u   v   w   y   or u      v  y . Again there is a double infinity of solutions v and y are free u and w are not Complete solution x  x   x  x       u v w y                       v                 y                .  This has all solutions to Ax   plus the new x             . That x  is a particular solution to Ax  b .  The last two terms with v and y yield more solutions because they satisfy Ax  . Every solution to Ax  b is the sum of one particular solution and a solution to Ax   x   x   x  The particular solution in equation  comes from solving the equation with all free variables set to zero . That is the only new part since the nullspace is already computed. When you multiply the highlighted equation by A  you get Ax   b  . Geometrically the solutions again fill a twodimensional surfacebut it is not a sub space.  It does not contain x  .  It is parallel to the nullspace we had before shifted by the particular solution x  as in Figure ..  Equation  is a good way to write the answer .  Reduce Ax  b to U x  c .         .  With free variables   find a particular solution to Ax   b and U x   c . .  Find the special solutions to Ax   or U x   or Rx  .  Each free variable in turn is . Then x  x   any combination x  of special solutions. When the equation was Ax   the particular solution was the zero vector! It fits the pat tern but x    was not written in equation . Now x  is added to the nullspace solutions as in equation . Question  How does the reduced form R make this solution even clearer?  You will see it in our example.  Subtract equation  from equation  and then divide equation  by its pivot.  On the lefthand side this produces R  as before.  On the righthand side these operations change c         to a new vector d           Reduced equation Rx  d                                     u v w y                 .  Our  particular  solution x    one  choice  out  of  many  has  free  variables v  y  . Columns  and  can be ignored. Then we immediately have u    and w   exactly as in equation . The entries of d go directly into x  .  This is because the identity matrix is sitting in the pivot columns of R ! Let me summarize this section before working a new example.  Elimination reveals the pivot variables and free variables. If there are r pivots there are r pivot variables and n  r free variables . That important number r will be given a nameit is the rank of the matrix . D Suppose elimination reduces Ax  b to U x  c and Rx  d  with r pivot rows and r pivot columns. The rank of those matrices is r .  The last m  r rows of U and R are zero so there is a solution only if the last m  r entries of c and d are also zero. The complete solution is x  x   x  .  One particular solution x  has all free variables zero. Its pivot variables are the first r entries of d  so Rx   d . The nullspace solutions x  are combinations of n  r special solutions with one free variable equal to . The pivot variables in that special solution can be found in the corresponding column of R with sign reversed. You see how the rank r is crucial.  It counts the pivot rows in the row space and the pivot columns in the column space.  There are n  r special solutions in the nullspace. There are m  r solvability conditions on b or c or d .  Another Worked Example The full picture uses elimination and pivot columns to find the column space nullspace and rank. The  by  matrix A has rank  Ax  b is  x    x    x    x   b   x    x    x    x   b   x    x    x    x   b   .  Reduce  A  b  to  U  c   to reach a triangular system U x  c . .  Find the condition on b   b   b  to have a solution. .  Describe the column space of A  Which plane in R  ? .  Describe the nullspace of A  Which special solutions in R  ? .  Find a particular solution to Ax          and the complete x   x  . .  Reduce  U  c  to  R  d   Special solutions from R and x  from d . Solution. Notice how the righthand side is included as an extra column! .  The multipliers in elimination are  and  and   taking  A  b  to  U  c  .                                                                                                                              . .  The last equation shows the solvability condition b   b    b   . Then   . .  The column space of A is the plane containing all combinations of the pivot columns        and        . Second description The column space contains all vectors with b   b    b   . That makes Ax  b solvable so b is in the column space. All columns of A pass this test b   b    b    . This is the equation for the plane  in the first description of the column space . .  The special solutions in N have free variables x    x    and x    x    Nullspace matrix Special solutions to Ax   Backsubstitution in U x   Just switch signs in Rx   N                       .         .  Choose b           which has b   b    b   .  Elimination takes Ax  b to U x  c         . Backsubstitute with free variables   Particular solution to Ax           x                  free free The complete solution to Ax          is this x    all x  . .  In the reduced R  the third column changes from        to        . The righthand side c         becomes d          . Then   and  go into x    U   c                                         R   d                                 . That final matrix  R  d  is rref  A  b   rref  U  c  .  The numbers  and  and  and  in the free columns of R have opposite sign in the special solutions the nullspace matrix N . Everything is revealed by Rx  d . Pr oblem Set . . Construct a system with more unknowns than equations but no solution. Change the righthand side to zero and find all solutions x  . . Reduce A and B to echelon form to find their ranks. Which variables are free? A                                      B                             . Find the special solutions to Ax   and Bx  . Find all solutions. . Find the echelon form U  the free variables and the special solutions A                         b   b  b   . Ax  b is consistent has a solution when b satisfies b   . Find the complete solution in the same form as equation .  . Carry out the same steps as in the previous problem to find the complete solution of Mx  b  M                             b       b  b  b  b       . . Write the complete solutions x  x   x  to these systems as in equation                     u v w                             u v w         . . Describe the set of attainable righthand sides b in the column space for                    u v      b  b  b      by finding the constraints on b that turn the third equation into    after elimina tion. What is the rank and a particular solution? . Find the value of c that makes it possible to solve Ax  b  and solve it u  v   w    u   v  w    u   v  w  c . . Under what conditions on b  and b  if any does Ax  b have a solution? A                         b   b  b   . Find two vectors in the nullspace of A  and the complete solution to Ax  b . . a  Find the special solutions to U x  . Reduce U to R and repeat U x                                           x  x  x  x                 . b  If the righthand side is changed from        to  a  b     what are all solutions? . Find a  by  system Ax  b whose complete solution is x            w          .         Find a  by  system with these solutions exactly when b   b   b  . . Write a  by  system Ax  b with many solutions x  but no solution x  . Therefore the system has no solution. Which b s allow an x  ? . Which of these rules give a correct definition of the rank of A ? a  The number of nonzero rows in R . b  The number of columns minus the total number of rows. c  The number of columns minus the number of free columns. d  The number of s in R . . Find the reduced row echelon forms R and the rank of these matrices a  The  by  matrix of all s. b  The  by  matrix with a        . c  The  by  matrix with a        . . Find R for each of these block matrices and the special solutions A                             B   A   A  C   A   A A   . . If the r pivot variables come first the reduced R must look like R   I    F      I is r by r F is r by n  r What is the nullspace matrix N containing the special solutions? . Suppose  all r pivot  variables  come last .   Describe  the  four  blocks  in  the m by n reduced echelon form the block B should be r by r  R   A   B C   D  . What is the nullspace matrix N of special solutions? What is its shape? . Silly problem Describe all  by  matrices A  and A  with row echelon forms R  and R   such that R   R  is the row echelon form of A   A  . Is it true that R   A  and R   A  in this case? . If A has r pivot columns then A  has r pivot columns.  Give a  by  example for which the column numbers are different for A and A  .  . What are the special solutions to Rx   and R  y   for these R ? R                                      R                             . . If A has rank r then it has an r by r submatrix S that is invertible . Find that submatrix S from the pivot rows and pivot columns of each A  A                  A                  A                             . . Explain why the pivot rows and pivot columns of A not R  always give an r by r invertible submatrix of A . . Find the ranks of AB and AM rank  matrix times rank  matrix A            and B          .      and M    b c   bc  . . Multiplying the rank  matrices A  uv  and B  wz  gives uz  times the number . AB has rank  unless  . . Ev ery column of AB is a combination of the columns of A .  Then the dimensions of the column spaces give rank  AB   rank  A  . Problem Prove also that rank  AB   rank  B  . . Important Suppose A and B are n by n matrices and AB  I . Prove from rank  AB   rank  A  that the rank of A is n . So A is invertible and B must be its twosided inverse. Therefore BA  I  which is not so obvious! . . If A is  by  and C is  by  show from its rank that CA   I .  Give an example in which AC  I . For m  n  a right inverse is not a left inverse. . Suppose A and B have the same reducedrow echelon form R . Explain how to change A to B by elementary row operations. So B equals an matrix times A . . Every m by n matrix of rank r reduces to  m by r  times  r by n  A   pivot columns of A  first r rows of R    COL  ROW  . Write the  by  matrix A at the start of this section as the product of the  by  matrix from the pivot columns and the  by  matrix from R  A                                      . Suppose A is an m by n matrix of rank r .  Its reduced echelon form is R .  Describe exactly the reduced row echelon form of  R  not A  . . Recommended  Execute  the  six  steps  following  equation    to  find  the  column space and nullspace of A and the solution to Ax  b  A                                      b     b  b  b               . . For every c  find R and the special solutions to Ax   A                          c        and A     c     c  . . What is the nullspace matrix N of special solutions for A  B  C ? A   I   I  and B   I    I      and C   I   I   I  . Problems  are about the solution of Ax  b . Follow the steps in the text to x  and x  . Reduce the augmented matrix  A  b  . . Find the complete solutions of x   y   z    x   y   z    x   y   z   and                                          x y z t                . . Under what condition on b   b   b  is the following system solvable? Include b as a fourth column in  A  b  . Find all solutions when that condition holds x   y   z  b   x   y   z  b   x   y   z  b  . . What conditions on b   b   b   b  make each system solvable? Solve for x                             x  x         b  b  b  b                                          x  x  x           b  b  b  b       .  . Which vectors  b   b   b   are in the column space of A ? Which combinations of the rows of A give zero? a A                             b A                             . . Why cant a  by  system have x          and x   any multiple of        ? . a  If Ax  b has two solutions x  and x   find two solutions to Ax  . b  Then find another solution to Ax  b . . Explain why all these statements are false a  The complete solution is any linear combination of x  and x  . b  A system Ax  b has at most one particular solution. c  The solution x  with all free variables zero is the shortest solution minimum length  x  . Find a  by  counterexample. d  If A is invertible there is no solution x  in the nullspace. . Suppose column  of U has no pivot.  Then x  is a v ariable.  The zero vector is is not the only solution to Ax  .  If Ax  b has a solution then it has solutions. . If you know x  free variables   and all special solutions for Ax  b  find x  and all special solutions for these systems Ax   b  A   A   x X   b  A A   x    b b  . . If Ax  b has infinitely many solutions why is it impossible for Ax  B new right hand side to have only one solution? Could Ax  B have no solution? . Choose the number q so that if possible the ranks are a  b  c  A                q    and B          q  q  . . Give examples of matrices A for which the number of solutions to Ax  b is a   or  depending on b . b   regardless of b . c   or   depending on b . d   regardless of b .         . Write all known relations between r and m and n if Ax  b has a  no solution for some b . b  infinitely many solutions for every b . c  exactly one solution for some b  no solution for other b . d  exactly one solution for every b . . Apply GaussJordan elimination righthand side becomes extra column to U x   and U x  c . Reach Rx   and Rx  d   U                      and  U   c                     . Solve Rx   to find x  its free variable is x   . Solve Rx  d to find x  its free variable is x   . . Apply elimination with the extra column to reach Rx   and Rx  d   U                                        and  U   c                                 . Solve Rx   free variable  . What are the solutions to Rx  d ? . Reduce to U x  c Gaussian elimination and then Rx  d  Ax                                           x  x  x  x                  b . Find a particular solution x  and all nullspace solutions x  . . Find A and B with the given property or explain why you cant. a  The only solution to Ax       is x      . b  The only solution to Bx      is x       . . The complete solution to Ax      is x       c     . Find A . . The nullspace of a  by  matrix A is the line through          . a  What is the rank of A and the complete solution to Ax  ? b  What is the exact row reduced echelon form R of A ?  . Reduce these matrices A and B to their ordinary echelon forms U  a A                                               b B                             . Find a special solution for each free variable and describe every solution to Ax   and Bx  .  Reduce the echelon forms U to R  and draw a box around the identity matrix in the pivot rows and pivot columns. . True or False? Give reason if true or counterexample to show it is false. a  A square matrix has no free variables. b  An invertible matrix has no free variables. c  An m by n matrix has no more than n pivot variables. d  An m by n matrix has no more than m pivot variables. . Is there a  by  matrix with no zero entries for which U  R  I ? . Put as many s as possible in a  by  echelon matrix U and in a reduced form R whose pivot columns are   . . Suppose column  of a  by  matrix is all s.  Then x  is certainly a v ariable. The special solution for this variable is the vector x  . . Suppose the first and last columns of a  by  matrix are the same nonzero.  Then is a free variable. Find the special solution for this variable. . The equation x   y  z   determines a plane in R  .  What is the matrix A in this equation? Which are the free variables? The special solutions are        and . The parallel plane x   y  z   contains the particular point        . All points on this plane have the following form fill in the first components    x y z              y          z         . . Suppose column   column   column    in a  by  matrix with four pivots. Which column is sure to have no pivot and which variable is free?  What is the special solution? What is the nullspace? Problems  ask for matrices if possible with specific properties. . Construct a matrix whose nullspace consists of all combinations of          and          . . Construct a matrix whose nullspace consists of all multiples of          .   . Construct a matrix whose column space contains        and     .   and whose nullspace contains        . . Construct a matrix whose column space contains        and        and whose nullspace contains        and        . . Construct a matrix whose column space contains        and whose nullspace is the line of multiples of          . . Construct a  by  matrix whose nullspace equals its column space. . Why does no  by  matrix have a nullspace that equals its column space? . The reduced form R of a  by  matrix with randomly chosen entries is almost sure to be . What R is virtually certain if the random A is  by ? . Show by example that these three statements are generally false a A and A  have the same nullspace. b A and A  have the same free variables. c  If R is the reduced form rref  A  then R  is rref  A   . . If the special solutions to Rx   are in the columns of these N  go backward to find the nonzero rows of the reduced matrices R  N                    and N           and N        empty  by . . Explain why A and  A always have the same reduced echelon form R . . Linear Independence Basis and Dimension By themselves the numbers m and n give an incomplete picture of the true size of a linear system. The matrix in our example had three rows and four columns but the third row was only a combination of the first two.  After elimination it became a zero row It had no effect on the homogeneous problem Ax  .  The four columns also failed to be independent and the column space degenerated into a twodimensional plane. The important number that is beginning to emerge the true size is the rank r .  The rank was introduced as the number of pivots in the elimination process.  Equivalently the final matrix U has r nonzero rows. This definition could be given to a computer. But it would be wrong to leave it there because the rank has a simple and intuitive meaning The rank counts the number of genuinely independent rows in the matrix A .  We want definitions that are mathematical rather than computational. The goal of this section is to explain and use four ideas  .  Linear independence or dependence. .  Spanning a subspace. .  Basis for a subspace a set of vectors. .  Dimension of a subspace a number. The first step is to define linear independence .  Given a set of vectors v  ... v   we look at their combinations c  v   c  v     c  v  .   The trivial combination  with all weights c    obviously produces the zero vector  v      v   . The question is whether this is the only way to produce zero. If so the vectors are independent. If any other combination of the vectors gives zero they are dependent . E Suppose c  v     c  v    only happens when c     c   . Then the vectors v  ... v  are linearly independent .  If any c s are nonzero the v s are linearly dependent . One vector is a combination of the others. Linear dependence is easy to visualize in threedimensional space when all vectors go out from the origin.  Two vectors are dependent if they lie on the same line. Three vectors are dependent if they lie in the same plane .  A random choice of three vectors without any special accident should produce linear independence not in a plane. Four vectors are always linearly dependent in R  . Example . If v   zero vector then the set is linearly dependent.  We may choose c    and all other c    this is a nontrivial combination that produces zero. Example . The columns of the matrix A                              are linearly dependent since the second column is three times the first. The combination of columns with weights      gives a column of zeros. The rows are also linearly dependent row  is two times row  minus five times row . This is the same as the combination of b   b   b   that had to vanish on the righthand side in order for Ax  b to be consistent.  Unless b    b    b    the third equation would not become   . Example . The columns of this triangular matrix are linearly independent  No zeros on the diagonal A                             .   Look for a combination of the columns that makes zero Solve Ac   c            c            c                     . We  have  to  show  that c   c   c  are  all  forced  to  be  zero .   The  last  equation  gives c   . Then the next equation gives c    and substituting into the first equation forces c   . The only combination to produce the zero vector is the trivial combination. The nullspace of A contains only the zero vector c   c   c   . The columns of A are independent exactly when   A    zero vector  . A similar reasoning applies to the rows of A  which are also independent. Suppose c         c         c                 . From the first components we find  c    or c   . Then the second components give c    and finally c   . The nonzero rows of any echelon matrix U must be independent. Furthermore if we pick out the columns that contain the pivots  they also are linearly independent.  In our earlier example with Two independent rows Two independent columns U                                       the pivot columns  and  are independent. No set of three columns is independent and certainly not all four.  It is true that columns  and  are also independent but if that last  were changed to  they would be dependent. It is the columns with pivots that are guaranteed to be independent . The general rule is this F The r nonzero rows of an echelon matrix U and a reduced matrix R are linearly independent. So are the r columns that contain pivots. Example . The columns of the n by n identity matrix are independent I                                      . These columns e  ... e  represent unit vectors in the coordinate directions in R   e                  e                  e                  e                 .  Most sets of four vectors in R  are independent. Those e s might be the safest. To check any set of vectors v  ... v  for independence put them in the columns of A . Then solve the system Ac   the vectors are dependent if there is a solution other than c  . With no free variables  rank n  there is no nullspace except c   the vectors are independent. If the rank is less than n  at least one free variable can be nonzero and the columns are dependent. One case has special importance. Let the n vectors have m components so that A is an m by n matrix. Suppose now that n  m . There are too many columns to be independents There cannot be n pivots since there are not enough rows to hold them.  The rank will be less than n .  Every system Ac   with more unknowns than equations has solutions c   . G A set of n vectors in R  must be linearly dependent if n  m . The reader will recognize this as a disguised form of C Every m by n system Ax   has nonzero solutions if n  m . Example . These three columns in R  cannot be independent A                  . To find the combination of the columns producing zero we solve Ac   A  U                  . If we give the value  to the free variable c    then backsubstitution in U c   gives c     c   .  With these three weights the first column minus the second plus the third equals zero Dependence. Spanning a Subspace Now we define what it means for a set of vectors to span a space . The column space of A is spanned by the columns. Their combinations produce the whole space  H If a vector space V consists of all linear combinations of w  ... w   then these vectors span the space.  Every vector v in V is some combination of the w s Every v comes from w s v  c  w     c  w  for some coefficients c  . It is permitted that a different combination of w s could give the same vector v .  The c s need not be unique because the spanning set might be excessively largeit could include the zero vector or even all vectors.   Example . The vectors w           w           and w           span a plane the x  y plane in R  . The first two vectors also span this plane whereas w  and w  span only a line. Example . The column space of A is exactly the space that is spanned by its columns . The row space is spanned by the rows.  The definition is made to order.  Multiplying A by any x gives a combination of the columns it is a vector Ax in the column space. The coordinate vectors e  ... e  coming from the identity matrix span R  .  Every vector b   b  ... b   is a combination of those columns.  In this example the weights are  the  components b  themselves b  b  e     b  e  .   But  the  columns  of  other matrices also span R  ! Basis for a Vector Space To decide if b is a combination of the columns we try to solve Ax  b .  To decide if the columns are independent we solve Ax  . Spanning involves the column space and independence involves the nullspace .  The coordinate vectors e  ... e  span R  and they are linearly independent. Roughly speaking no vectors in that set are wasted . This leads to the crucial idea of a basis . I A basis for V is a sequence of vectors having two properties at once .  The vectors are linearly independent not too many vectors. .  They span the space V not too few vectors. This combination of properties is absolutely fundamental to linear algebra. It means that every vector in the space is a combination of the basis vectors because they span. It  also  means  that  the  combination  is  unique  If v  a  v     a  v  and  also v  b  v     b  v   then subtraction gives     a   b   v  .  Now independence plays its part every coefficient a   b  must be zero.  Therefore a   b  . There is one and only one way to write v as a combination of the basis vectors . We had better say at once that the coordinate vectors e  ... e  are not the only basis for R  .   Some  things  in  linear  algebra  are  unique  but  not  this.   A  vector  space  has infinitely many different bases . Whenever a square matrix is invertible its columns are independentand they are a basis for R  .  The two columns of this nonsingular matrix are a basis for R   A            Every twodimensional vector is a combination of those independent! columns. Example . The x  y plane in Figure . is just R  .  The vector v  by itself is linearly independent but it fails to span R  .  The three vectors v   v   v  certainly span R   but are not independent. Any two of these vectors say v  and v   have both propertiesthey  span and they are independent.  So they form a basis.  Notice again that a vector space does not have a unique basis .                                       Example . These four columns span the column space of U  but they are not indepen dent Echelon matrix U                                      . There are many possibilities for a basis but we propose a specific choice The columns that contain pivots in this case the first and third which correspond to the basic vari ables are a basis for the column space . These columns are independent and it is easy to see that they span the space.   In fact  the column space of U is just the x  y plane within R  .   U  is not the same as the column space   A  before eliminationbut the number of independent columns didnt change. To summarize The columns of any matrix span its column space . If they are indepen dent they are a basis for the column spacewhether the matrix is square or rectangular. If we are asking the columns to be a basis for the whole space R   then the matrix must be square and invertible. Dimension of a Vector Space A space has infinitely many different bases but there is something common to all of these choices. The number of basis vectors is a property of the space itself J Any two bases for a vector space V contain the same number of vec tors.  This number which is shared by all bases and expresses the number of degrees of freedom of the space is the dimension of V . We have to prove this fact  All possible bases contain the same number of vectors. The x  y plane in Figure . has two vectors in every basis its dimension is .  In three   dimensions we need three vectors  along the x  y  z axes or in three other linearly in dependent!  directions. The dimension of the space R  is n .  The column space of U in Example  had dimension  it was a twodimensional subspace of R  .  The zero matrix is rather exceptional because its column space contains only the zero vector. By convention the empty set is a basis for that space and its dimension is zero. Here is our first big theorem in linear algebra K If v  ... v  and w  ... w  are both bases for the same vector space then m  n . The number of vectors is the same. Proof. Suppose there are more w s than v s  n  m .  We will arrive at a contradiction. Since the v s form a basis  they must span the space. Every w  can be written as a combination of the vs   If w   a  v     a   v   this is the first column of a matrix multiplication VA  W   w  w   w     v   v      a  . . . a       VA . We dont know each a   but we know the shape of A it is m by n .  The second vector w  is also a combination of the v s.  The coefficients in that combination fill the second column of A .  The key is that A has a row for every v and a column for every w . A is a short wide matrix since n  m . There is a nonzero solution to Ax  . Then VAx   which is W x  . A combination of the ws gives zero! The w s could not be a basisso we cannot have n  m . If m  n we exchange the v s and w s and repeat the same steps.  The only way to avoid a contradiction is to have m  n . This completes the proof that m  n . To repeat The dimension of a space is the number of vectors in every basis. This proof was used earlier to show that every set of m   vectors in R  must be de pendent. The v s and w s need not be column vectorsthe proof was all about the matrix A of coefficients.  In fact we can see this general result In a subspace of dimension k no set of more than k vectors can be independent and no set of more than k vectors can. span the space . There are other dual theorems of which we mention only one. We can start with a set of vectors that is too small or too big and end up with a basis L Any linearly independent set in V can be extended to a basis by adding more vectors if necessary. Any  spanning  set  in V can  be  reduced  to  a  basis  by  discarding  vectors  if necessary. The point is that a basis is a maximal independent set . It cannot be made larger without losing independence. A basis is also a minimal spanning set . It cannot be made smaller and still span the space.  You must notice that the word dimensional is used in two different ways. We speak about a fourdimensional vector  meaning a vector in R  .  Now we have defined a four dimensional subspace  an example is the set of vectors in R  whose first and last com ponents are zero.  The members of this fourdimensional subspace are sixdimensional vectors like              . One final note about the language of linear algebra. We never use the terms basis of a matrix or rank of a space or dimension of a basis. These phrases have no meaning. It is the dimension of the column space that equals the rank of the matrix  as we prove in the coming section. Pr oblem Set . Problems  are about linear independence and linear dependence. . Show that v   v   v  are independent but v   v   v   v  are dependent v            v            v            v            . Solve c  v     c  v    or Ac  . The v s go in the columns of A . . Find the largest possible number of independent vectors among v                  v                  v                  v                  v                  v                  . This number is the of the space spanned by the v s. . Prove that if a   d   or f    cases the columns of U are dependent U     a   b   c  d    e     f    . . If a  d  f in Problem  are all nonzero show that the only solution to U x   is x  . Then U has independent columns. . Decide the dependence or independence of a  the vectors                 and   .     . b  the vectors                   and         .   . Choose three independent columns of U . Then make two other choices. Do the same for A . You have found bases for which spaces? U                                                    and A                                                    . . If w   w   w  are independent vectors show that the differences v   w   w   v   w   w   and v   w   w  are dependent .  Find a combination of the v s that gives zero. . If w   w   w  are independent vectors show that the sums v   w   w   v   w   w   and v   w   w  are independent . Write c  v   c  v   c  v    in terms of the w s. Find and solve equations for the c s. . Suppose v   v   v   v  are vectors in R  . a  These four vectors are dependent because . b  The two vectors v  and v  will be dependent if . c  The vectors v  and        are dependent because . . Find two independent vectors on the plane x   y   z  t   in R  . Then find three independent vectors. Why not four? This plane is the nullspace of what matrix? Problems  are about the space spanned by a set of vectors. Take all linear combinations of the vectors . Describe the subspace of R  is it a line or a plane or R  ? spanned by a  the two vectors         and          . b  the three vectors        and        and        . c  the columns of a  by  echelon matrix with  pivots. d  all vectors with positive components. . The vector b is in the subspace spanned by the columns of A when there is a solution to . The vector c is in the row space of A when there is a solution to . T rue or false  If the zero vector is in the row space the rows are dependent. . Find the dimensions of a the column space of A  b the column space of U  c the row space of A  d the row space of U . Which two of the spaces are the same? A                        and U                             .  . Choose x   x   x   x   x   in R  .   It has  rearrangements like  x   x   x   x   and  x   x   x   x   . Those  vectors including x itself span a subspace S . Find specific vectors x so that the dimension of S is a  b  c  d . . v  w and v  w are combinations of v and w. Write v and w as combinations of v  w and v  w . The two pairs of vectors the same space. When are they a basis for the same space? . Decide whether or not the following vectors are linearly independent  by solving c  v   c  v   c  v   c  v    v                  v                  v                  v                 . Decide also if they span R   by trying to solve c  v     c  v            . . Suppose the vectors to be tested for independence are placed into the rows instead of the columns of A  How does the elimination process from A to U decide for or against independence? . To decide whether b is in the sub space spanned by w  ... w   let the vectors w be the columns of A and try to solve Ax  b . What is the result for a w           w           w           b         ? b w           w           w           w           and any b ? Problems  are about the requirements for a basis. . If v  ... v  are linearly independent the space they span has dimension . These vectors are a for that space. If the vectors are the columns of an m by n matrix then m is than n . . Find a basis for each of these subspaces of R   a  All vectors whose components are equal. b  All vectors whose components add to zero. c  All vectors that are perpendicular to          and          . d  The column space in R   and nullspace in R   of U      . . Find three different bases for the column space of U above. Then find two different bases for the row space of U . . Suppose v   v  ... v  are six vectors in R  . a  Those vectors dodo notmight not span R  .   b  Those vectors areare notmight be linearly independent. c  Any four of those vectors areare notmight be a basis for R  . d  If those vectors are the columns of A  then Ax  b has does not have might not have a solution. . The columns of A are n vectors from R  .  If they are linearly independent what is the rank of A ?  If they span R   what is the rank?  If they are a basis for R   what then? . Find a basis for the plane x   y   z   in R  . Then find a basis for the intersection of that plane with the xy plane. Then find a basis for all vectors perpendicular to the plane. . Suppose the columns of a  by  matrix A are a basis for R  . a  The equation Ax   has only the solution x   because . b  If b is in R  then Ax  b is solvable because . Conclusion A is invertible. Its rank is . . Suppose S is a fivedimensional subspace of R  . True or false? a  Every basis for S can be extended to a basis for R  by adding one more vector. b  Every basis for R  can be reduced to a basis for S by removing one vector. . U comes from A by subtracting row  from row  A                             and U                             . Find bases for the two column spaces. Find bases for the two row spaces. Find bases for the two nullspace. . True or false give a good reason? a  If the columns of a matrix are dependent so are the rows. b  The column space of a  by  matrix is the same as its row space. c  The column space of a  by  matrix has the same dimension as its row space. d  The columns of a matrix are a basis for the column space. . For which numbers c and d do these matrices have rank ? A                      c            d     and B   c   d d   c  .  . By locating the pivots find a basis for the column space of U                                                    . Express each column that is not in the basis as a combination of the basic columns Find also a matrix A with this echelon form U  but a different column space. . Find a counterexample to the following statement If v   v   v   v  is a basis for the vector space R   and if W is a subspace then some subset of the v s is a basis for W . . Find the dimensions of these vector spaces a  The space of all vectors in R  whose components add to zero. b  The nullspace of the  by  identity matrix. c  The space of all  by  matrices. . Suppose V is known to have dimension k . Prove that a  any k independent vectors in V form a basis b  any k vectors that span V form a basis. In other words if the number of vectors is known to be correct either of the two properties of a basis implies the other. . Prove that if V and W are threedimensional subspaces of R   then V and W must have a nonzero vector in common. Hint   Start with bases for the two subspaces making six vectors in all. . True or false? a  If the columns of A are linearly independent then Ax  b has exactly one solution for every b . b  A  by  matrix never has linearly independent columns . If A is a  by  matrix of rank  how many independent vectors satisfy Ax  ? How many independent vectors satisfy A  y  ? . Find a basis for each of these subspaces of  by  matrices a  All diagonal matrices. b  All symmetric matrices  A   A . c  All skewsymmetric matrices  A    A . Problems  are about spaces in which the vectors are functions.   . a  Find all functions that satisfy     . b  Choose a particular function that satisfies     . c  Find all functions that satisfy     . . The cosine space F  contains all combinations y  x   A cos x  B cos  x  C cos  x . Find a basis for the subspace that has y     . . Find a basis for the space of functions that satisfy a dy d x   y   . b dy d x  y x   . . Suppose y   x   y   x   y   x  are three different functions of x . The vector space they span could have dimension   or .  Give an example of y   y   y  to show each possibility. . Find a basis for the space of polynomials p  x  of degree  .  Find a basis for the subspace with p     . . Write the  by  identity matrix as a combination of the other five permutation matri ces! Then show that those five matrices are linearly independent. Assume a combi nation gives zero and check entries to prove each term is zero.  The five permuta tions are a basis for the subspace of  by  matrices with row and column sums all equal. . Review  Which of the following are bases for R  ? a        and         . b                                   . c                         . d                         . . Review  Suppose A is  by  with rank . Show that Ax  b has no solution when the  by  matrix  A  b  is invertible. Show that Ax  b is solvable when  A  b  is singular. . The Four Fundamental Subspaces The previous section dealt with definitions rather than constructions.  We know what a basis is but not how to find one. Now starting from an explicit description of a subspace we would like to compute an explicit basis. Subspaces can be described in two ways. First we may be given a set of vectors that span the space.   Example   The columns span the column space.  Second we may be  told which conditions the vectors in the space must satisfy.   Example   The nullspace consists of all vectors that satisfy Ax  . The first description may include useless vectors dependent columns.  The second description may include repeated conditions dependent rows.  We cant write a basis by inspection and a systematic procedure is necessary. The reader can guess what that procedure will be.  When elimination on A produces an  echelon  matrix U or  a  reduced R   we  will  find  a  basis  for  each  of  the  subspaces associated with A . Then we have to look at the extreme case of full rank  When the rank is as large as possible  r  n or r  m or r  m  n  the matrix has a leftinverse B or a rightinverse C or a twosided A   . T o organize the whole discussion we take each of the four subspaces in turn.  Two of them are familiar and two are new. .  The column space of A is denoted by   A  . Its dimension is the rank r . .  The nullspace of A is denoted by   A  . Its dimension is n  r . .  The row space of A is the column space of A  . It is   A    and it is spanned by the rows of A . Its dimension is also r . .  The left nullspace of A is the nullspace of A  .  It contains all vectors y such that A  y   and it is written   A   . Its dimension is . The point about the last two subspaces is that they come from A  .  If A is an m by n matrix you can see which host spaces contain the four subspaces by looking at the number of components The nullspace   A  and row space   A   are subspaces of R  . The left nullspace   A   and column space   A  are subspaces of R  . The rows have n components and the columns have m . For a simple matrix like A  U  R                   the column space is the line through     . The row space is the line through         . It is in R  . The nullspace is a plane in R  and the left nullspace is a line in R     A  contains          and             A   contains     .   Note that all vectors are column vectors.  Even the rows are transposed  and the row space of A is the column space of A   Our problem will be to connect the four spaces for U after elimination to the four spaces for A  Basic example U                                      came from A                              . For novelty we take the four subspaces in a more interesting order. .  The row space of A For an echelon matrix like U  the row space is clear. It contains all combinations of the rows as every row space doesbut here the third row contributes nothing. The first two rows are a basis for the row space. A similar rule applies to every echelon matrix U or R  with r pivots and r nonzero rows The nonzero rows are a basis and the row space has dimension r . That makes it easy to deal with the original matrix A . M The row space of A has the same dimension r as the row space of U  and it has the same bases because the row spaces of A and U and R  are the same . The reason is that each elementary operation leaves the row space unchanged. The rows in U are combinations of the original rows in A . Therefore the row space of U contains nothing new.  At the same time because every step can be reversed nothing is lost the rows of A can be recovered from U .  It is true that A and U have different rows but the combinations of the rows are identical same space ! Note that we did not start with the m rows of A  which span the row space and discard m  r of them to end up with a basis.  According to L we could have done so.  But it might be hard to decide which rows to keep and which to discard so it was easier just to take the nonzero rows of U . .   The  nullspace  of A Elimination  simplifies  a  system  of  linear  equations  without changing the solutions.  The system Ax   is reduced to U x   and this process is reversible. The nullspace of A is the same as the nullspace of U and R .  Only r of the equations Ax   are independent. Choosing the n  r special solutions to Ax   provides a definite basis for the nullspace N The nullspace   A  has dimension n  r .  The special solutions are a basiseach free variable is given the value  while the other free variables are .  Then Ax   or U x   or Rx   gives the pivot variables by back substitution. This is exactly the way we have been solving U x  .  The basic example above has pivots in columns  and . Therefore its free variables are the second and fourth v and y .  The basis for the nullspace is Special solutions v   y   x                   v   y   x                  . Any combination c  x   c  x  has c  as its v component and c  as its y component. The only way to have c  x   c  x    is to have c   c    so these vectors are independent. They also span the nullspace the complete solution is vx   yx  . Thus the n  r     vectors are a basis. The nullspace is also called the kernel of A  and its dimension n  r is the nullity . .   The column space of A The column space is sometimes called the range .  This is consistent with the usual idea of the range as the set of all possible values f  x   x is in the domain and f  x  is in the range. In our case the function is f  x   Ax . Its domain consists of all x in R   its range is all possible vectors Ax  which is the column space. In an earlier edition of this book we called it R  A  . Our problem is to find bases for the column spaces of U and A . Those spaces are different just look at the matrices! but their dimensions are the same. The  first  and  third  columns  of U are  a  basis  for  its  column  space.   They  are  the columns with pivots .  Every other column is a combination of those two.  Furthermore the same is true of the original A even though its columns are different. The pivot columns  of A are  a  basis  for  its  column  space .   The  second  column  is  three  times the first just as in U .  The fourth column equals column   column .  The same nullspace is telling us those dependencies. The reason is this Ax   exactly when U x  . The two systems are equivalent and have the same solutions.  The fourth column of U was also column   column . Every linear dependence Ax   among the columns of A is matched by a dependence U x   among the columns of U  with exactly the same coefficients. If a set of columns of A is independent then so are the corresponding columns of U  and vice versa . To find a basis for the column space   A   we use what is already done for U .  The r columns containing pivots are a basis for the column space of U .  We will pick those same r columns in A  O The dimension of the column space   A  equals the rank r  which also equals the dimension of the row space The number of independent columns equals the number of independent rows . A basis for   A  is formed by the r columns of A that correspond in U  to the columns containing pivots. The row space and the column space have the same dimension r !   This is one of the most important theorems in linear algebra.  It is often abbreviated as  row rank  column rank .  It expresses a result that  for a random  by  matrix  is not at all   obvious.  It also says something about square matrices If the rows of a square matrix are linearly independent then so are the columns and vice versa. Again that does not seem selfevident at least not to the author. To see once more that both the row and column spaces of U have dimension r  con sider  a  typical  situation  with  rank r  .   The  echelon  matrix U certainly  has  three independent rows U       d            d                 d                        . We claim that U also has three independent columns and no more The columns have only three nonzero components. If we can show that the pivot columnsthe first fourth and sixthare linearly independent they must be a basis for the column space of U  not A !. Suppose a combination of these pivot columns produced zero c       d           c        d          c         d                       . Working upward in the usual way c  must be zero because the pivot d     then c  must be zero because d     and finally c   . This establishes independence and completes the proof. Since Ax   if and only if U x   the first fourth and sixth columns of A  whatever the original matrix A was which we do not even know in this exampleare a basis for   A  . The row space and column space both became clear after elimination on A .   Now comes the fourth fundamental subspace which has been keeping quietly out of sight. Since the first three spaces were   A     A    and   A     the fourth space must be   A    It is the nullspace of the transpose or the left nullspace of A . A  y   means y  A   and the vector appears on the lefthand side of A . .   The left nullspace of A   the nullspace of A   If A is an m by n matrix  then A  is n by m . Its nullspace is a subspace of R   the vector y has m components. Written as y  A   those components multiply the rows of A to produce the zero row y  A   y   y   A        . The dimension of this nullspace   A   is easy to find For any matrix the number of pivot variables plus the number of free variables must match the total number of columns . For A  that was r  n  r   n . In other words rank plus nullity equals n  dimension of   A  dimension of   A   number of columns .  This law applies equally to A   which has m columns. A  is just as good a matrix as A . But the dimension of its column space is also r  so r  dimension    A     m .  P The left nullspace   A   has dimension m  r . The m  r solutions to y  A   are hiding somewhere in elimination. The rows of A combine to produce the m  r zero rows of U . Start from PA  LU  or L   PA  U . The last m  r rows of the invertible matrix L   P must be a basis of y s in the left nullspace because they multiply A to give the zero rows in U . In our  by  example the zero row was row   row   row .  Therefore the components of y are    . This is the same combination as in b    b    b  on the righthand side leading to    as the final equation.  That vector y is a basis for the left nullspace which has dimension m  r      .  It is the last row of L   P  and produces the zero row in U and we can often see it without computing L   . When desperate it is always possible just to solve A  y  . I realize that so far in this book we have given no reason to care about   A   .  It is correct but not convincing if I write in italics that the left nullspace is also important . The next section does better by finding a physical meaning for y from Kirchhoffs Current Law. Now we know the dimensions of the four spaces. We can summarize them in a table and it even seems fair to advertise them as the Fundamental Theorem of Linear Algebra Part I .   A   column space of A  dimension r . .   A   nullspace of A  dimension n  r . .   A    row space of A  dimension r . .   A    left nullspace of A  dimension m  r . Example . A            has m  n   and rank r  . .  The column space contains all multiples of     . The second column is in the same direction and contributes nothing new. .  The nullspace contains all multiples of      . This vector satisfies Ax  . .  The row space contains all multiples of     .  I write it as a column vector since strictly speaking it is in the column space of A  . .  The left nullspace contains all multiples of y       .  The rows of A with coeffi cients   and  add to zero so A  y  .   In this example all four subspaces are lines . That is an accident coming from r   and n  r   and m  r  . Figure . shows that two pairs of lines are perpendicular. That is no accident!      If you change the last entry of A from  to  all the dimensions are different.  The column space and row space have dimension r  .  The nullspace and left nullspace contain only the vectors x   and y  . The matrix is invertible . Existence of Inverses We know that if A has a leftinverse  BA  I  and a rightinverse  AC  I  then the two inverses are equal B  B  AC  BA  C  C . Now from the rank of a matrix it is easy to decide which matrices actually have these inverses. Roughly speaking an inverse exists only when the rank is as large as possible . The rank always satisfies r  m and also r  n . An m by n matrix cannot have more than m independent rows or n independent columns. There is not space for more than m pivots or more than n .  We want to prove that when r  m there is a rightinverse and Ax  b always has a solution.  When r  n there is a leftinverse and the solution  if it exists  is unique. Only a square matrix can have both r  m and r  n  and therefore only a square matrix can achieve both existence and uniqueness. Only a square matrix has a twosided inverse. Q EXISTENCE Full row rank r  m . Ax  b has at least one solution x for every b if and only if the columns span R  .  Then A has a rightinverse C such that AC  I   m by m . This is possible only if m  n . UNIQUENESS Full column rank r  n . Ax  b has at most one solution x for every b if and only if the columns are linearly independent. Then A has an n by m leftinverse B such that BA  I  . This is possible only if m  n .  In the existence case one possible solution is x  Cb  since then Ax  ACb  b . But there will be other solutions if there are other rightinverses.  The number of solutions when the columns span R  is  or  . In the uniqueness case if there is a solution to Ax  b  it has to be x  BAx  Bb . But there may be no solution. The number of solutions is  or . There are simple formulas for the best left and right inverses if they exist Onesided inverses B   A  A    A  and C  A   AA     . Certainly BA  I and AC  I .  What is not so certain is that A  A and AA  are actually invertible. We show in Chapter  that A  A does have an inverse if the rank is n  and AA  has an inverse when the rank is m . Thus the formulas make sense exactly when the rank is as large as possible and the onesided inverses are found. Example . Consider a simple  by  matrix of rank  A                  . Since r  m   the theorem guarantees a rightinverse C  AC                           c  c              . There are many rightinverses because the last row of C is completely arbitrary. This is a case of existence but not uniqueness. The matrix A has no leftinverse because the last column of BA is certain to be zero.  The specific rightinverse C  A   AA     chooses c  and c  to be zero Best rightinverse A   AA                                                C . This is the pseudoinverse a way of choosing the best C in Section ..  The transpose of A yields an example with infinitely many left inverses BA       b     b                              . Now it is the last column of B that is completely arbitrary. The best leftinverse also the pseudoinverse has b   b   .  This is a uniqueness case when the rank is r  n . There are no free variables since n  r  . If there is a solution it will be the only one. You can see when this example has one solution or no solution                    x  x       b  b  b     is solvable exactly when b    .   A rectangular matrix cannot have both existence and uniqueness. If m is different from n  we cannot have r  m and r  n . A square matrix is the opposite.  If m  n  we cannot have one property without the other.  A square matrix has a leftinverse if and only if it has a rightinverse.  There is only one inverse namely B  C  A   . Existence implies uniqueness and uniqueness implies existence when the matrix is square . The condition for invertibility is full rank  r  m  n . Each of these conditions is a necessary and sufficient test .  The columns span R   so Ax  b has at least one solution for every b . .  The columns are independent so Ax   has only the solution x  . This list can be made much longer especially if we look ahead to later chapters. Every condition is equivalent to every other and ensures that A is invertible. .  The rows of A span R  . .  The rows are linearly independent. .  Elimination can be completed PA  LDU  with all n pivots. .  The determinant of A is not zero. .  Zero is not an eigenvalue of A . . A  A is positive definite. Here is a typical application to  polynomials P  t  of degree n  .   The only such polynomial that vanishes at t  ... t  is P  t   . No other polynomial of degree n   can have n roots.  This is uniqueness and it implies existence  Given any values b  ... b   there exists a polynomial of degree n   interpolating these values P  t    b  .   The point is that we are dealing with a square matrix the number n of coefficients in P  t   x   x  t    x  t    matches the number of equations Interpolation P  t    b        t  t    t      t  t    t     . . . . . . . . . . . . . . .  t  t    t               x  x  . . . x             b  b  . . . b       . That Vandermonde matrix is n by n and full rank. Ax  b always has a solutiona polynomial can be passed through any b  at distinct points t  . Later we shall actually find the determinant of A  it is not zero. Matrices of Rank  Finally comes the easiest case when the rank is as small as possible except for the zero matrix with rank  One basic theme of mathematics is given something complicated  to show how it can be broken into simple pieces.  For linear algebra the simple pieces are matrices of rank   Rank  A                           has r   . Every row is a multiple of the first row so the row space is onedimensional. In fact we can write the whole matrix as the product of a column vector and a row vector  A   column  row                                                    . The product of a  by  matrix and a  by  matrix is a  by  matrix. This product has rank .  At the same time the columns are all multiples of the same column vector the column space shares the dimension r   and reduces to a line. Ev ery matrix of rank  has the simple form A  uv   column times row. The rows are all multiples of the same vector v   and the columns are all multiples of u . The row space and column space are linesthe easiest case. Pr oblem Set . . True or false If m  n  then the row space of A equals the column space.  If m  n  then the nullspace has a larger dimension than . . Find the dimension and construct a basis for the four subspaces associated with each of the matrices A                        and U                        . . Find the dimension and a basis for the four fundamental subspaces for A                                      and U                                      .   . Describe the four subspaces in threedimensional space associated with A                             . . If the product AB is the zero matrix AB   show that the column space of B is contained in the nullspace of A .  Also the row space of A is in the left nullspace of B  since each row of A multiplies B to give a zero row. . Suppose A is an m by n matrix of rank r .  Under what conditions on those numbers does a A have a twosided inverse AA    A   A  I ? b Ax  b have infinitely many solutions for every b ? . Why is there no matrix whose row space and nullspace both contain        ? . Suppose the only solution to Ax    m equations in n unknowns is x  . What is the rank and why? The columns of A are linearly . . Find a  by  matrix whose nullspace consists of all vectors in R  such that x    x    x   . Find a  by  matrix with that same nullspace. . If Ax  b always has at least one solution show that the only solution to A  y   is y  . Hint  What is the rank? . If Ax   has a nonzero solution show that A  y  f fails to be solvable for some righthand sides f . Construct an example of A and f . . Find the rank of A and write the matrix as A  uv   A                                      and A          . . If a  b  c are given with a    choose d so that A   a   b c   d   uv  has rank . What are the pivots? . Find a leftinverse andor a rightinverse when they exist for A                  and M                    and T   a   b  a  .  . If the columns of A are linearly independent  A is m by n  then the rank is  the nullspace is  the row space is  and there exists a in verse. .  A paradox  Suppose A has a rightinverse B .  Then AB  I leads to A  AB  A  or B  A  A    A  . But that satisfies BA  I  it is a left inverse. Which step is not justified? . Find a matrix A that has V as its row space and a matrix B that has V as its nullspace if V is the subspace spanned by                              . . Find a basis for each of the four subspaces of A                                                                                                                        . . If A has the same four fundamental subspaces as B  does A  cB ? . a  If a  by  matrix has rank  what are the dimensions of the four subspaces? What is the sum of all four dimensions? b  If a  by  matrix has rank  what are its column space and left nullspace? . Construct a matrix with the required property or explain why you cant. a  Column space contains             row space contains          . b  Column space has basis       nullspace has basis      . c  Dimension of nullspace    dimension of left nullspace. d  Left nullspace contains      row space contains     . e  Row space  column space nullspace   left nullspace. . Without elimination find dimensions and bases for the four subspaces for A                                      and B                    . . Suppose the  by  matrix A is invertible.  Write bases for the four subspaces for A  and also for the  by  matrix B   A  A  . . What are the dimensions of the four subspaces for A  B  and C  if I is the  by  identity matrix and  is the  by  zero matrix? A   I   and B   I I      and C     .   . Which subspaces are the same for these matrices of different sizes? a  A  and  A A  . b  A A  and  A   A A   A  . Prove that all three matrices have the same rank r . . If the entries of a  by  matrix are chosen randomly between  and  what are the most likely dimensions of the four subspaces? What if the matrix is  by ? . Important A is an m by n matrix of rank r . Suppose there are righthand sides b for which Ax  b has no solution . a  What inequalities   or   must be true between m  n  and r ? b  How do you know that A  y   has a nonzero solution? . Construct  a  matrix  with        and        as  a  basis  for  its  row  space  and  its column space. Why cant this be a basis for the row space and nullspace? . Without computing A  find bases for the four fundamental subspaces A                                                                 . . If you exchange the first two rows of a matrix A  which of the four subspaces stay the same?  If y           is in the left nullspace of A  write down a vector in the left nullspace of the new matrix. . Explain why v          cannot be a row of A and also be in the nullspace. . Describe the four subspaces of R  associated with A                             and I  A                             . . Left nullspace Add the extra column b and reduce A to echelon form  A   b             b         b         b            b       b    b     b    b   b     . A combination of the rows of A has produced the zero row. What combination is it? Look at b    b   b  on the righthand side. Which vectors are in the nullspace of A  and which are in the nullspace of A ?  . Following the method of Problem   reduce A to echelon form and look at zero rows. The b column tells which combinations you have taken of the rows a        b      b      b     . b          b      b      b      b       . From the b column after elimination read off m  r basis vectors in the left nullspace of A combinations of rows that give zero. . Suppose A is the sum of two matrices of rank one A  uv   wz  . a  Which vectors span the column space of A ? b  Which vectors span the row space of A ? c  The rank is less than  if or if . d  Compute A and its rank if u  z         and v  w         . . Without multiplying matrices find bases for the row and column spaces of A  A                                    . How do you know from these shapes that A is not invertible? . True or false with a reason or a counterexample? a A and A  have the same number of pivots. b A and A  have the same left nullspace. c  If the row space equals the column space then A   A . d  If A    A then the row space of A equals the column space. . If AB   the columns of B are in the nullspace of A .  If those vectors are in R   prove that rank  A  rank  B   n . . Can tictactoe be completed  ones and  zeros in A  so that rank  A    but neither side passed up a winning move? . Construct any  by  matrix of rank .  Copy Figure . and put one vector in each subspace two in the nullspace. Which vectors are orthogonal? . Redraw Figure . for a  by  matrix of rank r  .  Which subspace is  zero vector only? The nullspace part of any vector x in R  is x   .   .    Graphs and Networks I am not entirely happy with the  by  matrix in the previous section. From a theoretical point of view it was very satisfactory  the four subspaces were computable and their dimensions r  n  r  r  m  r were nonzero.  But the example was not produced by a genuine application. It did not show how fundamental those subspaces really are. This section introduces a class of rectangular matrices with two advantages.  They are simple and they are important.  They are incidence matrices of graphs  and every entry is    or .  What is remarkable is that the same is true of L and U and basis vectors for all four subspaces.  Those subspaces play a central role in network theory. We emphasize that the word graph does not refer to the graph of a function like a parabola for y  x  . There is a second meaning completely different which is closer to computer science than to calculusand it is easy to explain. This section is optional  but it gives a chance to see rectangular matrices in actionand how the square symmetric matrix A  A turns up in the end. A graph consists of a set of vertices or nodes  and a set of edges that connect them. The graph in Figure . has  nodes and  edges.  It does not have an edge between nodes  and  and edges from a node to itself are forbidden.  This graph is directed  because of the arrow in each edge. The edgenode incidence matrix is  by  with a row for every edge. If the edge goes from node j to node k  then that row has   in column j and   in column k .  The incidence matrix A is shown next to the graph and you could recover the graph if you only had A . Row  shows the edge from node  to node . Row  comes from the fifth edge from node  to node .      Notice the columns of A .  Column  gives information about node it tells which edges enter and leave. Edges  and  go in edge  goes out with the minus sign. A is sometimes called the connectivity matrix or the topology matrix. When the graph has m edges and n nodes A is m by n and normally m  n .  Its transpose is the nodeedge incidence matrix. Each of the four fundamental subspaces has a meaning in terms of the graph. We can do linear algebra or write about voltages and currents. We do both!  Nullspace of A  Is there a combination of the columns that gives Ax  ? Normally the answer comes from elimination but here it comes at a glance. The columns add up to the zero column .  The nullspace contains x            since Ax  .  The equation Ax  b does not have a unique solution if it has a solution at all. Any constant vector x   c  c  c  c  can be added to any particular solution of Ax  b . The complete solution has this arbitrary constant c like the  C when we integrate in calculus. This has a meaning if we think of x   x   x   x  as the potentials the voltages at the nodes . The five components of Ax give the differences in potential across the five edges. The difference across edge  is x   x   from the   in the first row. The equation Ax  b asks Given the differences b  ... b   find the actual potentials x  ... x  .  But that is impossible to do!  We can raise or lower all the potentials by the same constant c  and the differences will not changeconfirming that x   c  c  c  c  is in the nullspace of A . Those are the only vectors in the nullspace since Ax   means equal potentials across every edge. The nullspace of this incidence matrix is onedimensional. The rank is    . Column Space For which differences b  ... b  can we solve Ax  b ?  To find a direct test look back at the matrix.  Row  plus row  equals row .  On the righthand side we need b   b   b   or no solution is possible. Similarly row  plus row  is row .  The righthand side must satisfy b   b   b   for elimination to arrive at   .  To repeat if b is in the column space then b   b   b    and b   b   b    .  Continuing the search we also find that rows    equal rows   . But this is nothing new  subtracting the equations in  already produces b   b   b   b  .   There are two conditions on the five components because the column space has dimension   . Those conditions would come from elimination but here they have a meaning on the graph. Loops Kirchhoffs Voltage Law says that potential differences around a loop must add to zero Around the upper loop in Figure . the differences satisfy  x   x   x   x     x   x   . Those differences are b   b   b  . To circle the lower loop and arrive back at the same potential we need b   b   b  . R The test for b to be in the column space is Kirchhoffs Voltage Law  The sum of potential differences around a loop must be zero. Left Nullspace To solve A  y   we find its meaning on the graph. The vector y has five components one for each edge.  These numbers represent currents flowing along the five edges.  Since A  is  by  the equations A  y   give four conditions on those   five currents. They are conditions of conservation at each node Flow in equals flow out at every node  A  y    y   y    y   y   y    y   y   y    y   y    Total current to node  is zero to node  to node  to node  The beauty of network theory is that both A and A  have important roles. Solving A  y   means finding a set of currents that do not pile up at any node. The traffic keeps circulating and the simplest solutions are currents around small loops . Our graph has two loops and we send  amp of current around each loop Loop vectors y                  and y                  . Each loop produces a vector y in the left nullspace. The component   or   indicates whether the current goes with or against the arrow.  The combinations of y  and y  fill the left nullspace so y  and y  are a basis the dimension had to be m  r      . In fact y   y                gives the big loop around the outside of the graph. The column space and left nullspace are closely related.  The left nullspace contains y               and the vectors in the column space satisfy b   b   b   .  Then y  b   Vectors in the column space and left nullspace are perpendicular! That is soon to become Part Two of the Fundamental Theorem of Linear Algebra. Row  Space The  row  space  of A contains  vectors  in R    but  not  all  vectors.   Its dimension is the rank r  .  Elimination will find three independent rows and we can also look to the graph. The first three rows are dependent row   row   row  and those edges form a loop. Rows      are independent because edges      contain no loops . Rows    are a basis for the row space. In each row the entries add to zero . Every combination  f   f   f   f   in the row space will have that same property f in row space f   f   f   f    x in nullspace x  c           Again this illustrates the Fundamental Theorem The row space is perpendicular to the nullspace. If  f  is in the row space and x is in the nullspace then  f  x  . For A   the basic law of network theory is Kirchhoffs Current Law .  The total flow into every node is zero . The numbers f   f   f   f  are current sources into the nodes. The source f  must balance  y   y   which is the flow leaving node  along edges  and . That is the first equation in A  y  f .  Similarly at the other three nodesconservation of charge requires flow in  flow out . The beautiful thing is that A  is exactly the right matrix for the Current Law . S The equations A  y  f at the nodes express Kirchhoffs Current Law   The net current into every node is zero. Flow in  Flow out. This law can only be satisfied if the total current from outside is f   f   f   f   . With f   the law A  y   is satisfied by a current that goes around a loop . Spanning Trees and Independent Rows Every component of y  and y  in the left nullspace is  or   or  from loop flows. The same is true of x           in the nullspace and all the entries in PA  LDU ! The key point is that every elimination step has a meaning for the graph. You can see it in the first step for our matrix A  subtract row  from row .  This replaces edge  by a new edge  minus  That elimination step destroys an edge and                  creates a new edge.  Here the new edge    is just the old edge  in the opposite direction.  The next elimination step will produce zeros in row  of the matrix.  This shows that rows    are dependent. Rows are dependent if the corresponding edges contain a loop . At the end of elimination we have a full set of r independent rows. Those r edges form a treea graph with no loops . Our graph has r   and edges    form one possible tree.  The full name is spanning tree because the tree spans all nodes of the graph. A spanning tree has n   edges if the graph is connected and including one more edge will produce a loop. In the language of linear algebra n   is the rank of the incidence matrix A . The row space has dimension n  . The spanning tree from elimination gives a basis for that row spaceeach edge in the tree corresponds to a row in the basis. The fundamental theorem of linear algebra connects the dimensions of the subspaces Nullspace dimension  contains x    ...   . Column space dimension r  n   any n   columns are independent. Row space dimension r  n   independent rows from any spanning tree. Left nullspace dimension m  r  m  n   contains y s from the loops. Those four lines give Eulers formula  which in some way is the first theorem in topol ogy. It counts zerodimensional nodes minus onedimensional edges plus twodimensional   loops. Now it has a linear algebra proof for any connected graph   of nodes     of edges   of loops    n    m  m  n      .  For a single loop of  nodes and  edges the Euler number is     . If those  nodes are each connected to an eleventh node in the center then      is still . Every vector f in the row space has x  f  f     f   the currents from outside add to zero. Every vector b in the column space has y  b  the potential differences add to zero around all loops. In a moment we link x to y by a third law  Ohms law for each resistor .  First we stay with the matrix A for an application that seems frivolous but is not. The Ranking of Football Teams At the end of the season the polls rank college football teams. The ranking is mostly an average of opinions and it sometimes becomes vague after the top dozen colleges.  We want to rank all teams on a more mathematical basis. The first step is to recognize the graph.  If team j played team k  there is an edge between them.  The teams are the nodes  and the games are the edges .  There are a few hundred nodes and a few thousand edgeswhich will be given a direction by an arrow from  the  visiting  team  to  the  home  team.   Figure  .  shows  part  of  the  Ivy  League and  some  serious  teams  and  also  a  college  that  is  not  famous  for  big  time  football. Fortunately  for  that  college  from  which  I  am  writing  these  words  the  graph  is  not connected. Mathematically speaking we cannot prove that MIT is not number  unless it happens to play a game against somebody.                        If football were perfectly consistent we could assign a potential x  to every team. Then if visiting team v played home team h  the one with higher potential would win. In the ideal case the difference b in the score would exactly equal the difference x   x  in their potentials.  They wouldnt even have to play the game!  There would be complete agreement that the team with highest potential is the best. This method has two difficulties at least. We are trying to find a number x for every team and we want x   x   b   for every game.  That means a few thousand equations  and only a few hundred unknowns.  The equations x   x   b  go into a linear system Ax  b  in which A is an incidence matrix . Every game has a row with   in column h and   in column v to indicate which teams are in that game. First difficulty If b is not in the column space there is no solution.  The scores must fit perfectly or exact potentials cannot be found.  Second difficulty  If A has nonzero vectors in its nullspace the potentials x are not well determined. In the first case x does not exist in the second case x is not unique. Probably both difficulties are present. The nullspace always contains the vector of s since A looks only at the differences x   x  . To determine the potentials we can arbitrarily assign zero potential to Harvard. I am speaking mathematically not meanly.  But if the graph is not connected every separate piece of the graph contributes a vector to the nullspace. There is even the vector with x    and all other x   .  We have to ground not only Harvard but one team in each piece. There is nothing unfair in assigning zero potential if all other potentials are below zero then the grounded team ranks first.  The dimension of the nullspace is the number of pieces of the graphand there will be no way to rank one piece against another since they play no games. The column space looks harder to describe.  Which scores fit perfectly with a set of potentials?  Certainly Ax  b is unsolvable if Harvard beats Yale Yale beats Princeton and Princeton beats Harvard. More than that the score differences in that loop of games have to add to zero  Kirchhoffs law for score differences b   b   b    . This is also a law of linear algebra. Ax  b can be solved when b satisfies the same linear dependencies as the rows of A . Then elimination leads to   . In reality b is almost certainly not in the column space.  Football scores are not that consistent. To obtain a ranking we can use least squares  Make Ax as close as possible to b . That is in Chapter  and we mention only one adjustment. The winner gets a bonus of  or even  points on top of the score difference.  Otherwise winning by  is too close to losing by . This brings the computed rankings very close to the polls and Dr. Leake Notre Dame gave a full analysis in Management Science in Sports . After writing that subsection I found the following in the New York Times  In its final rankings for  the computer placed Miami  in the sev enth spot above Tennessee .   A few days after publication  packages containing oranges and angry letters from disgruntled Tennessee fans began arriving at the Times sports department. The irritation stems from the fact that Tennessee thumped Miami  in the Sugar Bowl.  Final AP and UPI polls ranked Tennessee fourth with Miami significantly lower. Yesterday morning nine cartons of oranges arrived at the loading dock.  They were sent to Bellevue Hospital with a warning that the quality and contents of the oranges were uncertain.   So much for that application of linear algebra. Networks and Discrete Applied Mathematics A graph becomes a network when numbers c  ... c  are assigned to the edges.  The number c  can be the length of edge i  or its capacity  or its stiffness if it contains a spring or its conductance if it contains a resistor.  Those numbers go into a diagonal matrix C  which is m by m . C reflects material properties in contrast to the incidence matrix A which gives information about the connections. Our description will be in electrical terms.  On edge i  the conductance is c  and the resistance is   c  . Ohms Law says that the current y  through the resistor is proportional to the voltage drop e   Ohms Law y   c  e  current  conductancevoltage drop . This is also written E  IR  voltage drop equals current times resistance.  As a vector equation on all edges at once Ohms Law is y  Ce . We need Kirchhoffs Voltage Law and Current Law to complete the framework KVL  The voltage drops around each loop add to zero. KCL  The currents y  and f   into each node add to zero. The voltage law allows us to assign potentials x  ... x  to the nodes.   Then the dif ferences around a loop give a sum like  x   x     x   x     x   x     in which everything cancels.  The current law asks us to add the currents into each node by the multiplication A  y . If there are no external sources of current Kirchhoff s Current Law is A  y  . The  other  equation  is  Ohms  Law  but  we  need  to  find  the  voltage  drop e across the  resistor.   The  multiplication Ax gave  the  potential  difference  between  the  nodes. Reversing the signs  Ax gives the drop in potential.  Part of that drop may be due to a battery in the edge of strength b  . The rest of the drop is e  b  Ax across the resistor Ohms Law y  C  b  Ax  or C   y  Ax  b .  The fundamental equations of equilibrium combine Ohm and Kirchhoff into a cen tral problem of applied mathematics. These equations appear everywhere Equilibrium equations C   y  Ax  b A  y  f .  That is a linear symmetric system from which e has disappeared. The unknowns are the currents y and the potentials x . You see the symmetric block matrix Block form  C   A A    y x    b f  .   For block elimination the pivot is C    the multiplier is A  C  and subtraction knocks out A  below the pivot. The result is  C   A   A  CA  y x    b f  A  Cb  The equation for x alone is in the bottom row with the symmetric matrix A  CA  Fundamental equation A  CAx  A  Cb  f .  Then backsubstitution in the first equation produces y. Nothing mysterioussubstitute y  C  b  Ax  into A  y  f to reach . Important  Remark One  potential  must  be  fixed  in  advance x   .   The n th  node is grounded  and the n th column of the original incidence matrix is removed.  The re sulting matrix is what we now mean by A  its n   columns are independent. The square matrix A  CA  which is the key to solving equation  for x  is an invertible matrix of order n    A             C          A               A  CA               Example . Suppose a battery b  and a current source f  and five resistors connect four nodes.  Node  is grounded and the potential x    is fixed.  The first thing is the current law A  y  f at nodes     y   y   y    y   y   f  y   y   y    and A                             . No equation is written for node  where the current law is y   y   f   . This follows from adding the other three equations. The other equation is C   y  Ax  b .  The potentials x are connected to the currents y by Ohms Law. The diagonal matrix C contains the five conductances c     R  . The   righthand side accounts for the battery of strength b  in edge .  The block form has C   y  Ax  b above A  y  f   C   A A    y x                 R      R      R      R      R                                                      y  y  y  y  y  x  x  x                                 b     f                 The system is  by  with five currents and three potentials. Elimination of y s reduces to the  by  system A  CAx  A  Cb  f .  The matrix A  CA contains the reciprocals c     R  because in elimination you divide by the pivots.  We also show the fourth row and column from the grounded node outside the  by  matrix A  CA     c   c   c   c   c   c  c   c   c   c   c  c   c   c      c    c   c  node   node   c  node  c   c  node  The first entry is      or c   c   c  when C is included because edges    touch node . The next diagonal entry is    or c   c   from the edges touching node .  Off the diagonal the c s appear with minus signs. The edges to the grounded node  belong in the fourth row and column which are deleted when column  is removed from A making A  CA invertible. The  by  matrix would have all rows and columns adding to zero and          would be in its nullspace. Notice that A  CA is symmetric.  It has positive pivots and it comes from the basic framework of applied mathematics illustrated in Figure ..                                         In mechanics x and y become displacements and stresses.  In fluids the unknowns are pressure and flow rate. In statistics e is the error and x is the best leastsquares fit to  the data. These matrix equations and the corresponding differential equations are in our textbook Introduction to Applied Mathematics  and the new Applied Mathematics and Scientific Computing . See www.wellesleycambridge.com . We end this chapter at that high pointthe formulation of a fundamental problem in applied mathematics. Often that requires more insight than the solution of the problem. We solved linear equations in Chapter  as the first step in linear algebra. To set up the equations has required the deeper insight of Chapter . The contribution of mathematics and of people is not computation but intelligence. Pr oblem Set . . For the node triangular graph in the figure following write the  by  incidence matrix A . Find a solution to Ax   and describe all other vectors in the nullspace of A .  Find a solution to A  y   and describe all other vectors in the left nullspace of A .                                  . F or the same  by  matrix show directly from the columns that every vector b in the column space will satisfy b   b   b   . Derive the same thing from the three rowsthe equations in the system Ax  b .   What does that mean about potential differences around a loop? . Show directly from the rows that every vector f in the row space will satisfy f   f   f   . Derive the same thing from the three equations A  y  f . What does that mean when the f s are currents into the nodes? . Compute the  by  matrix A  A  and show that it is symmetric but singularwhat vectors are in its nullspace?  Removing the last column of A and last row of A   leaves the  by  matrix in the upper left corner show that it is not singular. . Put the diagonal matrix C with entries c   c   c  in the middle and compute A  CA . Show again that the  by  matrix in the upper left corner is invertible. . Write the  by  incidence matrix A for the second graph in the figure.  The vector          is in the nullspace of A  but now there will be m  n     independent vectors that satisfy A  y  .  Find three vectors y and connect them to the loops in the graph .   . If that second graph represents six games between four teams  and the score dif ferences are b  ... b   when is it possible to assign potentials x  ... x  so that the potential differences agree with the b s?  You are finding from Kirchhoff or from elimination the conditions that make Ax  b solvable. . Write down the dimensions of the four fundamental subspaces for this  by  inci dence matrix and a basis for each subspace. . Compute A  A and A  CA  where the  by  diagonal matrix C has entries c  ... c  . How can you tell from the graph where the c s will appear on the main diagonal of A  CA ? . Draw a graph with numbered and directed edges and numbered nodes whose inci dence matrix is A                                    . Is this graph a tree?  Are the rows of A independent?  Show that removing the last edge produces a spanning tree. Then the remaining rows are a basis for ? . W ith the last column removed from the preceding A  and with the numbers .    on the diagonal of C  write out the  by  system C   y  Ax   A  y  f . Eliminating y   y   y   y  leaves three equations A  CAx   f for x   x   x  .  Solve the equations when f         .  With those currents entering nodes    of the network what are the potentials at the nodes and currents on the edges? . If A is a  by  incidence matrix from a connected graph what is its rank?  How many free variables are there in the solution to Ax  b ?  How many free variables are there in the solution to A  y  f ?  How many edges must be removed to leave a spanning tree? . In the graph above with  nodes and  edges find all  spanning trees. . If MIT beats Harvard  Yale ties Harvard and Princeton beats Yale  what score differences in the other  games HP MITP MITY will allow potential dif ferences that agree with the score differences? If the score differences are known for the games in a spanning tree they are known for all games. . In our method for football rankings should the strength of the opposition be consid ered  or is that already built in?  . If there is an edge between every pair of nodes a complete graph how many edges are there? The graph has n nodes and edges from a node to itself are not allowed. . For both graphs drawn below verify Eulers formula   of nodes   of edges   of loops  . . Multiply matrices to find A  A  and guess how its entries come from the graph a  The diagonal of A  A tells how many into each node. b  The offdiagonals   or  tell which pairs of nodes are . . Wh y does the nullspace of A  A contain          ? What is its rank? . Why does a complete graph with n   nodes have m   edges?  A spanning tree connecting all six nodes has edges. There are n       spanning trees! . The adjacency matrix of a graph has M    if nodes i and j are connected by an edge otherwise M   .  For the graph in Problem  with  nodes and  edges write down M and also M  . Why does  M    count the number of  step paths from node i to node j ? . Linear Transformations We know how a matrix moves subspaces around when we multiply by A . The nullspace goes into the zero vector.  All vectors go into the column space since Ax is always a combination of the columns.  You will soon see something beautifulthat A takes its row space into its column space and on those spaces of dimension r it is  percent in vertible. That is the real action of A . It is partly hidden by nullspaces and left nullspaces which lie at right angles and go their own way toward zero. What matters now is what happens inside the spacewhich means inside n dimensional space if A is n by n . That demands a closer look. Suppose x is an n dimensional vector. When A multiplies x  it transforms that vector into a new vector Ax .   This happens at every point x of the n dimensional space R  . The whole space is transformed or mapped into itself by the matrix A .  Figure . illustrates four transformations that come from matrices   A   c   c  . A multiple of the identity matrix A  cI  stretches every vector by the same factor c .  The whole space expands or contracts or somehow goes through the origin and out the opposite side when c is negative. A         . A rotation matrix turns the whole space around the origin. This example turns all vectors through     transforming every point  x  y  to   y  x  . A            .  A reflection matrix transforms every vector into its image on the opposite side of a mirror.   In this example the mirror is the   line y  x  and a point like      is unchanged.  A point like       is reversed to       . On a combination like v                    the matrix leaves one part and reverses the other part. The output is Av                  That  reflection  matrix  is  also  a  permutation  matrix!   It  is  alge braically so simple sending  x  y  to  y  x   that the geometric pic ture was concealed. A            .    A projection matrix  takes  the  whole  space  onto  a  lower dimensional  subspace  not  invertible.   The  example  transforms each vector  x  y  in the plane to the nearest point  x    on the hor izontal axis.  That axis is the column space of A .  The y axis that projects to      is the nullspace.                                     Those examples could be lifted into three dimensions.  There are matrices to stretch the earth or spin it or reflect it across the plane of the equator forth pole transforming to south pole. There is a matrix that projects everything onto that plane both poles to the center.  It is also important to recognize that matrices cannot do everything and some transformations T  x  are not possible with Ax  i  It is impossible to move the origin since A    for every matrix. ii  If the vector x goes to x   then  x must go to  x  . in general cx must go to cx   since A  cx   c  Ax  . iii  If the vectors x and y go to x  and y   then their sum x  y must go to x   y  since A  x  y   Ax  Ay . Matrix multiplication imposes those rules on the transformation.  The second rule con tains the first take c   to get A   .  We saw rule iii in action when      was  reflected across the   line.  It was split into             and the two parts were reflected separately.  The same could be done for projections  split project separately and add the projections.  These rules apply to any transformation that comes from a matrix . Their importance has earned them a name  Transformations that obey rules iiii are called linear transformations . The rules can be combined into one requirement T For all numbers c and d and all vectors x and y  matrix multiplication satisfies the rule of linearity A  cx  dy   c  Ax  d  Ay  .  Every transformation T  x  that meets this requirement is a linear transforma tion . Any matrix leads immediately to a linear transformation. The more interesting question is in the opposite direction Does every linear transformation lead to a matrix ?  The object of this section is to find the answer yes.  This is the foundation of an approach to linear algebrastarting with property  and developing its consequencesthat is much more abstract than the main approach in this book. We preferred to begin directly with matrices and now we see how they represent linear transformations. A transformation need not go from R  to the same space R  . It is absolutely permitted to transform vectors in R  to vectors in a different space R  . That is exactly what is done by an m by n matrix! The original vector x has n components and the transformed vector Ax has m components. The rule of linearity is equally satisfied by rectangular matrices so they also produce linear transformations. Having gone that far there is no reason to stop.  The operations in the linearity con dition  are addition and scalar multiplication but x and y need not be column vectors in R  .  Those are not the only spaces.  By definition any vector space allows the com binations cx  dy the vectors are x and y  but they may actually be polynomials or matrices or functions x  t  and y  t  . As long as the transformation satisfies equation  it is linear. We take as examples the spaces P    in which the vectors are polynomials p  t  of degree n . They look like p  a   a  t    a  t   and the dimension of the vector space is n   because with the constant term there are n   coefficients. Example . The operation of differentiation  A  d  dt  is linear Ap  t   d d t  a   a  t    a  t    a     na  t    .  The nullspace of this A is the onedimensional space of constants da   dt  .   The column space is the n dimensional space P      the righthand side of equation  is always in that space.  The sum of nullity    and rank   n  is the dimension of the original space P  .   Example . Integration from  to t is also linear  it takes P  to P     A p  t       a     a  t   dt  a  t    a  n   t    .  This time there is no nullspace except for the zero vector as always!  but integration does not produce all polynomials in P    . The right side of equation  has no constant term. Probably the constant polynomials will be the left nullspace. Example . Multiplication by a fixed polynomial like    t is linear Ap  t       t  a     a  t     a      a  t    . Again this transforms P  to P     with no nullspace except p  . In these examples and in almost all examples linearity is not difficult to verify.  It hardly even seems interesting.  If it is there it is practically impossible to miss.  Nev ertheless it is the most important property a transformation can have  .  Of course most transformations are not linearfor example to square the polynomial  Ap  p   or to add   A p  p   or to keep the positive coefficients  A  t  t    t .  It will be linear transformations and only those  that lead us back to matrices. Transformations Represented by Matrices Linearity has a crucial consequence If we know Ax for each vector in a basis then we know Ax for each vector in the entire space . Suppose the basis consists of the n vectors x  ... x  .  Every other vector x is a combination of those particular vectors they span the space. Then linearity determines Ax  Linearity If x  c  x     c  x  then Ax  c   Ax     c   Ax   .  The transformation T  x   Ax has no freedom left after it has decided what to do with the basis vectors. The rest is determined by linearity. The requirement  for two vectors x and y leads to condition  for n vectors x  ... x  .  The transformation does have a free hand with the vectors in the basis they are independent.  When those are settled the transformation of every vector is settled. Example . What linear transformation takes x  and x  to Ax  and Ax  ? x       goes to Ax             x       goes to Ax            . It must be multiplication T  x   Ax by the matrix A                    .     Starting  with  a  different  basis      and         this  same A is  also  the  only  linear transformation with A               and A                . Next we find matrices that represent differentiation and integration. First we must decide on a basis . For the polynomials of degree  there is a natural choice for the four basis vectors Basis for P  p     p   t  p   t   p   t  . That basis is not unique it never is but some choice is necessary and this is the most convenient. The derivatives of those four basis vectors are    t   t   Action of d  dt A p     A p   p   A p    p   A p    p  .   d  dt  is acting exactly like a matrix but which matrix? Suppose we were in the usual fourdimensional space with the usual basisthe coordinate vectors p             p             p             p            . The matrix is decided by equation  Differentiation matrix A                                                     . Ap  is its first column which is zero. A p  is the second column which is p  . Ap  is  p  and Ap  is  p  .  The nullspace contains p  the derivative of a constant is zero. The column space contains p   p   p  the derivative of a cubic is a quadratic.  The derivative of a combination like p    t  t   t  is decided by linearity and there is nothing new about thatit is the way we all differentiate. It would be crazy to memorize the derivative of every polynomial. The matrix can differentiate that p  t   because matrices build in linearity! d p d t  Ap                                                                                         t   t  . In short the matrix carries all the essential information .  If the basis is known and the matrix is known then the transformation of every vector is known. The coding of the information is simple.  To transform a space to itself one basis is enough. A transformation from one space to another requires a basis for each.   U Suppose the vectors x  ... x  are a basis for the space V  and vectors y  ... y  are a basis for W .   Each linear transformation T from V to W is represented by a matrix A .  The j th column is found by applying T to the j th basis vector x   and writing T  x   as a combination of the y s Column j of A T  x    Ax   a   y   a   y     a  y  .  For the differentiation matrix column  came from the first basis vector p   .  Its derivative is zero so column  is zero. The last column came from  d  dt  t    t  . Since  t    p    p    p    p   the last column contained   . . The rule  constructs the matrix a column at a time. We  do  the  same  for  integration.   That  goes  from  cubics  to  quartics  transforming V  P  into W  P   so we need a basis for W .  The natural choice is y    y   t  y   t   y   t   y   t   spanning the polynomials of degree . The matrix A will be m by n  or  by . It comes from applying integration to each basis vector of V      dt  t or Ax   y    ...    t  dt    t  or Ax     y  . Integration matrix A                                                        . Dif ferentiation and integration are inverse operations .  Or at least integration followed by differentiation brings back the original function.  To make that happen for matrices we need the differentiation matrix from quartics down to cubics which is  by  A                                                                 and A  A                 . Differentiation is a leftinverse of integration.  Rectangular matrices cannot have two sided inverses! In the opposite order A  A   I cannot be true. The  by  product has zeros in column . The derivative of a constant is zero. In the other columns A  A  is the identity and the integral of the derivative of t  is t  .  Rotations Q  Projections P  and Reflections H This section began with   rotations projections onto the x axis and reflections through the   line. Their matrices were especially simple Q          P             H             . The underlying linear transformations of the x  y plane are also simple.  But rotations through other angles projections onto other lines and reflections in other mirrors are almost as easy to visualize They are still linear transformations provided that the origin is fixed A   . They must be represented by matrices. Using the natural basis     and      we want to discover those matrices. .   Rotation Figure . shows rotation through an angle  . It also shows the effect on the two basis vectors.  The first one goes to  cos   sin    whose length is still  it lies on the   line.  The second basis vector      rotates into   sin   cos   .  By rule  those numbers go into the columns of the matrix we use c and s for cos  and sin  . This family of rotations Q  is a perfect chance to test the correspondence between transformations and matrices Does the inverse of Q  equal Q   rotation backward through  ? Yes. Q  Q     c  s s c  c s  s   c             . Does the square of Q  equal Q   rotation through a double angle? Yes. Q     c  s s c  c  s s c    c   s    cs  cs c   s     cos    sin   sin   cos    . Does the product of Q  and Q  equal Q    rotation through  then  ? Yes. Q  Q    cos  cos   sin  sin   sin  cos   cos  sin      cos       sin        . The last case contains the first two.   The inverse appears when  is     and the square appears when  is   .  All three questions were decided by trigonometric identities and they give a new way to remember those identities. It was no accident that all the answers were yes. Matrix multiplication is defined exactly so that the product of the matrices corresponds to the product of the transformations . V Suppose A and B are linear transformations from V to W and from U to V .  Their product AB starts with a vector u in U  goes to Bu in V  and                                                                             finishes with ABu in W .  This composition AB is again a linear transfor mation from U to W .  Its matrix is the product of the individual matrices representing A and B . For A  A   the composite transformation was the identity and A  A  annihilated all constants.  For rotations the order of multiplication does not matter.  Then U  V  W is  the x  y plane  and Q  Q  is  the  same  as Q  Q  .   For  a  rotation  and  a reflection the order makes a difference. T echnical note To construct the matrices we need bases for V and W  and then for U and V .  By keeping the same basis for V  the product matrix goes correctly from the basis in U to the basis in W .   If we distinguish the transformation A from its matrix call that  A   then the product rule  V becomes extremely concise  AB    A  B  . The rule for multiplying matrices in Chapter  was totally determined by this requirementit must match the product of linear transformations. .   Projection Figure . also shows the projection of      onto the  line.   The length of the projection is c  cos  . Notice that the point of projection is not  c  s   as I mistakenly thought that vector has length  it is the rotation so we must multiply by c . Similarly the projection of      has length s  and falls at s  c  s    cs  s    That gives the second column of the projection matrix P  Projection onto  line P   c  cs cs   s   . This matrix has no inverse because the transformation has no inverse. Points on the perpendicular line are projected onto the origin that line is the nullspace of P . Points on the  line are projected to themselves! Projecting twice is the same as projecting once and P   P  P    c  cs cs   s      c   c   s   cs  c   s   cs  c   s   s   c   s     P .                                                                                                Of course c   s   cos    sin    . A projection matrix equals its own square . .   Reflection Figure . shows the reflection of      in the  line. The length of the reflection equals the length of the original as it did after rotationbut here the   line stays where it is. The perpendicular line reverses direction all points go straight through the mirror Linearity decides the rest. Reflection matrix H    c     cs  cs  s     . This matrix H has  the  remarkable  property H   I . Two  reflections  bring  back the  original .   A  reflection  is  its  own  inverse H  H     which  is  clear  from  the geometry but less clear from the matrix. One approach is through the relationship of reflections to projections H   P  I .  This means that Hx  x   Px the image plus the original equals twice the projection. It also confirms that H   I  H     P  I     P    P  I  I  since P   P . Other transformations Ax can increase the length of x  stretching and shearing are in the exercises. Each example has a matrix to represent itwhich is the main point of this section.  But there is also the question of choosing a basis and we emphasize that the matrix depends on the choice of basis . Suppose the first basis vector is on the  line and the second basis vector is perpendicular  i  The projection matrix is back to P      .  This matrix is constructed as always its first column comes from the first basis vector projected to itself.  The second column comes from the basis vector that is projected to zero.   ii  For reflections that same basis gives H        .  The second basis vector is re flected onto its negative to produce this second column. The matrix H is still  P  I when the same basis is used for H and P . iii  For rotations the matrix is not changed. Those lines are still rotated through   and Q        as before. The whole question of choosing the best basis is absolutely central and we come back to it in Chapter . The goal is to make the matrix diagonal as achieved for P and H . To make Q diagonal requires complex vectors since all real vectors are rotated. We mention here the effect on the matrix of a change of basis while the linear trans formation stays the same. The matrix A or Q or P or H  is altered to S   AS .  Thus a single transformation is represented by different matrices via different bases accounted for by S .  The theory of eigenvectors will lead to this formula S   AS  and to the best basis. Pr oblem Set . . What matrix has the effect of rotating every vector through   and then projecting the result onto the x axis? What matrix represents projection onto the x axis followed by projection onto the y axis? . Does the product of  reflections and  rotations of the x  y plane produce a rotation or a reflection? . The matrix A      produces a stretching in the x direction. Draw the circle x   y    and sketch around it the points   x  y  that result from multiplication by A . What shape is that curve? . Every  straight  line  remains  straight  after  a  linear  transformation.   If z is  halfway between x and y  show that Az is halfway between Ax and Ay . . The matrix A      yields a shearing transformation which leaves the y axis un changed.  Sketch its effect on the x axis by indicating what happens to      and      and       and how the whole axis is transformed. . What  by  matrices represent the transformations that a  project every vector onto the x  y plane? b  reflect every vector through the x  y plane? c  rotate the x  y plane through    leaving the z axis alone? d  rotate the x  y plane then x  z  then y  z  through   ? e  carry out the same three rotations but each one through   ?  . On the space P  of cubic polynomials what matrix represents d   dt  ?  Construct the  by  matrix from the standard basis  t  t   t  .  Find its nullspace and column space. What do they mean in terms of polynomials? . From  the  cubics P  to  the  fourthdegree  polynomials P    what  matrix  represents multiplication by    t ?  The columns of the  by  matrix A come from applying the transformation to  t  t   t  . . The solutions to the linear differential equation d  u  dt   u form a vector space since combinations of solutions are still solutions. Find two independent solutions to give a basis for that solution space. . With initial values u  x and du  dt  y at t   what combination of basis vectors in Problem  solves u   u ?  This transformation from initial values to solution is linear. What is its  by  matrix using x   y   and x   y   as basis for V  and your basis for W ? . Verify directly from c   s    that reflection matrices satisfy H   . . Suppose A is a linear transformation from the x  y plane to itself. Why does A    x  y   A   x  A   y ?  If A is represented by the matrix M  explain why A   is repre sented by M   . . The  product  AB  C of  linear  transformations  starts  with  a  vector x and  produces u  Cx . Then rule V applies AB to u and reaches  AB  Cx . a  Is this result the same as separately applying C then B then A ? b  Is the result the same as applying BC followed by A ? Parentheses are unnecessary and the associative law  AB  C  A  BC  holds for linear transformations. This is the best proof of the same law for matrices. . Prove that T  is a linear transformation if T is linear from R  to R  . . The space of all  by  matrices has the four basis vectors                                            . For the linear transformation of transposing  find its matrix A with respect to this basis. Why is A   I ? . Find the  by  cyclic permutation matrix  x   x   x   x   is transformed to Ax   x   x   x   x   . What is the effect of A  ? Show that A   A   . . Find the  by  matrix A that represents a right shift   x   x   x   is transformed to    x   x   x   .   Find also the left shift matrix B from R  back to R    transforming  x   x   x   x   to  x   x   x   . What are the products AB and BA ?   . In the vector space P  of all p  x   a   a  x  a  x   a  x   let S be the subset of polynomials with    p  x  dx  . Verify that S is a subspace and find a basis. . A nonlinear transformation  is  invertible  if T  x   b has  exactly  one  solution  for every b .  The example T  x   x  is not invertible because x   b has two solutions for positive b and no solution for negative b . Which of the following transformations from the real numbers R  to the real numbers R   are invertible?  None are linear not even c. a T  x   x  . b T  x   e  . c T  x   x  . d T  x   cos x . . What is the axis and the rotation angle for the transformation that takes  x   x   x   into  x   x   x   ? . A linear transformation must leave the zero vector fixed T     . Prove this from T  v  w   T  v  T  w  by choosing w  . Prove it also from the requirement T  cv   cT  v  by choosing c  . Which of these transformations is not linear? The input is v   v   v   . a T  v    v   v   . b T  v    v   v   . c T  v      v   . d T  v        . . If S and T are linear with S  v   T  v   v  then S  T  v   v or v  ? . Suppose T  v   v  except that T    v         . Show that this transformation satis fies T  cv   cT  v  but not T  v  w   T  v  T  w  . . Which of these transformations satisfy T  v  w   T  v   T  w   and which satisfy T  cv   cT  v  ? a T  v   v   v  . b T  v   v   v   v  . c T  v    v    v    v   . d T  v   largest component of v . . For these transformations of V  R  to W  R   find T  T  v  . a T  v    v . b T  v   v      . c T  v     rotation    v   v   . d T  v   projection   v   v    v   v    . . The cyclic  transformation T is  defined  by T  v   v   v     v   v   v   .   What  is T  T  T  v  ? What is T   v  ?  . Find the range and kernel those are new words for the column space and nullspace of T . a T  v   v     v   v   . b T  v   v   v     v   v   . c T  v   v         . d T  v   v     v   v   . . A linear transformation from V to W has an inverse from W to V when the range is all of W and the kernel contains only v  .  Why are these transformations not invertible? a T  v   v     v   v   W  R  . b T  v   v     v   v   v   v   W  R  . c T  v   v    v  W  R  . . Suppose a linear T transforms      to      and      to      . Find T  v  when a v       . b v       . c v        . d v   a  b  . Problems  may be harder. The input space V contains all  by  matrices M . . M is any  by  matrix and A      .  The linear transformation T is defined by T  M   AM . What rules of matrix multiplication show that T is linear? . Suppose A      .  Show that the identity matrix I is not in the range of T .  Find a nonzero matrix M such that T  M   AM is zero. . Suppose T transposes  every  matrix M .   Try  to  find  a  matrix A that  gives AM  M  for every M .  Show that no matrix A will do it. To professors Is this a linear transformation that doesnt come from a matrix? . The transformation T that transposes every matrix is definitely linear. Which of these extra properties are true? a T   identity transformation. b  The kernel of T is the zero matrix. c  Every matrix is in the range of T . d T  M    M is impossible. . Suppose T  M              .  Find a matrix with T  M    .  Describe all ma trices with T  M    the kernel of T  and all output matrices T  M  the range of T . Problems  are about changing the basis . a  What matrix transforms      into      and transforms      to      ? b  What matrix transforms      to      and      to      ?   c  Why does no matrix transform      to      and      to      ? . a  What matrix M transforms      and      to  r  t  and  s  u  ? b  What matrix N transforms  a  c  and  b  d  to      and      ? c  What condition on a  b  c  d will make part b impossible? . a  How do M and N in Problem  yield the matrix that transforms  a  c  to  r  t  and  b  d  to  s  u  ? b  What matrix transforms      to      and      to      ? . If you keep the same basis vectors but put them in a different order the changeof basis matrix M is a matrix. If you keep the basis vectors in order but change their lengths M is a matrix. . The matrix that transforms      and      to      and      is M  . The combination a       b      that equals      has  a  b      .  How are those new coordinates of      related to M or M   ? . What are the three equations for A  B  C if the parabola Y  A  Bx  Cx  equals  at x  a   at x  b  and  at x  c ?  Find the determinant of the  by  matrix.  For which numbers a  b  c will it be impossible to find this parabola Y ? . Suppose v   v   v  are eigenvectors for T .  This means T  v      v  for i      . What is the matrix for T when the input and output bases are the v s? . Ev ery invertible linear transformation can have I as its matrix.  For the output basis just choose w   T  v   . Why must T be invertible? . Suppose T is reflection across the x axis and S is reflection across the y axis.  The domain V is the x  y plane. If v   x  y  what is S  T  v  ? Find a simpler description of the product ST . . Suppose T is reflection across the   line and S is reflection across the y axis If v       then T  v        .  Find S  T  v  and T  S  v  .  This shows that generally ST   T S . . Show that the product ST of two reflections is a rotation.  Multiply these reflection matrices to find the rotation angle  cos   sin   sin    cos      cos   sin   sin    cos    . . The  by  Hadamard matrix is entirely   and   H                                  .  Find H   and write v           as a combination of the columns of H . . Suppose we have two bases v  ... v  and w  ... w  for R  .  If a vector has coeffi cients b  in one basis and c  in the other basis what is the changeofbasis matrix in b  Mc ? Start from b  v     b  v   V b  c  w     c  w   W c . Your answer represents T  v   v with input basis of v s and output basis of w s. Because of different bases the matrix is not I . . True or false If we know T  v  for n different nonzero vectors in R   then we know T  v  for every vector in R  . . Recommended Suppose all vectors x in the unit square   x      x    are transformed to Ax  A is  by . a  What is the shape of the transformed region all Ax ? b  For which matrices A is that region a square? c  For which A is it a line? d  For which A is the new area still ? Re view Exercises . Find a basis for the following subspaces of R   a  The vectors for which x    x  . b  The vectors for which x   x   x    and x   x   . c  The subspace spanned by                     and          . . By giving a basis describe a twodimensional subspace of R  that contains none of the coordinate vectors                        . . True or false with counterexample if false a  If the vectors x  ... x  span a subspace S  then dim S  m . b  The intersection of two subspaces of a vector space cannot be empty. c  If Ax  Ay  then x  y . d  The row space of A has a unique basis that can be computed by reducing A to echelon form. e  If a square matrix A has independent columns so does A  .   . What is the echelon form U of A ? A                            . What are the dimensions of its four fundamental subspaces? . Find the rank and the nullspace of A                             and B                                      . . Find bases for the four fundamental subspaces associated with A             B             C                        . . What is the most general solution to u  v  w   u  w  ? . a  Construct a matrix whose nullspace contains the vector x         . b  Construct a matrix whose left nullspace contains y       . c  Construct a matrix whose column space is spanned by        and whose row space is spanned by      . d  If you are given any three vectors in R  and any three vectors in R   is there a  by  matrix whose column space is spanned by the first three and whose row space is spanned by the second three? . In the vector space of  by  matrices a  is the set of rank  matrices a subspace? b  what subspace is spanned by the permutation matrices? c  what subspace is spanned by the positive matrices all a   ? d  what subspace is spanned by the invertible matrices? . Invent a vector space that contains all linear transformations from R  to R  .  You have to decide on a rule for addition. What is its dimension? . a  Find the rank of A  and give a basis for its nullspace. A  LU                                                                                                            .  b  The first  rows of U are a basis for the row space of A true or false? Columns    of U are a basis for the column space of A true or false? The four rows of A are a basis for the row space of A true or false? c  Find as many linearly independent vectors b as possible for which Ax  b has a solution. d  In elimination on A  what multiple of the third row is subtracted to knock out the fourth row? . If A is an n by n   matrix  and its rank is n    what is the dimension of its nullspace? . Use elimination to find the triangular factors in A  LU  if A       a   a   a   a a   b   b   b a   b   c   c a   b   c   d      . Under what conditions on the numbers a  b  c  d are the columns linearly indepen dent? . Do the vectors                 and        form a basis for R  ? . What do you know about   A  when the number of solutions to Ax  b is a   or  depending on b . b   independent of b . c   or   depending on b . d   regardless of b . . In the previous exercise how is r related to m and n in each example? . If x is a vector in R   and x  y   for every y  prove that x  . . If A is an n by n matrix such that A   A and rank A  n  prove that A  I . . What subspace of  by  matrices is spanned by the elementary matrices E   with s on the diagonal and at most one nonzero entry below? . How many  by  permutation matrices are there?  Are they linearly independent? Do they span the space of all  by  matrices? No need to write them all down. . What is the rank of the n by n matrix with every entry equal to ?  How about the checkerboard matrix with a    when i  j is even a    when i  j is odd?   . a Ax  b has a solution under what conditions on b  for the following A and b ? A                                      and b     b  b  b     . b  Find a basis for the nullspace of A . c  Find the general solution to Ax  b  when a solution exists. d  Find a basis for the column space of A . e  What is the rank of A  ? . How can you construct a matrix that transforms the coordinate vectors e   e   e  into three given vectors v   v   v  ? When will that matrix be invertible? . If e   e   e  are in the column space of a  by  matrix does it have a leftinverse? Does it have a rightinverse? . Suppose T is the linear transformation on R  that takes each point  u  v  w  to  u  v  w  u  v  u   Describe what T   does to the point  x  y  z  . . True or false? a  Every subspace of R  is the nullspace of some matrix. b  If A has the same nullspace as A   the matrix must be square. c  The transformation that takes x to mx  b is linear from R  to R  . . Find bases for the four fundamental subspaces of A                                                     and A                  . . a  If the rows of A are linearly independent  A is m by n  then the rank is  the column space is  and the left nullspace is . b  If A is  by  with a twodimensional nullspace show that Ax  b can be solved for every b . . Describe the linear transformations of the x  y plane that are represented with stan dard basis      and      by the matrices A           A              A            . . a  If A is square show that the nullspace of A  contains the nullspace of A .  b  Show also that the column space of A  is contained in the column space of A . . When does the rank matrix A  uv  have A   ? . a  Find a basis for the space of all vectors in R  with x   x   x   x   x   x  . b  Find a matrix with that subspace as its nullspace. c  Find a matrix with that subspace as its column space. . Suppose the matrices in PA  LU are                                                                                                                                                                                       . a  What is the rank of A ? b  What is a basis for the row space of A ? c True or false  Rows    of A are linearly independent. d  What is a basis for the column space of A ? e  What is the dimension of the left nullspace of A ? f  What is the general solution to Ax  ?   . Orthogonal Vectors and Subspaces A basis is a set of independent vectors that span a space.  Geometrically it is a set of coordinate axes. A vector space is defined without those axes but every time I think of the x  y plane or threedimensional space or R   the axes are there. They are usually per pendicular! The coordinate axes that the imagination constructs are practically always orthogonal . In choosing a basis we tend to choose an orthogonal basis. The idea of an orthogonal basis is one of the foundations of linear algebra. We need a basis to convert geometric constructions into algebraic calculations and we need an orthogonal basis to make those calculations simple.  A further specialization makes the basis just about optimal  The vectors should have length .  For an orthonormal basis orthogonal unit vectors we will find .  the length  x  of a vector .  the test x  y   for perpendicular vectors and .  how to create perpendicular vectors from linearly independent vectors. More than just vectors subspaces can also be perpendicular.  We will discover so beautifully and simply that it will be a delight to see that the fundamental subspaces meet at right angles .  Those four subspaces are perpendicular in pairs two in R  and two in R  . That will complete the fundamental theorem of linear algebra. The  first  step  is  to  find  the length  of  a  vector .   It  is  denoted  by  x    and  in  two dimensions it comes from the hypotenuse of a right triangle Figure .a.  The square of the length was given a long time ago by Pythagoras  x    x    x   . In threedimensional space x   x   x   x   is the diagonal of a box Figure .b. Its length comes from two applications of the Pythagorean formula.  The twodimensional case takes care of  x   x             across the base. This forms a right angle with the vertical side      x           . The hypotenuse of the bold triangle Pythagoras again                                                                                                is the length  x  we want Length in D  x            and  x    x    x    x   . The extension to x   x  ... x   in n dimensions is immediate. By Pythagoras n   times the length  x  in R  is the positive square root of x  x  Length squared  x    x    x      x    x  x .  The sum of squares matches x  x and the length of x          is   x  x                               . Orthogonal Vectors How can we decide whether two vectors x and y are perpendicular?  What is the test for  orthogonality  in  Figure  .?   In  the  plane  spanned  by x and y   those  vectors  are orthogonal provided they form a right triangle . We go back to a   b   c   Sides of a right triangle  x     y     x  y   .  Applying the length formula  this test for orthogonality in R  becomes  x      x      y      y      x   y       x   y    . The righthand side has an extra   x  y  from each  x   y     righthand side   x      x       x  y     x  y    y      y    .                                                       We have a right triangle when that sum of crossproduct terms x  y  is zero  Orthogonal vectors x  y  x  y     x  y    .  This sum is x  y   x  y   y  x  the row vector x  times the column vector y  Inner product x  y   x   x      y  . . . y      x  y     x  y  .  This number is sometimes called the scalar product or dot product and denoted by  x  y  or x  y . We will use the name inner product and keep the notation x  y . A The inner product x  y is zero if and only if x and y are orthogonal vectors. If x  y   their angle is less than   .  If x  y   their angle is greater than   . The length squared is the inner product of x with itself x  x  x      x     x   . The only vector with length zerothe only vector orthogonal to itselfis the zero vector. This vector x   is orthogonal to every vector in R  . Example .         is orthogonal to         . Both have length        . Useful fact If nonzero vectors v  ... v  are mutually orthogonal every vector is perpendicular to every other then those vectors are linearly independent . Proof. Suppose c  v     c  v   .   To  show  that c  must  be  zero  take  the  inner product of both sides with v  . Orthogonality of the v s leaves only one term v    c  v     c  v    c  v   v    .  The vectors are nonzero so v   v     and therefore c   . The same is true of every c  . The only combination of the v s producing zero has all c    independence ! The coordinate vectors e  ... e  in R  are the most important orthogonal vectors. Those are the columns of the identity matrix.  They form the simplest basis for R   and  they are unit vectors each has length  e    . They point along the coordinate axes. If these axes are rotated the result is a new orthonormal basis  a new system of mutually orthogonal unit vectors . In R  we have cos    sin     Orthonormal vectors in R  v    cos   sin   and v     sin   cos   . Orthogonal Subspaces W e come to the orthogonality of two subspaces. Every vector in one subspace must be orthogonal to every vector in the other subspace . Subspaces of R  can have dimension       or .   The subspaces are represented by lines or planes through the origin and in the extreme cases  by the origin alone or the whole space.  The subspace    is orthogonal to all subspaces.  A line can be orthogonal to another line or it can be orthogonal to a plane but a plane cannot be orthogonal to a plane . I have to admit that the front wall and side wall of a room look like perpendicular planes in R  .  But by our definition that is not so!  There are lines v and w in the front and side walls that do not meet at a right angle.  The line along the corner is in both walls and it is certainly not orthogonal to itself. B Two subspaces V and W of the same space R  are orthogonal if every vector v in V is orthogonal to every vector w in W  v  w   for all v and w . Example . Suppose V is the plane spanned by v            and v            . If W is the line spanned by w            then w is orthogonal to both v s.  The line W will be orthogonal to the whole plane V . In this case with subspaces of dimension  and  in R   there is room for a third subspace.  The line L through z            is perpendicular to V and W .  Then the dimensions add to       . What space is perpendicular to all of V  W  and L ? The important orthogonal subspaces dont come by accident and they come two at a time.  In fact orthogonal subspaces are unavoidable They are the fundamental sub spaces! The first pair is the nullspace and row space .  Those are subspaces of R  the rows have n components and so does the vector x in Ax  .  We have to show using Ax   that the rows of A are orthogonal to the nullspace vector x . C Fundamental theorem of orthogonality The row space is orthogonal to the nullspace in R  .  The column space is orthogonal to the left nullspace in R  . First Proof. Suppose x is a vector in the nullspace.  Then Ax   and this system of m   equations can be written out as rows of A multiplying x  Every row is orthogonal to x Ax        row    row   . . . . . . . . .  row m            x  x  . . . x               . . .       .  The main point is already in the first equation row  is orthogonal to x .  Their inner product is zero that is equation .  Every righthand side is zero so x is orthogonal to every row.  Therefore x is orthogonal to every combination of the rows.  Each x in the nullspace is orthogonal to each vector in the row space so   A     A   . The other pair of orthogonal subspaces comes from A  y   or y  A   y  A   y   y           c c o o l l u  u m m n n  n               .  The vector y is orthogonal to every column.  The equation says so from the zeros on the righthand side.   Therefore y is orthogonal to every combination of the columns. It  is  orthogonal  to  the  column  space  and  it  is  a  typical  vector  in  the  left  nullspace   A      A  .   This is the same as the first half of the theorem  with A replaced by A  . Second Proof. The contrast with this coordinatefree proof should be useful to the reader.  It shows a more abstract method of reasoning.  I wish I knew which proof is clearer and more permanently understood. If x is in the nullspace then Ax  . If v is in the row space it is a combination of the rows v  A  z for some vector z . Now in one line Nullspace  Row space v  x   A  z   x  z  Ax  z     .  Example . Suppose A has rank  so its row space and column space are lines Rank matrix A                    . The rows are multiples of      . The nullspace contains x         which is orthogonal to all the rows. The nullspace and row space are perpendicular lines in R                   and                 and              .  In contrast the other two subspaces are in R  .  The column space is the line through        .  The left nullspace must be the perpendicular plane y    y    y   .  That equation is exactly the content of y  A  . The first two subspaces the two lines had dimensions      in the space R  . The second pair line and plane had dimensions      in the space R  .  In general the row space and nullspace have dimensions that add to r   n  r   n .  The other pair adds to r  m  r   m . Something more than orthogonality is occurring and I have to ask your patience about that one further point the dimensions . It is certainly true that the null space is perpendicular to the row spacebut it is not the whole truth.   A  contains every vector orthogonal to the row space . The nullspace was formed from all solutions to Ax  . Definition. Gi ven a subspace V of R   the space of all vectors orthogonal to V is called the orthogonal complement of V . It is denoted by V    V perp. Using this terminology the nullspace is the orthogonal complement of the row space   A      A    . At the same time the row space contains all vectors that are orthog onal to the nullspace. A vector z cant be orthogonal to the nullspace but outside the row space.  Adding z as an extra row of A would enlarge the row space but we know that there is a fixed formula r  n  r   n  Dimension formula dimrow space  dimnullspace  number of columns. Every vector orthogonal to the nullspace is in the row space   A       A   . The same reasoning applied to A  produces the dual result The left nullspace   A   and the column space   A  are orthogonal complements .  Their dimensions add up to  m  r   r  m  This completes the second half of the fundamental theorem of linear algebra. The first half gave the dimensions of the four subspaces. including the fact that row rank  column rank.  Now we know that those subspaces are perpendicular.  More than that the subspaces are orthogonal complements. D Fundamental Theorem of Linear Algebra Part II The nullspace is the orthogonal complement of the row space in R  . The left nullspace is the orthogonal complement of the column space in R  . To repeat the row space contains everything orthogonal to the nullspace.  The column space contains everything orthogonal to the left nullspace. That is just a sentence hidden in the middle of the book but it decides exactly which equations can be solved ! Looked at directly Ax  b requires b to be in the column space.  Looked at indirectly. Ax  b requires b to be perpendicular to the left nullspace . E Ax  b is solvable if and only if y  b   whenever y  A  .   The direct approach was  b must be a combination of the columns.  The indirect ap proach is  b must be orthogonal to every vector that is orthogonal to the columns. That doesnt sound like an improvement to put it mildly.  But if only one or two vectors are orthogonal to the columns.  it is much easier to check those one or two conditions y  b  .  A good example is Kirchhoffs Voltage Law in Section ..  Testing for zero around loops is much easier than recognizing combinations of the columns. When the lefthand sides of Ax  b add to zero the righthand sides must too x   x   b  x   x   b  x   x   b  is solvable if and only if b   b   b    . Here A                    . This test b   b   b    makes b orthogonal to y         in the left nullspace.  By the Fundamental Theorem b is a combination of the columns! The Matrix and the Subspaces We  emphasize  that V and W can  be  orthogonal  without  being  complements.   Their dimensions can be too small.  The line V spanned by        is orthogonal to the line W spanned by         but V is not W  .  The orthogonal complement of W is a two dimensional plane  and the line is only part of W  .   When the dimensions are right orthogonal subspaces are necessarily orthogonal complements If W  V  then V  W  and    dim V  dim W  n . In other words V   V .  The dimensions of V and W are right and the whole space R  is being decomposed into two perpendicular parts Figure ..                        Splitting R  into orthogonal parts will split every vector into x  v  w . The vector v is the projection onto the subspace V . The orthogonal component w is the projection of x onto W . The next sections show how to find those projections of x . They lead to what is probably the most important figure in the book Figure .. Figure . summarizes the fundamental theorem of linear algebra.  It illustrates the true effect of a matrixwhat is happening inside the multiplication Ax .  The nullspace                    is carried to the zero vector.  Every Ax is in the column space.  Nothing is carried to the left nullspace. The real action is between the row space and column space  and you see it by looking at a typical vector x .  It has a row space component and a nullspace component with x  x   x  . When multiplied by A  this is Ax  Ax   Ax   The nullspace component goes to zero Ax   . The row space component goes to the column space Ax   Ax . Of course everything goes to the column spacethe matrix cannot do anything else.  I tried to make the row and column spaces the same size with equal dimension r . F From the row space to the column space A is actually invertible.  Every vector b in  the  column  space  comes  from  exactly  one  vector x  in  the  row space. Proof. Every b in the column space is a combination Ax of the columns.  In fact b is Ax   with x  in the row space since the nullspace component gives Ax    If another vector x   in the row space gives Ax    b  then A  x   x     b  b  .  This puts x   x   in the nullspace and the row space which makes it orthogonal to itself.  Therefore it is zero and x   x   . Exactly one vector in the row space is carried to b . Ev ery matrix transforms its row space onto its column space. On those r dimensional spaces A is invertible.  On its nullspace A is zero.  When A is diagonal you see the invertible submatrix holding the r nonzeros. A  goes in the opposite direction  from R  to R  and from   A  back to   A   . Of course the transpose is not the inverse! A  moves the spaces correctly but not the   individual vectors. That honor belongs to A   if it existsand it only exists if r  m  n . We cannot ask A   to bring back a whole nullspace out of the zero vector. When A   fails to exist the best substitute is the pseudoinverse A  .  This inverts A where that is possible A  Ax  x for x in the row space.  On the left nullspace nothing can be done A  y  . Thus A  inverts A where it is invertible and has the same rank r . One formula for A  depends on the singular value decomposition for which we first need to know about eigenvalues. Pr oblem Set . . Find the lengths and the inner product of x           and y            . . Give an example in R  of linearly independent vectors that are not orthogonal. Also give an example of orthogonal vectors that are not independent. . Two  lines  in  the  plane  are  perpendicular  when  the  product  of  their  slopes  is  . Apply this to the vectors x   x   x   and y   y   y    whose slopes are x   x  and y   y   to derive again the orthogonality condition x  y  . . How do we know that the i th row of an invertible matrix B is orthogonal to the j th column of B    if i   j ? . Which pairs are orthogonal among the vectors v   v   v   v  ? v                   v                  v                     v                 . . Find  all  vectors  in R  that  are  orthogonal  to        and         .   Produce  an orthonormal basis from these vectors mutually orthogonal unit vectors. . Find a vector x orthogonal to the row space of A  and a vector y orthogonal to the column space and a vector z orthogonal to the nullspace A                             . . If V and W are orthogonal subspaces show that the only vector they have in common is the zero vector V  W     . . Find the orthogonal complement of the plane spanned by the vectors        and         by taking these to be the rows of A and solving Ax  . Remember that the complement is a whole line.  . Construct a homogeneous equation in three unknowns whose solutions are the linear combinations of the vectors        and        . This is the reverse of the previous exercise but the two problems are really the same. . The fundamental theorem is often stated in the form of Fredholms alternative  For any A and b  one and only one of the following systems has a solution i Ax  b . ii A  y   y  b   . Either b is in the column space   A  or there is a y in   A   such that y  b   . Show that it is contradictory for i and ii both to have solutions. . Find a basis for the orthogonal complement of the row space of A  A                  . Split x         into a row space component x  and a nullspace component x  . . Illustrate the action of A  by a picture corresponding to Figure . sending   A  back to the row space and the left nullspace to zero. . Show that x  y is orthogonal to x  y if and only if  x    y  . . Find a matrix whose row space contains        and whose nullspace contains          or prove that there is no such matrix. . Find all vectors that are perpendicular to          and          . . If V is the orthogonal complement of W in R   is there a matrix with row space V and nullspace W ? Starting with a basis for V  construct such a matrix. . If S     is the subspace of R  containing only the zero vector what is S  ? If S is spanned by           what is S  ? What is  S    ? . Why are these statements false ? a  If V is orthogonal to W  then V  is orthogonal to W  . b V orthogonal to W and W orthogonal to Z makes V orthogonal to Z . . Let S be a subspace of R  . Explain what  S     S means and why it is true. . Let P be the plane in R  with equation x   y  z  .  Find a vector perpendicular to P . What matrix has the plane P as its nullspace and what matrix has P as its row space? . Let S be the subspace of R  containing all vectors with x   x   x   x   . Find a basis for the space S   containing all vectors orthogonal to S .   . Construct an unsymmetric  by  matrix of rank .  Copy Figure . and put one vector in each subspace. Which vectors are orthogonal? . Redraw Figure . for a  by  matrix of rank r  .  Which subspace is Z zero vector only? The nullspace part of any vector x in R  is x   . . Construct a matrix with the required property or say why that is impossible. a  Column space contains       and        nullspace contains      . b  Row space contains       and        nullspace contains      . c Ax       has a solution and A             . d  Every row is orthogonal to every column  A is not the zero matrix. e  The columns add up to a column of s the rows add to a row of s. . If AB   then the columns of B are in the of A . The rows of A are in the of B . Why cant A and B be  by  matrices of rank ? . a  If Ax  b has a solution and A  y   then y is perpendicular to . b  If A  y  c has a solution and Ax   then x is perpendicular to . . This is a system of equations Ax  b with no solution  x   y   z    x   y   z    x   y   z   . Find numbers y   y   y  to multiply the equations so they add to   .  You have found a vector y in which subspace? The inner product y  b is . . In Figure . how do we know that Ax  is equal to Ax ?  How do we know that this vector is in the column space? If A      and x      what is x  ? . If Ax is in the nullspace of A  then Ax  . Reason Ax is also in the of A and the spaces are . Conclusion A  A has the same nullspace as A . . Suppose A is a symmetric matrix  A   A . a  Why is its column space perpendicular to its nullspace? b  If Ax   and Az   z  which subspaces contain these eigenvectors x and z ? Symmetric matrices have perpendicular eigenvectors see Section .. . Recommended Draw Figure . to show each subspace for A            and B            .  . Find the pieces x  and x   and draw Figure . properly if A               and x      . Problems  are about orthogonal subspaces. . Put bases for the orthogonal subspaces V and W into the columns of matrices V and W . Why does V  W  zero matrix ? This matches v  w   for vectors. . The floor and the wall are not orthogonal subspaces because they share a nonzero vector along the line where they meet.  Two planes in R  cannot be orthogonal! Find a vector in both column spaces   A  and   B   A                    and B                    . This will be a vector Ax and also B  x . Think  by  with the matrix  A  B  . . Extend Problem  to a p dimensional subspace V and a q dimensional subspace W of R  . What inequality on p  q guarantees that V intersects W in a nonzero vector? These subspaces cannot be orthogonal. . Prove that every y in   A   is perpendicular to every Ax in the column space using the matrix shorthand of equation . Start from A  y  . . If S is the subspace of R  containing only the zero vector what is S  ? If S is spanned by         what is S  ? If S is spanned by        and         what is S  ? . Suppose S only  contains        and        not  a  subspace.   Then S  is  the nullspace of the matrix A  . S  is a subspace even if S is not. . Suppose L is a onedimensional subspace a line in R  . Its orthogonal complement L  is the perpendicular to L .  Then  L    is a perpendicular to L  .  In fact  L    is the same as . . Suppose V is the whole space R  .  Then V  contains only the vector . Then  V    is . So  V    is the same as . . Suppose S is spanned by the vectors          and          . Find two vectors that span S  . This is the same as solving Ax   for which A ? . If P is the plane of vectors in R  satisfying x   x   x   x    write a basis for P  . Construct a matrix that has P as its nullspace. . If a subspace S is contained in a subspace V  prove that S  contains V  . Problems  are about perpendicular columns and rows.   . Suppose an n by n matrix is invertible AA    I .  Then the first column of A   is orthogonal to the space spanned by which rows of A ? . Find A  A if the columns of A are unit vectors all mutually perpendicular. . Construct a  by  matrix A with no zero entries whose columns are mutually per pendicular. Compute A  A . Why is it a diagonal matrix? . The lines  x  y  b  and  x   y  b  are . They are the same line if . In that case  b   b   is perpendicular to the vector . The nullspace of the matrix is the line  x  y  . One particular vector in that nullspace is . . Wh y is each of these statements false? a        is perpendicular to          so the planes x  y  z   and x  y   z   are orthogonal subspaces. b  The subspace spanned by            and            is the orthogonal com plement of the subspace spanned by             and              . c  Two subspaces that meet only in the zero vector are orthogonal. . Find a matrix with v         in the row space and column space.  Find another matrix with v in the nullspace and column space. Which pairs of subspaces can v not be in? . Suppose A is  by  B is  by  and AB  . Prove rank  A  rank  B   . . The command N  nullA will produce a basis for the nullspace of A .  Then the command B  nullN will produce a basis for the of A . . Cosines and Projections onto Lines Vectors with x  y   are orthogonal.  Now we allow inner products that are not zero  and angles that are not right angles .  We want to connect inner products to angles and also to transposes. In Chapter  the transpose was constructed by flipping over a matrix as if it were some kind of pancake. We have to do better than that. One fact is unavoidable The orthogonal case is the most important .  Suppose we want to find the distance from a point b to the line in the direction of the vector a .  We are looking along that line for the point p closest to b . The key is in the geometry The line connecting b to p the dotted line in Figure . is perpendicular to a . This fact will allow us to find the projection p .  Even though a and b are not orthogonal the distance problem automatically brings in orthogonality. The situation is the same when we are given a plane or any subspace S  instead of a line. Again the problem is to find the point p on that subspace that is closest to b . This                          point p is the projection of b onto the subspace . A perpendicular line from b to S meets the subspace at p . Geometrically that gives the distance between points b and subspaces S . But there are two questions that need to be asked .  Does this projection actually arise in practical applications? .  If we have a basis for the subspace S  is there a formula for the projection p ? The answers are certainly yes. This is exactly the problem of the leastsquares solu tion to an overdetermined system .  The vector b represents the data from experiments or questionnaires and it contains too many errors to be found in the subspace S . When we try to write b as a combination of the basis vectors for S  it cannot be donethe equations are inconsistent and Ax  b has no solution. The leastsquares method selects p as the best choice to replace b .  There can be no doubt of the importance of this application.  In economics and statistics least squares enters regression  analysis .   In  geodesy  the  U.S.  mapping  survey  tackled  .  million equations in  unknowns. A formula for p is easy when the subspace is a line. We will project b onto a in several different ways and relate the projection p to inner products and angles. Projection onto a higher dimensional subspace is by far the most important case it corresponds to a least squares problem with several parameters and it is solved in Section ..  The formulas are even simpler when we produce an orthogonal basis for S . inner products and cosines We pick up the discussion of inner products and angles. You will soon see that it is not the angle but the cosine of the angle  that is directly related to inner products. We look back to trigonometry in the twodimensional case to find that relationship. Suppose the vectors a and b make angles  and  with the x axis Figure .. The length  a  is the hypotenuse in the triangle OaQ . So the sine and cosine of  are sin   a   a   cos   a   a  .                                                              F or the angle   the sine is b    b  and the cosine is b    b  . The cosine of      comes from an identity that no one could forget Cosine formula cos   cos  cos   sin  sin   a  b   a  b   a  b  .  The numerator  in  this  formula  is  exactly  the  inner  product  of a and b .   It  gives  the relationship between a  b and cos   G The cosine of the angle between any nonzero vectors a and b is Cosine of  cos   a  b  a  b  .  This formula is dimensionally correct if we double the length of b  then both numerator and denominator are doubled and the cosine is unchanged. Reversing the sign of b  on the other hand reverses the sign of cos  and changes the angle by   . There is another law of trigonometry that leads directly to the same result. It is not so unforgettable as the formula in equation  but it relates the lengths of the sides of any triangle Law of Cosines  b  a     b     a      b  a  cos  .  When  is a right angle we are back to Pythagoras  b  a     b     a   .  For any angle   the expression  b  a   is  b  a    b  a   and equation  becomes b  b   a  b  a  a  b  b  a  a    b  a  cos  . Canceling b  b and a  a on both sides of this equation you recognize formula  for the cosine a  b   a  b  cos  .  In fact this proves the cosine formula in n dimensions since we only have to worry about the plane triangle Oab . Projection onto a Line No w we want to find the projection point p . This point must be some multiple p   xa of the given vector a every point on the line is a multiple of a . The problem is to compute                                              the coefficient  x .  All we need is the geometrical fact that the line from b to the closest point p   xa is perpendicular to the vector a   b   a   a  or a   b   a     or  x  a  b a  a .  That gives the formula for the number  x and the projection p  H The projection of the vector b onto the line in the direction of a is p   xa  Projection onto a line p   xa  a  b a  a a .  This allows us to redraw Figure . with a correct formula for p Figure .. This leads to the Schwarz inequality in equation   which is the most important inequality in mathematics.  A special case is the fact that arithmetic means    x  y  are lar ger than geometric means  x y .  It is also equivalentsee Problem  at the end of this sectionto the triangle inequality for vectors.  The Schwarz inequality seems to come almost accidentally from the statement that  e     b  p   in Figure . cannot be negative     b  a  b a  a a       b  b    a  b   a  a   a  b a  a   a  a   b  b  a  a    a  b    a  a    . This tells us that  b  b  a  a    a  b   and then we take square roots I All vectors a and b satisfy the Schwarz inequality  which is  cos    in R   Schwarz inequality  a  b  a  b  .  According to formula  the ratio between a  b and  a  b  is exactly  cos   .  Since all cosines lie in the interval    cos    this gives another proof of equation  the Schwarz inequality is the same as  cos   .  In some ways that is a more easily understood proof because cosines are so familiar.  Either proof is all right in R   but   notice that ours came directly from the calculation of  b  p   .  This stays nonnegative when we introduce new possibilities for the lengths and inner products.  The name of Cauchy is also attached to this inequality  a  b  a  b   and the Russians refer to it as the CauchySchwarzBuniakowsky inequality!  Mathematical historians seem to agree that Buniakowskys claim is genuine. One  final  observation  about  a  b  a  b  . Equality  holds  if  and  only  if  b  is  a multiple of a .  The angle is     or     and the cosine is  or  .  In this case b is identical with its projection p  and the distance between b and the line is zero. Example . Project b         onto the line through a         to get  x and p   x  a  b a  a      . The projection is p   xa         . The angle between a and b has cos    p   b       and also cos   a  b  a  b        . The Schwarz inequality  a  b  a  b  is      . If we write  as   that is the same as     . The cosine is less than  because b is not parallel to a . Projection Matrix of Rank  The projection of b onto the line through a lies at p  a  a  b  a  a  . That is our formula p   xa   but  it  is  written  with  a  slight  twist  The  vector a is  put  before  the  number  x  a  b  a  a .  There is a reason behind that apparently trivial change.  Projection onto a line is carried out by a projection matrix P  and written in this new order we can see what it is. P is the matrix that multiplies b and produces p  P  a a  b a  a so the projection matrix is P  aa  a  a .  That is a column times a rowa square matrixdivided by the number a  a . Example . The matrix that projects onto the line through a         is P  aa  a  a                                             . This matrix has two properties that we will see as typical of projections . P is a symmetric matrix. .  Its square is itself P   P . P  b is the projection of Pb and Pb is already on the line! So P  b  Pb . This matrix P also gives a great example of the four fundamental subspaces  The column space consists of the line through a         . The nullspace consists of the plane perpendicular to a . The rank is r  . Every column is a multiple of a  and so is Pb   xa .  The vectors that project to p   are especially important.  They satisfy a  b  they are perpendicular to a and their component along the line is zero. They lie in the nullspace  perpendicular plane. Actually that example is too perfect.  It has the nullspace orthogonal to the column space  which is haywire.   The nullspace should be orthogonal to the row space .   But because P is symmetric its row and column spaces are the same. Remark on scaling The projection matrix aa   a  a is the same if a is doubled a           gives P                                             as before. The line through a is the same and thats all the projection matrix cares about. If a has unit length the denominator is a  a   and the matrix is just P  aa  . Example . Project onto the   direction in the x  y plane.   The line goes through a   cos   sin   and the matrix is symmetric with P   P  P  aa  a  a   c s   c s   c s   c s    c  cs cs   s   . Here c is cos   s is sin   and c   s    in the denominator.  This matrix P was dis covered in Section . on linear transformations.  Now we know P in any number of dimensions. We emphasize that it produces the projection p  T o project b onto a  multiply by the projection matrix P  p  Pb . T ransposes from Inner Products Finally we connect inner products to A  .  Up to now A  is simply the reflection of A across its main diagonal the rows of A become the columns of A   and vice versa. The entry in row i  column j of A  is the  j  i  entry of A  Transpose by reflection A     A   . There is a deeper significance to A   Its close connection to inner products gives a new and much more abstract definition of the transpose   J The transpose A  can be defined by the following property  The inner product of Ax with y equals the inner product of x with A  y .  Formally this simply means that  Ax   y  x  A  y  x   A  y  .  This definition gives us another better way to verify the formula  AB    B  A   Use equation  twice Move A then move B  ABx   y   Bx    A  Y   x   B  A  y  . The transposes turn up in reverse order on the right side just as the inverses do in the formula  AB     B   A   . We mention again that these two formulas meet to give the remarkable combination  A       A     . Pr oblem Set . . a  Given any two positive numbers x and y  choose the vector b equal to   x   y   and choose a    y   x  . Apply the Schwarz inequality to compare the arith metic mean    x  y  with the geometric mean  x y . b  Suppose we start with a vector from the origin to the point x   and then add a vector of length  y  connecting x to x  y .  The third side of the triangle goes from the origin to x  y . The triangle inequality asserts that this distance cannot be greater than the sum of the first two   x  y  x    y  . After  squaring  both  sides  and  expanding  x  y    x  y    reduce  this  to  the Schwarz inequality. . Verify that the length of the projection in Figure . is  p    b  cos   using formula . . What multiple of a         is closest to the point b         ? Find also the point closest to a on the line through b . . Explain why the Schwarz inequality becomes an equality in the case that a and b lie on the same line through the origin and only in that case.  What if they lie on opposite sides of the origin? . In n dimensions what angle does the vector     ...   make with the coordinate axes? What is the projection matrix P onto that vector? . The Schwarz inequality has a oneline proof if a and b are normalized ahead of time to be unit vectors  a  b      a  b       a   b     a      b             a  b  .  Which previous problem justifies the middle step? . By choosing the correct vector b in the Schwarz inequality prove that  a     a     n  a      a    . When does equality hold? . The methane molecule CH  is arranged as if the carbon atom were at the center of a regular tetrahedron with four hydrogen atoms at the vertices.  If vertices are placed at                         and        note that all six edges have length   so the tetrahedron is regularwhat is the cosine of the angle between the rays going from the center           to the vertices?  The bond angle itself is about .   an old friend of chemists. . Square the matrix P  aa   a  a  which projects onto a line and show that P   P . Note the number a  a in the middle of the matrix aa  aa  ! . Is the projection matrix P invertible? Why or why not? . a  Find the projection matrix P  onto the line through a      and also the matrix P  that projects onto the line perpendicular to a . b  Compute P   P  and P  P  and explain. . Find the matrix that projects every point in the plane onto the line x   y  . . Prove that the trace of P  aa   a  a which is the sum of its diagonal entries always equals . . What matrix P projects every point in R  onto the line of intersection of the planes x  y  t   and x  t  ? . Show that the length of Ax equals the length of A  x if AA   A  A . . Suppose P is the projection matrix onto the line through a . a  Why is the inner product of x with Py equal to the inner product of Px with y ? b  Are the two angles the same?  Find their cosines if a           x          y         . c  Why  is  the  inner  product  of Px with Py again  the  same?   What  is  the  angle between those two? Problems  ask for projections onto lines. Also errors e  b  p and matri ces P . . Project the vector b onto the line through a . Check that e is perpendicular to a    a b           and a           . b b           and a              . . Draw the projection of b onto a and also compute it from p   xa  a b   cos  sin   and a      . b b      and a       . . In Problem  find the projection matrix P  aa   a  a onto the line through each vector a . Verify in both cases that P   P . Multiply Pb in each case to compute the projection p . . Construct the projection matrices P  and P  onto the lines through the a s in Problem . Is it true that  P   P     P   P  ? This would be true if P  P   . For Problems  consult the accompanying figures. . Compute the projection matrices aa   a  a onto the lines through a           and a             Multiply those projection matrices and explain why their product P  P  is what it is. . Project b         onto the lines through a  and a  in Problem  and also onto a           . Add the three projections p   p   p  . . Continuing Problems  find the projection matrix P  onto a           . Ver ify that P   P   P   I . The basis a   a   a  is orthogonal! . Project the vector b       onto the lines through a        and a        . Draw the projections p  and p  and add p   p  . The projections do not add to b because the a s are not orthogonal. . In Problem  the projection of b onto the plane of a  and a  will equal b .  Find P  A  A  A    A  for A   a  a       .  . Project a        onto a        . Then project the result back onto a  . Draw these projections and multiply the projection matrices P  P   Is this a projection? . Projections and Least Squares Up to this point Ax  b either has a solution or not. If b is not in the column space   A   the system is inconsistent and Gaussian elimination fails.  This failure is almost certain when there are several equations and only one unknown More equations than unknowns no solution?  x  b   x  b   x  b  . This is solvable when b   b   b  are in the ratio . The solution x will exist only if b is on the same line as the column a         . In spite of their unsolvability  inconsistent equations arise all the time in practice. They have to be solved!  One possibility is to determine x from part of the system and ignore the rest  this is hard to justify if all m equations come from the same source. Rather than expecting no error in some equations and large errors in the others  it is much better to choose the x that minimizes an average error E in the m equations. The most convenient average comes from the sum of squares  Squared error E     x  b      x  b      x  b    . If there is an exact solution the minimum error is E  . In the more likely case that b is not proportional to a  the graph of E  will be a parabola. The minimum error is at the lowest point where the derivative is zero dE  d x      x  b      x  b      x  b       . Solving for x  the leastsquares solution of this model system ax  b is denoted by  x  Leastsquares solution   x   b    b    b           a  b a  a . Y ou recognize a  b in the numerator and a  a in the denominator . The general case is the same. We solve ax  b by minimizing E    ax  b     a  x  b       a  x  b    . The derivative of E  is zero at the point  x  if  a   x  b   a     a   x  b   a    . We are minimizing the distance from b to the line through a  and calculus gives the same answer  x   a  b     a  b     a      a     that geometry did earlier   K The leastsquares solution to a problem ax  b in one unknown is  x  a  b a  a . Y ou see that we keep coming back to the geometrical interpretation of a leastsquares problemto minimize a distance. By setting the derivative of E  to zero calculus con firms the geometry of the previous section. The error vector e connecting b to p must be perpendicular to a  Orthogonality of a and e a   b   xa   a  b  a  b a  a a  a   . As a side remark notice the degenerate case a  .  All multiples of a are zero and the line is only a point.  Therefore p   is the only candidate for the projection.  But the formula for  x becomes a meaningless    and correctly reflects the fact that  x is completely undetermined.  All values of x give the same error E    x  b   so E  is a horizontal line instead of a parabola.  The pseudoinverse assigns the definite value  x   which is a more symmetric choice than any other number. Least Squares Problems with Several Variables Now we are ready for the serious step to project b onto a subspace rather than just onto a line.   This problem arises from Ax  b when A is an m by n matrix.   Instead of one column and one unknown x  the matrix now has n columns.  The number m of observations is still larger than the number n of unknowns so it must be expected that Ax  b will be inconsistent. Probably there will not exist a choice of x that perfectly fits the data b .  In other words the vector b probably will not be a combination of the columns of A  it will be outside the column space. Again the problem is to choose  x so as to minimize the error and again this mini mization will be done in the leastsquares sense.  The error is E   Ax  b   and this is exactly the distance from b to the point Ax in the column space .  Searching for the leastsquares solution  x  which minimizes E  is the same as locating the point p  A  x that is closer to b than any other point in the column space. We may use geometry or calculus to determine  x .  In n dimensions  we prefer the appeal of geometry p must be the projection of b onto the column space. The error vector e  b  A  x must be perpendicular to that space Figure ..  Finding  x and the projection p  A  x is so fundamental that we do it in two ways .  All vectors perpendicular to the column space lie in the left nullspace .  Thus the error vector e  b  A  x must be in the nullspace of A   A   b  A  x    or A  A  x  A  b .     .  The error vector must be perpendicular to each column a  ... a  of A  a    b  A  x    . . . a    b  A  x    or    a   . . . a         b  A  x      . This is again A   b  A  x    and A  A  x  A  b  The calculus way is to take partial derivatives of E    Ax  b    Ax  b  .  That gives the same  A  Ax   A  b  .  The fastest way is just to multiply the unsolvable equation Ax  b by A  . All these equivalent methods produce a square coefficient matrix A  A .  It is symmetric its transpose is not AA  ! and it is the fundamental matrix of this chapter. The equations A  A  x  A  b are known in statistics as the normal equations . L When Ax  b is inconsistent its leastsquares solution minimizes  Ax  b    Normal equations A  A  x  A  b .  A  A is  invertible  exactly  when  the  columns  of A are  linearly  independent! Then Best estimate  x  x   A  A    A  b .  The projection of b onto the column space is the nearest point A  x  Projection p  A  x  A  A  A    A  b .  We choose an example in which our intuition is as good as the formulas A                     b            Ax  b has no solution A  A  x  A  b gives the best x .   Both columns end with a zero so   A  is the x  y plane within threedimensional space The projection of b         is p         the x and y components stay the same but z   will disappear. That is confirmed by solving the normal equations A  A                                             .  x   A  A    A  b                                       . Projection p  A  x                                  . In this special case the best we can do is to solve the first two equations of Ax  b . Then  x    and  x   . The error in the equation  x    x    is sure to be . Remark  . Suppose b is actually in the column space of A it is a combination b  Ax of the columns. Then the projection of b is still b  b in column space p  A  A  A    A  Ax  Ax  b . The closest point p is just b itselfwhich is obvious. Remark  . At the other extreme suppose b is perpendicular to every column so A  b  . In this case b projects to the zero vector b in left nullspace p  A  A  A    A  b  A  A  A       . Remark  . When A is square and invertible the column space is the whole space. Every vector projects to itself p equals b  and  x  x  If A is invertible p  A  A  A    A  b  AA    A     A  b  b . This is the only case when we can take apart  A  A     and write it as A    A     . When A is rectangular that is not possible. Remark  . Suppose A has only one column containing a .  Then the matrix A  A is the number a  a and  x is a  b  a  a . We return to the earlier formula. The CrossProduct Matrix A  A The matrix A  A is certainly symmetric. Its transpose is  A  A    A  A   which is A  A again.  Its i  j entry and j  i entry is the inner product of column i of A with column j of A . The key question is the invertibility of A  A  and fortunately  A  A has the same nullspace as A . Certainly if Ax    then A  Ax  .   Vectors x in  the  nullspace  of A are  also  in  the nullspace of A  A .  To go in the other direction start by supposing that A  Ax   and take the inner product with x to show that Ax   x  A  Ax    or  Ax      or Ax   . The two nullspaces are identical. In particular if A has independent columns and only x   is in its nullspace then the same is true for A  A  M If A has independent columns then A  A is square symmetric  and invert ible . We show later that A  A is also positive definite all pivots and eigenvalues are positive. This case is by far the most common and most important.  Independence is not so hard in m dimensional space if m  n . We assume it in what follows. Projection Matrices We have shown that the closest point to b is p  A  A  A    A  b . This formula expresses in matrix terms the construction of a perpendicular line from b to the column space of A . The matrix that gives p is a projection matrix denoted by P  Projection matrix P  A  A  A    A  .  This matrix projects any vector b onto the column space of A .  In other words p  Pb is the component of b in the column space and the error e  b  Pb is the component in the orthogonal complement.   I  P is also a projection matrix!  It projects b onto the orthogonal complement and the projection is b  Pb . In short we have a matrix formula for splitting any b into two perpendicular compo nents. Pb is in the column space   A   and the other component  I  P  b is in the left nullspace   A   which is orthogonal to the column space. These projection matrices can be understood geometrically and algebraically. N The projection matrix P  A  A  A    A  has two basic properties i  It equals its square P   P . ii  It equals its transpose P   P . Conversely any symmetric matrix with P   P represents a projection.         Proof. It is easy to see why P   P . If we start with any b  then Pb lies in the subspace we are projecting onto. When we project again nothing is changed .  The vector Pb is already in the subspace and P  Pb  is still Pb .  In other words P   P .  Two or three or fifty projections give the same point p as the first projection P   A  A  A    A  A  A  A    A   A  A  A    A   P . To  prove  that P is  also  symmetric  take  its  transpose.   Multiply  the  transposes  in reverse order and use symmetry of  A  A     to come back to P  P    A      A  A      A   A  A  A    A   P . For the converse we have to deduce from P   P and P   P that Pb is the projection of b onto the column space of P .  The error vector b  Pb is orthogonal to the space . For any vector Pc in the space the inner product is zero  b  Pb   Pc  b   I  P   Pc  b   P  P   c   . Thus b  Pb is orthogonal to the space and Pb is the projection onto the column space. Example . Suppose A is actually invertible.  If it is  by  then its four columns are independent and its column space is all of R  .  What is the projection onto the whole space ? It is the identity matrix. P  A  A  A    A   AA    A     A   I .  The identity matrix is symmetric I   I  and the error b  Ib is zero. The point of all other examples is that what happened in equation  is not allowed . To repeat We cannot invert the separate parts A  and A when those matrices are rectan gular. It is the square matrix A  A that is invertible. LeastSquares Fitting of Data Suppose we do a series of experiments and expect the output b to be a linear function of the input t . We look for a straight line b  C  Dt . For example .  At different times we measure the distance to a satellite on its way to Mars. In this case t is the time and b is the distance.  Unless the motor was left on or gravity is strong the satellite should move with nearly constant velocity v  b  b   vt . .  We vary the load on a structure and measure the movement it produces.  In this experiment t is the load and b is the reading from the strain gauge. Unless the load is so great that the material becomes plastic a linear relation b  C  Dt is normal in the theory of elasticity.  .  The cost of producing t books like this one is nearly linear b  C  Dt  with editing and typesetting in C and then printing and binding in D . C is the setup cost and D is the cost for each additional book. How to compute C and D ? If there is no experimental error then two measurements of b will determine the line b  C  Dt .  But if there is error we must be prepared to average the experiments and find an optimal line. That line is not to be confused with the line through a on which b was projected in the previous section! In fact since there are two unknowns C and D to be determined we now project onto a twodimensional subspace. A perfect experiment would give a perfect C and D  C  Dt   b  C  Dt   b  . . . C  Dt   b  .  This is an overdetermined system with m equations and only two unknowns.  If errors are present it will have no solution. A has two columns and x   C  D         t   t  . . . . . .  t        C D        b  b  . . . b        or Ax  b .  The best solution   C   D  is the  x that minimizes the squared error E   Minimize E    b  Ax     b   C  Dt       b   C  Dt    . The vector p  A  x is as close as possible to b .  Of all straight lines b  C  Dt  we are choosing the one that best fits the data Figure ..  On the graph the errors are the vertical distances b  C  Dt to the straight line not perpendicular distances!. It is the vertical distances that are squared summed and minimized. Example . Three measurements b   b   b  are marked on Figure .a b      at t     b      at t    b      at t   . Note that the values t        are not required to be equally spaced.  The first step is to write the equations that would hold if a line could go through all three points. Then every C  Dt would agree exactly with b  Ax  b is C  D   C  D   C   D   or               C D            .          If those equations Ax  b could be solved there would be no errors. They cant be solved because the points are not on a line. Therefore they are solved by least squares A  A  x  A  b is            C  D       . The best solution is  C      D    and the best line is      t . Note the beautiful connections between the two figures. The problem is the same but the art shows it differently. In Figure .b b is not a combination of the columns        and         .  In Figure . the three points are not on a line.  Least squares replaces points b that are not on a line by points p that are!  Unable to solve Ax  b  we solve A  x  p . The line      t has heights         at the measurement times    . Those points do lie on a line . Therefore the vector p            is in the column space. This vector is the projection .  Figure .b is in three dimensions or m dimensions if there are m points and Figure .a is in two dimensions or n dimensions if there are n parameters. Subtracting p from b  the errors are e             . Those are the vertical errors in Figure .a and they are the components of the dashed vector in Figure .b. This error vector is orthogonal to the first column         since           . It is orthogonal to the second column          because           . It is orthogonal to the column space and it is in the left nullspace. Question   If the measurements b             were those errors what would be the best line and the best  x ?   Answer  The zero linewhich is the horizontal axisand  x  . Projection to zero. We can quickly summarize the equations for fitting by a straight line. The first column of A contains s and the second column contains the times t  .  Therefore A  A contains the sum of the s and the t  and the t     O The measurements b  ... b  are given at distinct points t  ... t  . Then the straight line  C   Dt which minimizes E  comes from least squares A  A   C  D   A  b or  m  t   t   t     C  D     b   t  b   . Remark . The mathematics of least squares is not limited to fitting the data by straight lines.   In  many  experiments  there  is  no  reason  to  expect  a  linear  relationship  and  it would be crazy to look for one. Suppose we are handed some radioactive material The output b will be the reading on a Geiger counter at various times t .  We may know that we are holding a mixture of two chemicals and we may know their halflives or rates of decay but we do not know how much of each is in our hands. If these two unknown amounts are C and D  then the Geiger counter readings would behave like the sum of two exponentials and not like a straight line b  Ce     De    .  In practice the Geiger counter is not exact.  Instead we make readings b  ... b  at times t  ... t   and equation  is approximately satisfied Ax  b is Ce      De      b  . . . Ce      De      b  . If there are more than two readings m   then in all likelihood we cannot solve for C and D . But the leastsquares principle will give optimal values  C and  D . The situation would be completely different if we knew the amounts C and D  and were trying to discover the decay rates  and  .  This is a problem in nonlinear least squares  and it is harder.  We would still form E   the sum of the squares of the errors and minimize it. But setting its derivatives to zero will not give linear equations for the optimal  and  . In the exercises we stay with linear least squares. Weighted Least Squares A simple leastsquares problem is the estimate  x of a patients weight from two obser vations x  b  and x  b  . Unless b   b   we are faced with an inconsistent system of two equations in one unknown      x    b  b   . Up to now we accepted b  and b  as equally reliable.  We looked for the value  x that minimized E    x  b     x  b     dE  d x   at  x  b   b   .   The optimal  x is the average.  The same conclusion comes from A  A  x  A  b .  In fact A  A is a  by  matrix and the normal equation is   x  b   b  . Now suppose the two observations are not trusted to the same degree .   The value x  b  may be obtained from a more accurate scaleor in a statistical problem from a larger samplethan x  b  .  Nevertheless if b  contains some information we are not willing to rely totally on b  . The simplest compromise is to attach different weights w   and w    and choose the  x  that minimizes the weighted sum of squares  Weighted error E   w    x  b     w    x  b    . If w   w   more importance is attached to b  . The minimizing process derivative   tries harder to make  x  b    small dE  d x    w    x  b   w    x  b      at  x   w   b   w   b  w    w   .  Instead of the average of b  and b  for w   w     x  is a weighted average of the data. This average is closer to b  than to b  . The ordinary leastsquares problem leading to  x  comes from changing Ax  b to the new system WAx  W b . This changes the solution from  x to  x  . The matrix W  W turns up on both sides of the weighted normal equations The least squares solution to WAx  W b is  x   Weighted normal equations  A  W  WA   x   A  W  W b . What happens to the picture of b projected to A  x ?  The projection A  x  is still the point in the column space that is closest to b . But the word closest has a new meaning when the length involves W . The weighted length of x equals the ordinary length of W x . Perpendicularity no longer means y  x   in the new system the test is  Wy    W x   . The matrix W  W appears in the middle.  In this new sense the projection A  x  and the error b  A  x  are again perpendicular. That last paragraph describes all inner products  They come from invertible matrices W .  They involve only the symmetric combination C  W  W . The inner product of x and y is y  Cx . For an orthogonal matrix W  Q  when this combination is C  Q  Q  I  the inner product is not new or different.  Rotating the space leaves the inner product unchanged. Every other W changes the length and inner product. For any invertible matrix W  these rules define a new inner product and length  Weighted by W  x  y     Wy    W x  and  x     W x  .  Since W is invertible no vector is assigned length zero except the zero vector.  All possible inner productswhich depend linearly on x and y and are positive when x  y   are found in this way from some matrix C  W  W .  In practice the important question is the choice of C .  The best answer comes from statisticians and originally from Gauss.  We may know that the average error is zero. That is the expected value of the error in b although the error is not really expected to be zero! We may also know the average of the square of the error that is the variance . If the errors in the b  are independent of each other and their variances are     then the right weights are w       .  A more accurate measurement  which means a smaller variance gets a heavier weight. In addition to unequal reliability the observations may not be independent .  If the errors are coupledthe polls for President are not independent of those for Senator and certainly not of those for VicePresidentthen W has offdiagonal terms. The best unbiased matrix C  W  W is the inverse of the covariance matrix whose i  j entry is the expected value of error in b   times error in b  .  Then the main diagonal of C   contains the variances     which are the average of error in b    . Example . Suppose two bridge partners both guess after the bidding the total num ber of spades they hold. For each guess the errors     might have equal probability   . Then the expected error is zero and the variance is    E  e                       E  e                            . The two guesses are dependent because they are based on the same biddingbut not identical  because  they  are  looking  at  different  hands.   Say  the  chance  that  they  are both  too  high  or  both  too  low  is  zero  but  the  chance  of  opposite  errors  is   . Then E  e  e           and the inverse of the covariance matrix is W  W   E  e    E  e  e   E  e  e   E  e                                C  W  W . This matrix goes into the middle of the weighted normal equations. Pr oblem Set . . Find the best leastsquares solution  x to  x    x  . What error E  is minimized? Check that the error vector      x      x  is perpendicular to the column      . . Suppose the values b    and b    at times t    and t    are fitted by a line b  Dt through the origin . Solve D   and  D   by least squares and sketch the best line.   . Solve Ax  b by least squares and find p  A  x if A                     b           . Verify that the error b  p is perpendicular to the columns of A . . Write out E    Ax  b   and set to zero its derivatives with respect to u and v  if A                     x   u v   b           . Compare the resulting equations with A  A  x  A  b  confirming that calculus as well as geometry gives the normal equations.  Find the solution  x and the projection p  A  x . Why is p  b ? . The following system has no solution Ax                C D             b . Sketch and solve a straightline fit that leads to the minimization of the quadratic  C  D      C      C  D     ? What is the projection of b onto the column space of A ? . Find the projection of b onto the column space of A  A                 b           . Split b into p  q   with p in the column space and q perpendicular to that space. Which of the four subspaces contains q ? . Find  the  projection  matrix P onto  the  space  spanned  by a          and a           . . If P is the projection matrix onto a k dimensional subspace S of the whole space R   what is the column space of P and what is its rank? . a  If P  P  P  show that P is a projection matrix. b  What subspace does the matrix P   project onto? . If the vectors a   a    and b are orthogonal  what are A  A and A  b ?   What is the projection of b onto the plane of a  and a  ?  . Suppose P is the projection matrix onto the subspace S and Q is the projection onto the orthogonal complement S  . What are P  Q and PQ ? Show that P  Q is its own inverse. . If V is the subspace spanned by          and           find a  a basis for the orthogonal complement V  . b  the projection matrix P onto V . c  the vector in V closest to the vector b            in V  . . Find the best straightline fit least squares to the measurements b      at t    b      at t    b      at t   b      at t  . Then find the projection of b           onto the column space of A                      . . The vectors a          and a          span a plane in R  .  Find the projection matrix P onto the plane and find a nonzero vector b that is projected to zero. . If P is the projection matrix onto a line in the x  y plane draw a figure to describe the effect of the reflection matrix H  I   P .   Explain both geometrically and algebraically why H   I . . Show that if u has unit length then the rank matrix P  uu  is a projection matrix It has properties i and ii in N. By choosing u  a   a   P becomes the projection onto the line through a  and Pb is the point p   xa .  Rank projections correspond exactly to leastsquares problems in one unknown. . What  by  matrix projects the x  y plane onto the    line x  y  ? . We want to fit a plane y  C  Dt  Ez to the four points y      at t    z   y      at t    z   y      at t    z   y      at t    z  . a  Find  equations in  unknowns to pass a plane through the points if there is such a plane. b  Find  equations in  unknowns for the best leastsquares solution. . If P   A  A  A    A  is the projection onto the column space of A  what is the pro jection P  onto the row space? It is not P   !   . If P is the projection onto the column space of A  what is the projection onto the left nullspace? . Suppose L  is the line through the origin in the direction of a  and L  is the line through b in the direction of a  .  To find the closest points x  a  and b  x  a  on the two lines write the two equations for the x  and x  that minimize  x  a   x  a   b  . Solve for x if a           a           b         . . Find the best line C  Dt to fit b            at times t            . . Show that the best leastsquares fit to a set of measurements y  ... y  by a horizontal line a constant function y  C  is their average C  y     y  m . . Find the best straightline fit to the following measurements and sketch your solu tion y   at t    y   at t   y       at t   y       at t  . . Suppose that instead of a straight line we fit the data in Problem  by a parabola y  C  Dt  Et  .  In the inconsistent system Ax  b that comes from the four mea surements  what are the coefficient matrix A   the unknown vector x   and the data vector b ? You need not compute  x . . A MiddleAged man was stretched on a rack to lengths L    and  feet under applied forces of F    and  tons.  Assuming Hookes law L  a  bF  find his normal length a by least squares. Problems    introduce  basic  ideas  of  statisticsthe  foundation  for  least squares. . Recommended This problem projects b   b  ... b   onto the line through a    ...   . We solve m equations ax  b in  unknown by least squares. a  Solve a  a  x  a  b to show that is the mean the average of the b s b  Find e  b  a  x  the variance  e    and the standard deviation  e  . c  The horizontal line  b   is closest to b          Check that p         is perpendicular to e and find the projection matrix P . . First assumption behind least squares Each measurement error has mean zero . Mul tiply the  error vectors b  Ax            by  A  A    A  to show that the  vectors  x  x also average to zero. The estimate  x is unbiased .  . Second  assumption  behind  least  squares   The m errors e  are  independent  with variance     so the average of  b  Ax  b  Ax   is   I .   Multiply on the left by  A  A    A  and on the right by A  A  A    to show that the average of   x  x   x  x   is    A  A    . This is the allimportant covariance matrix for the error in  x . . A doctor takes four readings of your heart rate. The best solution to x  b  ... x  b  is the average  x of b  ... b  . The matrix A is a column of s. Problem  gives the expected error   x  x   as    A  A     . By averaging the variance drops from   to    . . If you know the average  x  of  numbers b  ... b   how can you quickly find the average  x  with one more number b  ?  The idea of recursive least squares is to avoid adding  numbers. What coefficient of  x  correctly gives  x  ?  x      b    x      b      b   . Problems  use four points b           to bring out more ideas. . With b         at t         set up and solve the normal equations A  A  x  A  b . For the best straight line as in Figure .a find its four heights p  and four errors e  . What is the minimum value E   e    e    e    e   ? . Line C  Dt does go through p s With b         at times t         write the four equations Ax  b unsolvable. Change the measurements to p         and find an exact solution to A  x  p . . Check that e  b  p             is perpendicular to both columns of A .  What is the shortest distance  e  from b to the column space of A ? . For the closest parabola b  C  Dt  Et  to the same four points write the unsolv able equations Ax  b in three unknowns x   C  D  E  .   Set up the three normal equations A  A  x  A  b solution not required. You are now fitting a parabola to four pointswhat is happening in Figure .b? . For the closest cubic b  C  Dt  Et   Ft  to the same four points write the four equations Ax  b .  Solve them by elimination This cubic now goes exactly through the points. What are p and e ? . The average of the four times is  t              . The average of the four b s is  b              . a  Verify that the best line goes through the center point   t   b        . b  Explain why C  D  t   b comes from the first equation in A  A  x  A  b . . What happens to the weighted average  x    w   b   w   b     w    w    if the first weight w  approaches zero? The measurement b  is totally unreliable.   . From m independent measurements b  ... b  of your pulse rate weighted by w  ... w   what is the weighted average that replaces equation ? It is the best estimate when the statistical variances are       w   . . If W        find the W inner product of x       and y        and the W length of x . What line of vectors is W perpendicular to y ? . Find the weighted leastsquares solution  x  to Ax  b  A                    b           W                             . Check that the projection A  x  is still perpendicular in the W inner product! to the error b  A  x  . . a  Suppose you guess your professors age making errors e         with prob abilities         . Check that the expected error E  e  is zero and find the variance E  e   . b  If the professor guesses too or tries to remember making errors     with probabilities          what weights w  and w  give the reliability of your guess and the professors guess? . Orthogonal Bases and GramSchmidt In an orthogonal basis every vector is perpendicular to every other vector.  The coor dinate axes are mutually orthogonal.  That is just about optimal and the one possible improvement is easy  Divide each vector by its length to make it a unit vector .  That changes an orthogonal basis into an orthonormal basis of q s P The vectors q  ... q  are orthonormal if q   q        whenever i   j  giving the orthogonality     whenever i  j  giving the normalization. A matrix with orthonormal columns will be called Q . The most important example is the standard basis . For the x  y plane the bestknown axes e        and e        are not only perpendicular but horizontal and vertical. Q is the  by  identity matrix. In n dimensions the standard basis e  ... e  again consists  of the columns of  Q  I  Standard basis e              . . .           e              . . .             e              . . .          . That is not the only orthonormal basis!  We can rotate the axes without changing the right angles at which they meet. These rotation matrices will be examples of Q . If we have a subspace of R   the standard vectors e  might not lie in that subspace. But the subspace always has an orthonormal basis and it can be constructed in a simple way out of any basis whatsoever. This construction which converts a skewed set of axes into a perpendicular set is known as GramSchmidt orthogonalization . To summarize the three topics basic to this section are .  The definition and properties of orthogonal matrices Q . .  The solution of Qx  b  either n by n or rectangular least squares. .  The GramSchmidt process and its interpretation as a new factorization A  QR . Orthogonal Matrices Q If Q square or rectangular has orthonormal columns then Q  Q  I  Orthonormal columns       q     q    . . .  q                 q  q   q                                            I .  An orthogonal matrix is a square matrix with orthonormal columns.  Then Q  is Q   . For square orthogonal matrices the transpose is the inverse . When row i of Q  multiplies column j of Q  the result is q   q   .  On the diagonal where i  j  we have q   q   . That is the normalization to unit vectors of length . Note that Q  Q  I even if Q is rectangular. But then Q  is only a leftinverse. Example . Q   cos   sin  sin  cos    Q   Q     cos  sin   sin  cos   .          Q rotates every vector through the angle   and Q  rotates it back through   .  The columns are clearly orthogonal and they are orthonormal because sin    cos    . The matrix Q  is just as much an orthogonal matrix as Q . Example . Any permutation matrix P is an orthogonal matrix.  The columns are cer tainly unit vectors and certainly orthogonalbecause the  appears in a different place in each column The transpose is the inverse. If P                             then P    P                              . An antidiagonal P  with P   P   P   I  takes the x  y  z axes into the z  y  x axes a righthanded system into a lefthanded system. So we were wrong if we suggested that every orthogonal Q represents a rotation. A reflection is also allowed . P      reflects every point  x  y  into  y  x   its mirror image across the   line. Geometrically an orthogonal Q is the product of a rotation and a reflection. There does remain one property that is shared by rotations and reflections and in fact by every orthogonal matrix. It is not shared by projections which are not orthogonal or even invertible.  Projections reduce the length of a vector whereas orthogonal matrices have a property that is the most important and most characteristic of all R Multiplication by any Q preserves lengths Lengths unchanged  Qx    x  for every vector x .  It also preserves inner products and angles since  Qx    Qy   x  Q  Qy  x  y . The preservation of lengths comes directly from Q  Q  I   Qx     x   because  Qx    Qx   x  Q  Qx  x  x .  All inner products and lengths are preserved when the space is rotated or reflected. We come now to the calculation that uses the special property Q   Q   . If we have a basis then any vector is a combination of the basis vectors. This is exceptionally simple for an orthonormal basis which will be a key idea behind Fourier series.  The problem is to find the coefficients of the basis vectors  Write b as a combination b  x  q   x  q     x  q  . T o compute x  there is a neat trick. Multiply both sides of the equation by q   .  On the lefthand side is q   b .   On the righthand side all terms disappear because q   q    except the first term. We are left with q   b  x  q   q  .  Since q   q    we have found x   q   b .  Similarly the second coefficient is x   q   b  that term survives when we multiply by q   . The other terms die of orthogonality. Each piece of b has a simple formula and recombining the pieces gives back b  Every vector b is equal to  q   b  q   q   b  q     q   b  q  .  I cant resist putting this orthonormal basis into a square matrix Q . The vector equa tion x  q     x  q   b is identical to Qx  b . The columns of Q multiply the compo nents of x . Its solution is x  Q   b . But since Q    Q  this is where orthonormality entersthe solution is also x  Q  b  x  Q  b      q    . . .  q          b        q   b . . . q   b     The components of x are the inner products q   b  as in equation . The matrix form also shows what happens when the columns are not orthonormal. Expressing b as a combination x  a     x  a  is the same as solving Ax  b . The basis vectors go into the columns of A .  In that case we need A    which takes work.  In the orthonormal case we only need Q  . Remark  . The ratio a  b  a  a appeared earlier when we projected b onto a line. Here a is q   the denominator is  and the projection is  q   b  q  . Thus we have a new interpre tation for formula  Every vector b is the sum of its onedimensional projections onto the lines through the qs . Since those projections are orthogonal Pythagoras should still be correct. The square of the hypotenuse should still be the sum of squares of the components  b     q   b    q   b      q   b   which is  Q  b   .  Remark  . Since Q   Q    we also have QQ   I .  When Q comes before Q   mul tiplication takes the inner products of the rows of Q .  For Q  Q it was the columns. Since the result is again the identity matrix we come to a surprising conclusion The rows of a square matrix are orthonormal whenever the columns are .  The rows point in completely different directions from the columns and I dont see geometrically why they are forced to be orthonormalbut they are. Orthonormal columns Orthonormal rows Q                                           . Rectangular Matrices with Orthogonal Columns This chapter is about Ax  b  when A is not necessarily square.  For Qx  b we now admit the same possibilitythere may be more rows than columns. The n orthonormal   vectors q  in the columns of Q have m  n components. Then Q is an m by n matrix and we cannot expect to solve Qx  b exactly. We solve it by least squares . If  there  is  any  justice  orthonormal  columns  should  make  the  problem  simple.   It worked for square matrices and now it will work for rectangular matrices. The key is to notice that we still have Q  Q  I . So Q  is still the leftinverse of Q . For least squares that is all we need.  The normal equations came from multiplying Ax  b by the transpose matrix to give A  A  x  A  b .  Now the normal equations are Q  Q  Q  b .  But Q  Q is the identity matrix!  Therefore  x  Q  b  whether Q is square and  x is an exact solution or Q is rectangular and we need least squares. S If Q has orthonormal columns the leastsquares problem becomes easy rectangular system with no solution for most b. Qx  b Q  Q  x  Q  b  x  Q  b p  Q  x p  QQ  b rectangular system with no solution for most b . normal equation for the best  x in which Q  Q  I .  x  is q   b . the projection of b is  q   b  q     q   b  q  . the projection matrix is P  QQ  . The last formulas are like p  A  x and P  A  A  A    A  . When the columns are orthonor mal the crossproduct matrix A  A becomes Q  Q  I .  The hard part of least squares disappears when vectors are orthonormal. The projections onto the axes are uncoupled and p is the sum p   q   b  q     q   b  q  . We emphasize that those projections do not reconstruct b .  In the square case m  n  they did.   In the rectangular case m  n   they dont.   They give the projection p and not the original vector b which is all we can expect when there are more equations than  unknowns  and  the q s  are  no  longer  a  basis.   The  projection  matrix  is  usually A  A  A    A   and here it simplifies to P  Q  Q  Q    Q  or P  QQ  .  Notice that Q  Q is the n by n identity matrix whereas QQ  is an m by m projection P . It is the identity matrix on the columns of Q  P leaves them alone But QQ  is the zero matrix on the orthogonal complement the nullspace of Q  . Example  . The  following  case  is  simple  but  typical.   Suppose  we  project  a  point b   x  y  z  onto the x  y plane.  Its projection is p   x  y     and this is the sum of the separate projections onto the x  and y axes q            and  q   b  q      x       q            and  q   b  q       y     .  The overall projection matrix is P  q  q    q  q                                and P    x y z        x y     . Projection onto a plane  sum of projections onto orthonormal q  and q  . Example . When the measurement times average to zero fitting a straight line leads to orthogonal columns. Take t     t    and t   . Then the attempt to fit y  C  Dt leads to three equations in two unknowns C  Dt   y  C  Dt   y  C  Dt   y   or               C D      y  y  y     . The columns        and         are orthogonal .  We can project y separately onto each column and the best coefficients  C and  D can be found separately  C           y  y  y              D          y  y  y               . Notice that  C   y   y   y      is  the mean of  the  data.  C gives  the  best  fit  by  a horizontal  line  whereas  Dt is  the  best  fit  by  a  straight  line  through  the  origin. The columns are orthogonal so the sum of these two separate pieces is the best fit by any straight line whatsoever .  The columns are not unit vectors so  C and  D have the length squared in the denominator. Orthogonal columns are so much better that it is worth changing to that case.  if the average of the observation times is not zeroit is   t   t     t    m then the time origin can be shifted by   t . Instead of y  C  Dt we work with y  c  d  t    t  . The best line is the same! As in the example we find  c       y   y                y     y  m  d    t     t     t     t   y   y     t     t       t     t      t     t  y    t     t   .  The best  c is the mean and we also get a convenient formula for  d .  The earlier A  A had the offdiagonal entries  t   and shifting the time by   t made these entries zero. This shift is an example of the GramSchmidt process which orthogonalizes the situation in advance . Orthogonal matrices are crucial to numerical linear algebra because they introduce no instability.  While lengths stay the same roundoff is under control.  Orthogonalizing vectors has become an essential technique.  Probably it comes second only to elimina tion. And it leads to a factorization A  QR that is nearly as famous as A  LU .   The GramSchmidt Process Suppose you are given three independent vectors a  b  c . If they are orthonormal life is easy.  To project a vector v onto the first one you compute  a  v  a .  To project the same vector v onto the plane of the first two you just add  a  v  a   b  v  b .  To project onto the span of a  b  c  you add three projections.  All calculations require only the inner products a  v  b  v  and c  v .  But to make this true we are forced to say  If they are orthonormal. Now we propose to find a way to make them orthonormal. The method is simple.   We are given a  b  c and we want q   q   q  .   There is no problem with q    it can go in the direction of a .  We divide by the length so that q   a   a  is a unit vector.  The real problem begins with q  which has to be orthogonal to q  .  If the second vector b has any component in the direction of q  which is the direction of a  that component has to be subtracted  Second vector B  b   q   b  q  and q   B   B  .  B is orthogonal to q  . It is the part of b that goes in a new direction and not in the a . In Figure . B is perpendicular to q  . It sets the direction for q  .                           At this point q  and q  are set.  The third orthogonal direction starts with c .  It will not be in the plane of q  and q   which is the plane of a and b .  However it may have a component in that plane and that has to be subtracted. If the result is C   this signals that a  b  c were not independent in the first place What is left is the component C we want the part that is in a new direction perpendicular to the plane Third vector C  c   q   c  q    q   c  q  and q   C   C  .  This is the one idea of the whole GramSchmidt process to subtract from every new vector its components in the directions that are already settled . That idea is used over and over again.  When there is a fourth vector we subtract away its components in the directions of q   q   q  .     Example . GramSchmidt Suppose the independent vectors are a  b  c  a            b            c           . To find q   make the first vector into a unit vector q   a   . To find q   subtract from the second vector its component in the first direction B  b   q   b  q                                            . The normalized q  is B divided by its length to produce a unit vector q                   . T o find q   subtract from c its components along q  and q   C  c   q   c  q    q   c  q                                                           . This is already a unit vector so it is q  . I went to desperate lengths to cut down the num ber of square roots the painful part of GramSchmidt. The result is a set of orthonormal vectors q   q   q   which go into the columns of an orthogonal matrix Q  Orthonormal basis Q     q  q  q                                  . T The GramSchmidt process starts with independent vectors a  ... a  and ends with  orthonormal vectors q  ... q  .   At  step j it subtracts  from a  its components in the directions q  ... q    that are already settled A   a    q   a   q    q     a   q    .  Then q  is the unit vector A    A   . Remark  on  the  calculations I  think  it  is  easier  to  compute  the  orthogonal a  B  C  without forcing their lengths to equal one. Then square roots enter only at the end when   dividing by those lengths.  The example above would have the same B and C  without using square roots. Notice the   from a  b  a  a instead of    from q  b  B                       and then C                                   . The Factorization A  QR We started with a matrix A  whose columns were a  b  c .  We ended with a matrix Q  whose columns are q   q   q  . What is the relation between those matrices? The matrices A and Q are m by n when the n vectors are in m dimensional space and there has to be a third matrix that connects them. The idea is to write the a s as combinations of the q s. The vector b in Figure . is a combination of the orthonormal q  and q   and we know what combination it is b   q   b  q   q   b  q  . Every vector in the plane is the sum of its q  and q  components. Similarly c is the sum of its q   q   q  components c   q   c  q    q   c  q    q   c  q  .  If we express that in matrix form we have the new factorization A  QR  QR factors A     a   b   c        q  q  q        q   a   q   b   q   c q   b   q   c q   c     QR  Notice the zeros in the last matrix! R is upper triangular because of the way Gram Schmidt was done. The first vectors a and q  fell on the same line. Then q   q  were in the same plane as a  b . The third vectors c and q  were not involved until step . The QR factorization is like A  LU  except that the first factor Q has orthonormal columns.  The second factor is called R  because the nonzeros are to the right of the di agonal and the letter U is already taken. The offdiagonal entries of R are the numbers q   b      and q   c  q   c    found above. The whole factorization is A                                                                                QR . Y ou see the lengths of a B C on the diagonal of R . The orthonormal vectors q   q   q   which are the whole object of orthogonalization are in the first factor Q . Maybe QR is not as beautiful as LU because of the square roots.  Both factoriza tions are vitally important to the theory of linear algebra and absolutely central to the calculations. If LU is Hertz then QR is Avis.  The entries r   q   a  appear in formula  when  A   q  is substituted for A   a    q   a   q     q     a   q      A   q   Q times column j of R .  U Every m by n matrix  with  independent  columns  can  be  factored  into A  QR .  The columns of Q are orthonormal and R is upper triangular and invertible. When m  n and all matrices are square Q becomes an orthogonal matrix. I must not forget the main point of orthogonalization.  It simplifies the leastsquares problem Ax  b . The normal equations are still correct but A  A becomes easier A  A  R  Q  QR  R  R .  The fundamental equation A  A  x  A  b simplifies to a triangular system R  R  x  R  Q  b or R  x  Q  b .  Instead  of  solving QRx  b   which  cant  be  done  we  solve R  x  Q  b which  is  just backsubstitution because R is triangular.  The real cost is the mn  operations of Gram Schmidt which are needed to find Q and R in the first place. The same idea of orthogonality applies to functions The sines and cosines are or thogonal the powers  x  x  are not. When f  x  is written as a combination of sines and cosines that is a Fourier series . Each term is a projection onto a linethe line in func tion space containing multiples of cos nx or sin nx . It is completely parallel to the vector case and very important.  And finally we have a job for Schmidt To orthogonalize the powers of x and produce the Legendre polynomials. Function Spaces and Fourier Series This is a brief and optional section but it has a number of good intentions .  to introduce the most famous infinitedimensional vector space  Hilbert space  .  to extend the ideas of length and inner product from vectors v to functions f  x   .  to recognize the Fourier series as a sum of onedimensional projections the orthog onal columns are the sines and cosines .  to apply GramSchmidt orthogonalization to the polynomials   x  x  ...  and .  to find the best approximation to f  x  by a straight line. We will try to follow this outline which opens up a range of new applications for linear algebra in a systematic way. .  Hilbert Space. After studying R   it is natural to think of the space R  .  It con tains all vectors v   v   v   v  ...  with an infinite sequence of components. This space   is actually too big when there is no control on the size of components v  . A much better idea is to keep the familiar definition of length using a sum of squares and to include only those vectors that have a finite length  Length squared  v    v    v    v      The infinite series must converge to a finite sum. This leaves          ...  but not       ...  . Vectors with finite length can be added   v  w  v    w   and multiplied by scalars so they form a vector space. It is the celebrated Hilbert space . Hilbert space is the natural way to let the number of dimensions become infinite and at the same time to keep the geometry of ordinary Euclidean space. Ellipses become infinitedimensional ellipsoids and perpendicular lines are recognized exactly as before. The vectors v and w are orthogonal when their inner product is zero Orthogonality v  w  v  w   v  w   v  w      . This sum is guaranteed to converge and for any two vectors it still obeys the Schwarz inequality  v  w  v  w  . The cosine even in Hilbert space is never larger than . There is another remarkable thing about this space  It is found under a great many different disguises. Its vectors can turn into functions which is the second point. . Lengths and Inner Products. Suppose f  x   sin x on the interval   x    . This f is like a vector with a whole continuum of components the values of sin x along the whole interval.  To find the length of such a vector the usual rule of adding the squares of the components becomes impossible.  This summation is replaced in a natural and inevitable way by integration  Length  f  of function  f         f  x   dx       sin x   dx    Our Hilbert space has become a function space .  The vectors are functions we have a way to measure their length and the space contains all those functions that have a finite lengthjust as in equation .  It does not contain the function F  x     x  because the integral of   x  is infinite. The same idea of replacing summation by integration produces the inner product of two functions  If f  x   sin x and g  x   cos x  then their inner product is  f  g       f  x  g  x  dx      sin x cos xdx   .  This is  exactly  like  the  vector  inner  product f  g .   It  is  still  related  to  the  length  by  f  f    f   .  The Schwarz inequality is still satisfied   f  g   f  g  .  Of course two functions like sin x and cos x whose inner product is zerowill be called orthogo nal. They are even orthonormal after division by their length   .  . The Fourier series of a function is an expansion into sines and cosines f  x   a   a  cos x  b  sin x  a  cos  x  b  sin  x   . To compute a coefficient like b   multiply both sides by the corresponding function sin x and integrate from  to   . The function f  x  is given on that interval. In other words take the inner product of both sides with sin x      f  x  sin xdx  a      sin xdx  a      cos x sin xdx  b       sin x   dx   . On the righthand side every integral is zero except onethe one in which sin x multi plies itself. The sines and cosines are mutually orthogonal as in equation  Therefore b  is the lefthand side divided by that one nonzero integral b       f  x  sin xdx      sin x   dx   f  sin x   sin x  sin x  . The Fourier coefficient a  would have cos x in place of sin x  and a  would use cos  x . The whole point is to see the analogy with projections. The component of the vector b along the line spanned by a is b  a  a  a . A Fourier series is projecting f  x  onto sin x . Its component p in this direction is exactly b  sin x . The coefficient b  is the least squares solution of the inconsistent equation b  sin x  f  x  .  This brings b  sin x as close as possible to f  x  .  All the terms in the series are projections onto a sine or cosine. Since the sines and cosines are orthogonal the Fourier series gives the coordinates of the vector f  x  with respect to a set of infinitely many perpendicular axes . .   GramSchmidt for Functions. There are plenty of useful functions other than sines and cosines and they are not always orthogonal. The simplest are the powers of x  and unfortunately there is no interval on which even  and x  are perpendicular.  Their inner product is always positive because it is the integral of x  .  Therefore the closest parabola to f  x  is not the sum of its projections onto  x  and x  . There will be a matrix like  A  A     and this coupling is given by the illconditioned Hilbert matrix .  On the interval   x   A  A              x      x    x      x  x    x  x    x       x   x    x   x             x  x   x  x   x   x   x   x                             . This matrix has a large inverse because the axes  x  x  are far from perpendicular. The situation becomes impossible if we add a few more axes. It is virtually hopeless to solve A  A  x  A  b for the closest polynomial of degree ten . More precisely it is hopeless to solve this by Gaussian elimination every roundoff error would be amplified by more than   .  On the other hand we cannot just give   up  approximation by polynomials has to be possible.   The right idea is to switch to orthogonal axes by GramSchmidt.  We look for combinations of  x  and x  that are orthogonal. It is convenient to work with a symmetrically placed interval like    x   because this makes all the odd powers of x orthogonal to all the even powers    x       xdx     x  x        x  dx   . Therefore the GramSchmidt process can begin by accepting v    and v   x as the first two perpendicular axes. Since  x  x     it only has to correct the angle between  and x  . By formula  the third orthogonal polynomial is Orthogonalize v   x      x           x  x    x  x  x  x       x  d x      d x  x     . The polynomials constructed in this way are called the Legendre polynomials and they are orthogonal to each other over the interval    x  . Check    x            x      d x   x    x        . The closest polynomial of degree ten is now computable without disaster by projecting onto each of the first  or  Legendre polynomials. .   Best Straight Line. Suppose we want to approximate y  x  by a straight line C  Dx between x   and x  .  There are at least three ways of finding that line and if you compare them the whole chapter might become clear! .  Solve   x       x  by least squares. The equation A  A  x  A  b is           x   x      x  x   C D       x    x  x    or           C D         . .  Minimize E       x   C  Dx   d x       C    D  C   C D    D  . The deriva tives with respect to C and D  after dividing by  bring back the normal equations of method  and the solution is  C         D         C    D   and       C    D   . .  Apply GramSchmidt to replace x by x     x        .  That is x     which is or thogonal to . Now the onedimensional projections add to the best line C  Dx   x             x   x      x     x      x            x     .  Problem Set . . a  Write the four equations for fitting y  C  Dt to the data y    at t    y    at t    y    at t   y   at t  . Show that the columns are orthogonal. b  Find the optimal straight line draw its graph and write E  . c  Interpret the zero error in terms of the original system of four equations in two unknowns The righthand side             is in the space. . Project b         onto each of the orthonormal vectors a              and a               and then find its projection p onto the plane of a  and a  . . Find also the projection of b         onto a               and add the three pro jections. Why is P  a  a    a  a    a  a   equal to I ? . If Q  and Q  are  orthogonal  matrices  so  that Q  Q  I   show  that Q  Q  is  also orthogonal. If Q  is rotation through   and Q  is rotation through   what is Q  Q  ? Can you find the trigonometric identities for sin      and cos      in the matrix multiplication Q  Q  ? . If u is a unit vector show that Q  I   uu  is a symmetric orthogonal matrix.  It is  a  reflection  also  known  as  a  Householder  transformation.   Compute Q when u               . . Find a third column so that the matrix Q                                 is orthogonal.  It must be a unit vector that is orthogonal to the other columns how much freedom does this leave? Verify that the rows automatically become orthonor mal at the same time. . Show  by  forming b  b directly  that  Pythagorass  law  holds  for  any  combination b  x  q     x  q  of orthonormal vectors  b    x      x   . In matrix terms b  Qx  so this again proves that lengths are preserved  Qx     x   . . Project the vector b       onto two vectors that are not orthogonal a        and a        .   Show that  unlike the orthogonal case  the sum of the two one dimensional projections does not equal b . . If the vectors q   q   q  are orthonormal what combination of q  and q  is closest to q  ?   . If q  and q  are the outputs from GramSchmidt what were the possible input vectors a and b ? . Show that an orthogonal matrix that is upper triangular must be diagonal. . What multiple of a       should be subtracted from a       to make the result orthogonal to a  ? Factor     into QR with orthonormal vectors in Q . . Apply the GramSchmidt process to a            b            c           and write the result in the form A  QR . . From the nonorthogonal a  b  c  find orthonormal vectors q   q   q   a            b            c           . . Find an orthonormal set q   q   q  for which q   q  span the column space of A                . Which  fundamental  subspace  contains q  ?   What  is  the  leastsquares  solution  of Ax  b if b          ? . Express the GramSchmidt orthogonalization of a   a  as A  QR  a             a            . Given n vectors a  with m components what are the shapes of A  Q  and R ? . With the same matrix A as in Problem  and with b           use A  QR to solve the leastsquares problem Ax  b . . If A  QR  find a simple formula for the projection matrix P onto the column space of A . . Show that these modified GramSchmidt steps produce the same C as in equation  C   c   q   c  q  and C  C    q   C   q  . This is much more stable to subtract the projections one at a time.  . In Hilbert space find the length of the vector v                  ...  and the length of the function f  x   e  over the interval   x  .   What is the inner product over this interval of e  and e   ? . What is the closest function a cos x  b sin x to the function f  x   sin  x on the in terval from   to  ? What is the closest straight line c  dx ? . By setting the derivative to zero find the value of b  that minimizes  b  sin x  cos x         b  sin x  cos x   dx . Compare with the Fourier coefficient b  . . Find the Fourier coefficients a   a   b  of the step function y  x   which equals  on the interval   x   and  on the remaining interval   x     a    y         a    y  cos x   cos x  cos x  b    y  sin x   sin x  sin x  . . Find the fourth Legendre polynomial. It is a cubic x   ax   bx  c that is orthogonal to  x  and x     o ver the interval    x  . . What is the closest straight line to the parabola y  x  over    x  ? . In the GramSchmidt formula  verify that C is orthogonal to q  and q  . . Find  an  orthonormal  basis  for  the  subspace  spanned  by a              a              a             . . Apply GramSchmidt to                   and          to find an orthonormal basis on the plane x   x   x   . What is the dimension of this subspace and how many nonzero vectors come out of GramSchmidt? . Recommended Find orthogonal vectors A  B  C by GramSchmidt from a  b  c  a            b            c            . A  B  C and a  b  c are bases for the vectors perpendicular to d           . . If A  QR then A  A  R  R  triangular times triangular . GramSchmidt on A corresponds to elimination on A  A . Compare A                           with A  A                     . For A  A  the pivots are       and the multipliers are    and    .   a  Using  those  multipliers  in A   show  that  column    of A and B  column     column  and C  column     column  are orthogonal. b  Check that  column       B       and  C       using the pivots. . True or false give an example in either case a Q   is an orthogonal matrix when Q is an orthogonal matrix. b  If Q  by  has orthonormal columns then  Qx  always equals  x  . . a  Find a basis for the subspace S in R  spanned by all solutions of x   x   x   x    . b  Find a basis for the orthogonal complement S  . c  Find b  in S and b  in S  so that b   b   b           . . The Fast Fourier Transform The Fourier series is linear algebra in infinite dimensions.  The vectors are functions f  x   they are projected onto the sines and cosines that produces the Fourier coefficients a  and b  . From this infinite sequence of sines and cosines multiplied by a  and b   we can reconstruct f  x  . That is the classical case which Fourier dreamt about but in actual calculations it is the discrete Fourier transform that we compute. Fourier still lives but in finite dimensions. This is pure linear algebra based on orthogonality.  The input is a sequence of num bers y  ... y     instead of a function f  x  . The output c  ... c    has the same length n .  The relation between y and c is linear so it must be given by a matrix.  This is the Fourier matrix F  and the whole technology of digital signal processing depends on it. The Fourier matrix has remarkable properties. Signals are digitized whether they come from speech or images or sonar or TV or even oil exploration. The signals are transformed by the matrix F  and later they can be transformed backto reconstruct. What is crucially important is that F and F   can be quick F   must be simple. The multiplications by F and F   must be fast. Those are both true. F   has been known for years and it looks just like F .  In fact F is symmetric and orthogonal apart from a factor  n  and it has only one drawback Its entries are complex numbers . That is a small price to pay and we pay it below. The difficulties are minimized by the fact that all entries of F and F   tare powers of a single number w . That number has w   .  The  by  discrete Fourier transform uses w  i and notice i   . The success of the whole DFT depends on F times its complex conjugate F  F F                  i    i  i   i  i  i   i  i  i                   i     i     i      i     i     i      i     i     i          I .  Immediately F F   I tells us that F    F  . The columns of F are orthogonal to give the zero entries in  I . The n by n matrices will have F F  nI . Then the inverse of F is just F  n . In a moment we will look at the complex number w  e      which equals i for n  . It is remarkable that F is so easy to invert. If that were all and up to  it was all the discrete transform would have an important place. Now there is more. The multipli cations by F and F   can be done in an extremely fast and ingenious way.  Instead of n  separate multiplications coming from the n  entries in the matrix the matrixvector products F c and F   y require only   n log n steps. This rearrangement of the multiplica tion is called the Fast Fourier Transform . The section begins with w and its properties moves on to F    and ends with the FFT the fast transform. The great application in signal processing is filtering  and the key to its success is the convolution rule .  In matrix language all circulant matrices are diagonalized by F . So they reduce to two FFTs and a diagonal matrix. Complex Roots of Unity Real equations can have complex solutions. The equation x      led to the invention of i and also to  i !.  That was declared to be a solution and the case was closed.  If someone asked about x   i   there was an answer  The square roots of a complex number are again complex numbers.  You must allow combinations x  iy  with a real part x and an imaginary part y  but no further inventions are necessary.  Every real or complex polynomial of degree n has a full set of n roots possibly complex and possibly repeated. That is the fundamental theorem of algebra. We are interested in equations like x   . That has four solutionsthe fourth roots of unity .  The two square roots of unity are  and  .  The fourth roots are the square roots of the square roots  and   i and  i .  The number i will satisfy i    because it satisfies i    . For the eighth roots of unity we need the square roots of i  and that brings us to w     i    . Squaring w produces     i  i     which is i because   i  is zero. Then w   i   . There has to be a system here. The complex numbers cos   i sin  in the Fourier matrix are extremely special. The real part is plotted on the x axis and the imaginary part on the y axis Figure .. Then the number w lies on the unit circle  its distance from the origin is cos    sin    .   It makes an angle  with the horizontal.  The whole plane enters in Chapter  where complex numbers will appear as eigenvalues even of real matrices. Here we need only special points w  all of them on the unit circle in order to solve w   .                                                                               The square of w can be found directly it just doubles the angle w    cos   i sin     cos    sin     i sin  cos  . The real part cos    sin   is cos     and the imaginary part  sin  cos  is sin   . Note that i is not included the imaginary part is a real number.  Thus w   cos    i sin   .  The square of w is still on the unit circle but at the double angle   .  That makes us suspect that w  lies at the angle n   and we are right. There is a better way to take powers of w .  The combination of cosine and sine is a complex exponential with amplitude one and phase angle   cos   i sin   e   .  The rules for multiplying like  e   e    e   continue to hold when the exponents i  are imaginary. The powers of w  e   stay on the unit circle  Powers of w w   e     w   e     w  e    .  The n th power is at the angle n  .  When n    the reciprocal   w has angle   .  If we multiply cos   i sin  by cos     i sin      we get the answer  e   e      cos   i sin   cos   i sin    cos    sin     . Note . I remember the day when a letter came to MIT from a prisoner in New York asking  if  Eulers  formula    was  true.   It  is  really  astonishing  that  three  of  the  key  functions of mathematics should come together in such a graceful way. Our best answer was to look at the power series for the exponential e      i    i    !   i    !    . The real part         is cos  . The imaginary part         is the sine The formula is correct and I wish we had sent a more beautiful proof. W ith this formula we can solve w   .  It becomes e     so that n  must carry us around the unit circle and back to the start. The solution is to choose      n  The primitive nth root of unity is w   e       cos   n  i sin   n .  Its n th power is e     which equals . For n   this root is    i     w   cos    i sin    i and w   cos    i sin      i   The fourth root is at      which is       . The other fourth roots are the powers i     i    i  and i   . The other eighth roots are the powers w    w   ... w   . The roots are equally spaced around the unit circle at intervals of    n . Note again that the square of w  is w   which will be essential in the Fast Fourier Transform. The roots add up to zero . First   i    i   and then Sum of eighth roots   w   w      w     .  One proof is to multiply the left side by w   which leaves it unchanged.  It yields w   w      w   and w   equals . The eight points each move through    but they remain the same eight points. Since zero is the only number that is unchanged when multiplied by w   the sum must be zero.  When n is even the roots cancel in pairs like   i    and i  i   . But the three cube roots of  also add to zero. The Fourier Matrix and Its Inverse In the continuous case the Fourier series can reproduce f  x  over a whole interval.  It uses infinitely many sines and cosines or exponentials. In the discrete case with only n coefficients c  ... c    to choose we only ask for equality at n points .  That gives n equations. We reproduce the four values y         when F c  y  F c  y c   c   c   c    c   ic   i  c   i  c    c   i  c   i  c   i  c    c   i  c   i  c   i  c    .    The input sequence is y        . The output sequence is c   c   c   c  . The four equa tions  look for a fourterm Fourier series that matches the inputs at four equally spaced points x on the interval from  to    Discrete Fourier Series c   c  e   c  e    c  e             at x       at x         at x       at x      . Those are the four equations in system .   At x    the series returns y    and continues periodically. The Discrete Fourier Series is best written in this complex form as a combination of exponentials e  rather than sin kx and cos kx . F or every n  the matrix connecting y to c can be inverted.  It represents n equations requiring the finite series c   c  e     n terms to agree with y at n points . The first agreement is at x   where c     c     y  . The remaining points bring powers of w  and the full problem is F c  y  F c  y               w w   w     w  w   w                  w    w        w                      c  c  c   c                   y  y  y   y           .  There stands the Fourier matrix F with entries F   w  .  It is natural to number the rows and columns from  to n   instead of  to n .  The first row has j   the first column has k   and all their entries are w   . To find the c s we have to invert F . In the  by  case F   was built from   i   i . That is the general rule that F   comes from the complex number w    w . It lies at the angle     n  where w was at the angle     n  V The inverse matrix is built from the powers of w      w  w  F     n               w   w    w        w                   w       w         w                 F n .  Thus F         e      e       e      e         has F             e       e        e       e          . Ro w j of F times column j of F   is always           n  . The harder part is off the diagonal to show that row j of F times column k of F   gives zero     w  w    w   w       w       w             if j   k .   The key is to notice that those terms are the powers of W  w  w      W  W     W      .  This number W is still a root of unity W   w  w   is equal to       .  Since j is different from k  W is different from .  It is one of the other roots on the unit circle. Those roots all satisfy   W    W     . Another proof comes from   W      W    W  W     W     .  Since W    the left side is zero. But W is not  so the last factor must be zero. The columns of F are orthogonal . The Fast Fourier Transform Fourier analysis is a beautiful theory and it is also very practical. To analyze a waveform into its frequencies is the best way to take a signal apart.  The reverse process brings it back.  For physical and mathematical reasons the exponentials are special and we can pinpoint one central cause If you differentiate e   or integrate it or translate x to x  h  the result is still a multiple of e  . Exponentials are exactly suited to differential equations integral equations and difference equations. Each frequency component goes its own way as an eigenvector and then they recombine into the solution. The analysis and synthesis of signalscomputing c from y and y from c is a central part of scientific computing. We want to show that F c and F   y can be done quickly. The key is in the relation of F  to F  or rather to two copies of F   which go into a matrix F    F                     i    i  i   i  i  i   i  i  i       is close to F                        . F  contains the powers of w   i  the fourth root of . F   contains the powers of w     the square root of .  Note especially that half the entries in F   are zero.  The  by  transform done twice requires only half as much work as a direct  by  transform.  If  by  transform could be replaced by two  by  transforms the work would be cut in half plus the cost of reassembling the results. What makes this true and possible in practice is the simple connection between w  and w    w     w   or  e         e      . The nd root is twice as far around the circle as the th root. If w    then  w     . The m th root is the square of the n th root if m is half of n  w    w  if m    n .    The speed of the FFT in the standard form presented here depends on working with highly composite numbers like    . Without the fast transform it takes     multiplications to produce F times c which we want to do often.  By contrast a fast transform can do each multiplication in only    steps. It is  times faster  because it replaces one factor of  by .   In general it replaces n  multiplications by   n   when n is   . By connecting F  to two copies of F     and then to four copies of F     and eventually to a very small F  the usual n  steps are reduced to   n log  n . W e need to see how y  F  c a vector with n components can be recovered from two vectors that are only half as long.  The first step is to divide c itself by separating its evennumbered components from its oddnumbered components c    c   c  ... c     and c    c   c  ... c     . The  coefficients  just  go  alternately  into c  and c  .   From  those  vectors  the  halfsize transform  gives y   F  c  and y   F  c  . Those  are  the  two  multiplications  by  the smaller matrix F  . The central problem is to recover y from the halfsize vectors y  and y   and Cooley and Tukey noticed how it could be done W The first m and the last m components of the vector y  F  c are y   y    w   y    j   ... m   y     y    w   y    j   ... m   .  Thus the three steps are  split c into c  and c   transform them by F  into y  and y   and reconstruct y from equation . We verify in a moment that this gives the correct y .  You may prefer the flow graph to the algebra. This idea can be repeated.  We go from F  to F  to F  .  The final count is   n   when starting with the power n    and going all the way to n  where no multiplication is needed.  This number   n  satisfies the rule given above twice the count for m plus m extra multiplications produces the count for n      m        m    n . Another way to count  There are  steps from n    to n  .  Each step needs n   multiplications by D    in equation  which is really a factorization of F   One FFT step F    I  D  I   D   F  F   evenodd permutation  .  The cost is only slightly more than linear. Fourier analysis has been completely trans formed by the FFT. To verify equation  split y  into even and odd  y          w   c  is identical to        w    c           w         c     .  Each sum on the right has m    n terms. Since w   is w   the two sums are y          w   c    w          w   c    y    w   y   .  For the second part of equation  j  m in place of j produces a sign change Inside the sums w        remains w   since w       . Outside w       w   because w    e       e     . The FFT idea is easily modified to allow other prime factors of n not only powers of . If n itself is a prime a completely different algorithm is used. Example . The steps from n   to m   are      c  c  c  c             c  c  c  c           F  c  F  c         y    . Combined the three steps multiply c by F  to give y .  Since each step is linear it must come from a matrix and the product of those matrices must be F                     i    i  i   i  i  i   i  i  i                i      i                                        .  You recognize the two copies of F  in the center. At the right is the permutation matrix that separates c into c  and c  .  At the left is the matrix that multiplies by w   .  If we started with F   the middle matrix would contain two copies of F  . Each of those would be split as above .  Thus the FFT amounts to a giant factorization of the Fourier matrix! The single matrix F with n  nonzeros is a product of approximately   log  n matrices and a permutation with a total of only n  nonzeros. The Complete FFT and the Butterfly The first step of the FFT changes multiplication by F  to two multiplications by F  . The evennumbered components  c   c   are transformed separately from  c   c    Figure . gives a flow graph for n  .  For n   the key idea is to replace each F  box by F  boxes .  The new factor w   i is the square of the old factor w  w   e      .  The flow graph shows the order that the c s enter the FFT and the log  n stages that take them through itand it also shows the simplicity of the logic. Ev ery stage needs   n multiplications so the final count is   n log n . There is an amaz ing rule for the overall permutation of c s before entering the FFT Write the subscripts                                                                ...   in  binary  and reverse  the  order  of  their  bits .   The  subscripts  appear  in  bit reversed order on the left side of the graph. Even numbers come before odd numbers ending in  come before numbers ending in . Pr oblem Set . . What are F  and F  for the  by  Fourier matrix F ? . Find a permutation P of the columns of F that produces F P  F  n by n  Combine with F F  nI to find F  and F  for the n by n Fourier matrix. . If you form a  by  submatrix of the  by  matrix F   keeping only the entries in its first third and fifth rows and columns what is that submatrix? . Mark all the sixth roots of  in the complex plane.  What is the primitive root w  ? Find its real and imaginary part.  Which power of w  is equal to   w  ?  What is   w  w   w   w   w  ? . Find all solutions to the equation e     and all solutions to e    i . . What are the square and the square root of w   the primitive th root of ? . Solve the  by  system  if the righthand sides are y    y    y    y   . In other words solve F  c  y . . Solve the same system with y            by knowing F    and computing c  F    y . Verify that c   c  e   c  e    c  e   takes the values      at the points x             . . a  If y            show that c           satisfies F  c  y . b  Now suppose y            and find c .  . For n   write y  from the first line of equation  and y  from the second line. For n   use the first line to find y  and y   and the second to find y  and y   all in terms of y  and y  . . Compute y  F  c by the three steps of the Fast Fourier Transform if c           . . Compute y  F  c by the three steps of the Fast Fourier Transform if c                   . Repeat the computation with c                   . . For the  by  matrix write out the formulas for c   c   c   c  and verify that if  f  is odd then c is odd .  The vector f is odd if f      f   for n   that means f    f    f   f    as in sin  sin    sin   sin    . This is copied by c and it leads to a fast sine transform. . Multiply the three matrices in equation  and compare with F . in which six entries do you need to know that i    ? . Invert the three factors in equation  to find a fast factorization of F   . . F is symmetric. So transpose equation  to find a new Fast Fourier Transform! . All entries in the factorization of F  involve powers of w  sixth root of  F    I D I  D  F  F    P  . Write these factors with  w  w  in D and  w   w  in F  . Multiply! Problems  introduce the idea of an eigenvector and eigenvalue when a matrix times a vector is a multiple of that vector. This is the theme of Chapter . . The columns of the Fourier matrix F are the eigenvectors of the cyclic permutation P . Multiply PF to find the eigenvalues   to                                                                        i    i  i   i  i  i   i  i  i                          i    i  i   i  i  i   i  i  i                         . This is PF  F  or P  F  F   . . Two eigenvectors of this circulant matrix C are          and    i  i   i   .  What are the eigenvalues e  and e  ?      c  c  c  c  c  c  c  c  c  c  c  c  c  c  c  c                      e                and C       i i  i        e        i i  i       .   . Find the eigenvalues of the periodic      matrix C .  The  s in the corners of C make it periodic  a circulant matrix  C                                    has c     c      c     c     . . To multiply C times x  when C  F EF    we can multiply F  E  F   x  instead. The direct Cx uses n  separate multiplications.  Knowing E and F  the second way uses only n log  n  n multiplications. How many of those come from E  how many from F  and how many from F   ? . How could you quickly compute these four components of F c starting from c   c   c   c   c   c   c   c  ? You are finding the Fast Fourier Transform! F c       c   c   c   c  c   ic   i  c   i  c  c   i  c   i  c   i  c  c   i  c   i  c   i  c       . Re view Exercises . Find the length of a           and write two independent vectors that are per pendicular to a . . Find all vectors that are perpendicular to        and         by making those the rows of A and solving Ax  . . What is the angle between a          and b         ? . What is the projection p of b         onto a          ? . Find the cosine of the angle between the vectors      and       . Where  is  the  projection  of b         onto  the  plane  spanned  by        and        ? . The system Ax  b has a solution if and only if b is orthogonal to which of the four fundamental subspaces? . Which straight line gives the best fit to the following data b   at t   b   at t   b   at t  ? . Construct the projection matrix P onto the space spanned by        and        .  . Which constant function is closest to y  x  in the leastsquares sense over the interval   x  ? . If Q is orthogonal is the same true of Q  ? . Find all  by  orthogonal matrices whose entries are zeros and ones. . What multiple of a  should be subtracted from a   to make the result orthogonal to a  ? Sketch a figure. . Factor  cos  sin  sin    into QR  recognizing that the first column is already a unit vector. . If every entry in an orthogonal matrix is either   or     how big is the matrix? . Suppose the  vectors q  ... q  are orthonormal.   If b  c  q     c  q    give a formula for the first coefficient c  in terms of b and the q s. . What words describe the equation A  A  x  A  b  the vector p  A  x  Pb  and the matrix P  A  A  A    A  ? . If the orthonormal vectors q              and q              are the columns of Q  what are the matrices Q  Q and QQ  ?  Show that QQ  is a projection matrix onto the plane of q  and q  . . If v  ... v  is an orthonormal basis for R   show that v  v      v  v    I . . True or false   If the vectors x and y are orthogonal and P is a projection then Px and Py are orthogonal. . Try to fit a line b  C  Dt through the points b   t   and b   t   and show that the normal equations break down. Sketch all the optimal lines minimizing the sum of squares of the two errors. . What point on the plane x  y  z   is closest to b         ? . Find an orthonormal basis for R  starting with the vector        . . CT scanners examine the patient from different directions and produce a matrix giving the densities of bone and tissue at each point.  Mathematically the problem is to recover a matrix from its projections.  in the  by  case can you recover the matrix A if you know the sum along each row and down each column? . Can you recover a  by  matrix if you know its row sums and column sums and also the sums down the main diagonal and the four other parallel diagonals?   . Find an orthonormal basis for the plane x  y  z   and find the matrix P that projects onto the plane. What is the nullspace of P ? . Let A          and let V be the nullspace of A . a  Find a basis for V and a basis for V  . b  Write an orthonormal basis for V   and find the projection matrix P  that projects vectors in R  onto V  . c  Find the projection matrix P  that projects vectors in R  onto V . . Use GramSchmidt to construct an orthonormal pair q   q  from a            and a             Express a  and a  as combinations of q  and q   and find the triangular R in A  QR . . For any A  b  x  and y  show that a  if Ax  b and y  A   then y  b  . b  if Ax   and A  y  b  then x  b  . What theorem does this prove about the fundamental subspaces? . Is there a matrix whose row space contains        and whose nullspace contains        ? . The distance from a plane a  x  c in m dimensional space to the origin is  c    a  . How far is the plane x   x   x   x    from the origin and what point on it is nearest? . In the parallelogram with corners at  v  w  and v  w  show that the sum of the squared lengths of the four sides equals the sum of the squared lengths of the two diagonals. . a  Find an orthonormal basis for the column space of A . A                           . b  Write A as QR  where Q has orthonormal columns and R is upper triangular. c  Find the leastsquares solution to Ax  b  if b              . . With weighting matrix W       what is the W inner product of      with      ?  . To  solve  a  rectangular  system Ax  b   we  replace A   which  doesnt  exist  by  A  A    A  which exists if A has independent columns.  Show that this is a left inverse of A but not a rightinverse. On the left of A it gives the identity on the right it gives the projection P . . Find the straight line C  Dt that best fits the measurements b         at times t        . . Find the curve y  C  D   which gives the best leastsquares fit to the measure ments y   at t   y   at t   y   at t  .  Write the three equations that are solved if the curve goes through the three points and find the best C and D . . If the columns of A are orthogonal to each other what can you say about the form of A  A ? If the columns are orthonormal what can you say then? . Under what condition on the columns of A which may be rectangular is A  A in vertible?   . Introduction Determinants are much further from the center of linear algebra than they were a hundred years ago.  Mathematics keeps changing direction!  After all a single number can tell only so much about a matrix. Still it is amazing how much this number can do. One viewpoint is this The determinant provides an explicit formula for each entry of A   and A   b .  This formula will not change the way we compute even the deter minant itself is found by elimination.  In fact elimination can be regarded as the most efficient way to substitute the entries of an n by n matrix into the formula.  What the formula does is to show how A   depends on the n  entries of A  and how it varies when those entries vary. We can list four of the main uses of determinants . They test for invertibility. If the determinant of A is zero then A is singular . If det A     then A is invertible and A   involves   det A . The most important application and the reason this chapter is essential to the book is to the family of matrices A   I .  The parameter  is subtracted all along the main diagonal and the problem is to find the eigenvalues for which A   I is singular.  The test is det  A   I   . This polynomial of degree n in  has exactly n roots. The matrix has n eigenvalues This is a fact that follows from the determinant formula and not from a computer. . The determinant of A equals the volume of a box in n dimensional space. The edges of the box come from the rows of A Figure ..   The columns of A would give an entirely different box with the same volume. The simplest box is a little cube dV  dxdydz  as in  f  x  y  z  dV .  Suppose we change to cylindrical coordinates by x  r cos   y  r sin   z  z . Just as a small inter val dx is stretched to  dx  du  du when u replaces x in a single integralso the volume element becomes J dr d  dz .  The Jacobian determinant is the threedimensional ana                                       logue of the stretching factor dx  du  Jacobian J          x   r  x        x   z  y   r  y        y   z  z   r  z        z   z                cos   r sin   sin  r cos             . The value of this determinant is J  r .   It is the r in the cylindrical volume element r dr d  dz  this element is our little box. It looks curved if we try to draw it but proba bly it gets straighter as the edges become infinitesimal. . The  determinant  gives  a  formula  for  each  pivot.   Theoretically  we  could  predict when a pivot entry will be zero requiring a row exchange.  From the formula determi nant    product of the pivots  it follows that regardless of the order of elimination the product of the pivots remains the same apart from sign . Y ears ago this led to the belief that it was useless to escape a very small pivot by exchanging rows since eventually the small pivot would catch up with us.  But what usually happens in practice if an abnormally small pivot is not avoided is that it is very soon followed by an abnormally large one.  This brings the product back to normal but it leaves the numerical solution in ruins. . The determinant measures the dependence of A   b on each element of b .   If one parameter is changed in an experiment or one observation is corrected the influence coefficient in A   is a ratio of determinants. There is one more problem about the determinant.  It is difficult not only to decide on its importance and its proper place in the theory of linear algebra but also to choose   the best definition.  Obviously det A will not be some extremely simple function of n  variables otherwise A   would be much easier to find than it actually is. The simple things about the determinant are not the explicit formulas but the prop erties it possesses . This suggests the natural place to begin. The determinant can be and will be defined by its three most basic properties  det I   the sign is reversed by a row exchange the determinant is linear in each row separately . The problem is then to show by systematically using these properties how the determinant can be computed. This will bring us back to the product of the pivots. Section . explains these three defining properties of the determinant and their most important consequences. Section . gives two more formulas for the determinantthe big formula with n ! terms and a formula by induction.  In Section . the determi nant is applied to find A   . Then we compute x  A   b by Cramers rule . And finally in an optional remark on permutations we show that whatever the order in which the prop erties are used the result is always the samethe defining properties are selfconsistent. Here is a lighthearted question about permutations. How many exchanges does it take to change VISA into AVIS ? Is this permutation odd or even? . Properties of the Determinant This will be a pretty long list.  Fortunately each rule is easy to understand  and even easier to illustrate  for a  by  example.   Therefore we shall verify that the familiar definition in the  by  case det  a   b c   d        a   b c   d       ad  bc  possesses every property in the list.  Notice the two accepted notations for the deter minant det A and  A  .  Properties  will be deduced from the previous ones. Every property is a consequence of the first three .  We emphasize that the rules apply to square matrices of any size . . The determinant of the identity matrix is . det I                       and                                      and ...  . The determinant changes sign when two rows are exchanged . Row exchange      c   d a   b       cb  ad        a   b c   d      . The determinant of every permutation matrix is det P   . By row exchanges we can turn P into the identity matrix. Each row exchange switches the sign of the determinant until we reach det I  . Now come all other matrices! . The determinant depends linearly on the first row .  Suppose A  B  C are the same from the second row downand row  of A is a linear combination of the first rows of B and C . Then the rule says det A is the same combination of det B and det C . Linear combinations involve two operationsadding vectors and multiplying by scalars. Therefore this rule can be split into two parts Add vectors in row       a  a  b  b  c d            a   b c   d            a  b  c d      . Multiply by t in row       ta   tb c d       t      a   b c   d      . Notice that the first part is not the false statement det  B  C   det B  det C . You cannot add all the rows  only one row is allowed to change.  Both sides give the answer ad  a  d  bc  b  c . The second part is not the false statement det  tA   t det A . The matrix tA has a factor t in every row and the determinant is multiplied by t  .  It is like the volume of a box when all sides are stretched by . In n dimensions the volume and determinant go up by   .  If only one side is stretched the volume and determinant go up by  that is rule . By rule  there is nothing special about the first row. The determinant is now settled  but that fact is not at all obvious. Therefore we grad ually use these rules to find the determinant of any matrix. . If two rows of A are equal then det A  . Equal rows      a   b a   b       ab  ba   . This follows from rule  since if the equal rows are exchanged the determinant is sup posed to change sign. But it also has to stay the same because the matrix stays the same. The only number which can do that is zero so det A  . The reasoning fails if     which is the case in Boolean algebra.  Then rule  should replace rule  as one of the defining properties.   . Subtracting a multiple of one row from another row leaves the same determinant . Row operation      a   c   b   d c d            a   b c   d      . Rule  would say that there is a further term          but that term is zero by rule . The usual elimination steps do not affect the determinant! . If A has a row of zeros then det A   . Zero row          c   d        . One proof is to add some other row to the zero row.  The determinant is unchanged by rule . Because the matrix will now have two identical rows det A   by rule . . If A is triangular then det A is the product a  a   a  of the diagonal entries.  If the triangular A has s along the diagonal then det A   . Triangular matrix      a   b  d       ad      a  c   d       ad . Proof. Suppose the diagonal entries are nonzero.  Then elimination can remove all the offdiagonal entries without changing the determinant by rule . If A is lower triangu lar the steps are downward as usual. If A is upper triangular the last column is cleared out firstusing multiples of a  . Either way we reach the diagonal matrix D  D     a  . . . a     has det D  a  a   a  det I  a  a   a  . To find det D we patiently apply rule .  Factoring out a  and then a  and finally a  leaves the identity matrix. At last we have a use for rule  det I  . If a diagonal entry is zero then elimination will produce a zero row .  By rule  these elimination steps do not change the determinant.  By rule  the zero row means a zero determinant. This means When a triangular matrix is singular because of a zero on the main diagonal its determinant is zero . This is a key property. All singular matrices have a zero determinant . . If A is singular then det A   . If A is invertible then det A    . Singular matrix  a   b c   d  is not invertible if and only if ad  bc   .  If A is singular  elimination leads to a zero row in U .   Then det A  det U  .   If A is nonsingular elimination puts the pivots d  ... d  on the main diagonal.  We have a product of pivots formula for det A ! The sign depends on whether the number of row exchanges is even or odd Product of pivots det A   det U   d  d   d  .  The ninth property is the product rule. I would say it is the most surprising. . The determinant of AB is the product of det A times det B. Product rule  A  B    AB       a   b c   d           e    f g   h            ae  bg   a f  bh ce  dg   c f  dh      . A particular case of this rule gives the determinant of A   . It must be   det A  det A     det A because  det A  det A     det AA    det I   .  In the  by  case the product rule could be patiently checked  ad  bc  eh  f g    ae  bg  c f  dh    a f  bh  ce  dg  . In the n by n case we suggest two possible proofssince this is the least obvious rule. Both proofs assume that A and B are nonsingular  otherwise AB is singular  and the equation det AB   det A  det B  is easily verified. By rule  it becomes   . i  We prove that the ratio d  A   det AB  det B has properties .  Then d  A  must equal det A .  For example d  I   det B  det B   rule  is satisfied.  If two rows of A are exchanged so are the same two rows of AB  and the sign of d changes as required by rule .  A linear combination in the first row of A gives the same linear combination in the first row of AB .  Then rule  for the determinant of AB  divided by the fixed quantity det B  leads to rule  for the ratio d  A  . Thus d  A   det AB  det B coincides with det A  which is our product formula. ii  This second proof is less elegant.  For a diagonal matrix det DB   det D  det B  follows  by  factoring  each d  from  its  row.   Reduce  a  general  matrix A to D by eliminationfrom A to U as usual and from U to D by upward elimination.  The determinant does not change except for a sign reversal when rows are exchanged. The same steps reduce AB to DB  with precisely the same effect on the determinant. But for DB it is already confirmed that rule  is correct. . The transpose of A has the same determinant as A itself det A   det A. Transpose rule    A          a   b c   d            a   c b   d          A     .   Again the singular case is separate A is singular if and only if A  is singular and we have   . If A is nonsingular then it allows the factorization PA  LDU  and we apply rule  for the determinant of a product det P det A  det L det D det U .  Transposing PA  LDU gives A  P   U  D  L   and again by rule  det A  det P   det U  det D  det L  .  This is simpler than it looks because L  U  L   and U  are triangular with unit diagonal. By rule  their determinants all equal .  Also any diagonal matrix is the same as its transpose D  D  . We only have to show that det P  det P  . Certainly det P is  or   because P comes from I by row exchanges. Observe also that PP   I .  The  in the first row of P matches the  in the first column of P   and misses the s in the other columns.  Therefore det P det P   det I   and P and P  must have the same determinant both  or both  . We conclude that the products  and  are the same  and det A  det A  .   This fact practically doubles our list of properties because every rule that applied to the rows can now be applied to the columns The determinant changes sign when two columns are exchanged two equal columns or a column of zeros produce a zero determinant and the determinant depends linearly on each individual column. The proof is just to transpose the matrix and work with the rows. I think it is time to stop and call the list complete.  It only remains to find a definite formula for the determinant and to put that formula to use. Pr oblem Set . . If a  by  matrix has det A     find det   A   det   A   det  A    and det  A    . . If a  by  matrix has det A    find det    A   det   A   det  A    and det  A    . . Row exchange   Add row  of A to row  then subtract row  from row .  Then add row  to row  and multiply row  by   to reach B .  Which rules show the following? det B       c   d a   b      equals  det A        a   b c   d      . Those rules could replace Rule  in the definition of the determinant.  . By applying row operations to produce an upper triangular U  compute det                                   and det                                  . Exchange rows  and  of the second matrix and recompute the pivots and determi nant. Note . Some readers will already know a formula for  by  determinants. It has six terms equation  of the next section three going parallel to the main diagonal and three others going the opposite way with minus signs. There is a similar formula for  by  determinants but it contains !   terms  not just eight . You cannot even be sure that a minus sign goes with the reverse diagonal as the next exercises show. . Count row exchanges to find these determinants det                                                      and det                                                      . . For each n   how many exchanges will put row n   row n   ... row  into the normal order row  ...  row n   row n ?  Find det P for the n by n permutation with s on the reverse diagonal. Problem  had n  . . Find the determinants of a  a rank one matrix A                   . b  the upper triangular matrix U                                                    . c  the lower triangular matrix U  . d  the inverse matrix U   .   e  the reversetriangular matrix that results from row exchanges M                                                    . . Show how rule  det   if a row is zero comes directly from rules  and . . Suppose you do two row operations at once  going from  a   b c   d  to  a  mc   b  md c   a d   b  . Find the determinant of the new matrix by rule  or by direct calculation. . If Q is an orthogonal matrix so that Q  Q  I  prove that det Q equals   or  . What kind of box is formed from the rows or columns of Q ? . Prove again that det Q   or   using only the Product rule.  If  det Q    then det Q  blows up. How do you know this cant happen to Q  ? . Use row operations to verify that the  by  Vandermonde determinant is det     a   a   b   b   c   c       b  a  c  a  c  b  . . a  A skewsymmetric matrix satisfies K    K  as in K      a b  a  c  b  c     . In the  by  case why is det   K        det K ?  On the other hand det K   det K always. Deduce that the determinant must be zero. b  Write down a  by  skewsymmetric matrix with det K not zero. . True or false with reason if true and counterexample if false a  If A and B are identical except that b    a   then det B   det A . b  The determinant is the product of the pivots. c  If A is invertible and B is singular then A  B is invertible. d  If A is invertible and B is singular then AB is singular. e  The determinant of AB  BA is zero.  . If every row of A adds to zero prove that det A  .  If every row adds to  prove that det  A  I   . Show by example that this does not imply det A  . . Find these  by  determinants by Gaussian elimination det                                                   and det       t t  t  t  t t  t  t  t t  t  t       . . Find the determinants of A             A               A   I            . F or which values of  is A   I a singular matrix? . Ev aluate det A by reducing the matrix to triangular form rules  and . A                              B                              C                             . What are the determinants of B  C  AB  A  A  and C  ? . Suppose that CD   DC  and find the flaw in the following argument  Taking de terminants gives  det C  det D     det D  det C   so either det C   or det D  . Thus CD   DC is only possible if C or D is singular. . Do these matrices have determinant    or ? A                             B                             C                             . . The inverse of a  by  matrix seems to have determinant   det A    det  ad  bc  d  b  c a   ad  bc ad  bc   . What is wrong with this calculation? What is the correct det A   ? Problems  use the rules to compute specific determinants. . Reduce A to U and find det A  product of the pivots A                             and A                             .   . By applying row operations to produce an upper triangular U  compute det                                              and det                                                   . . Use row operations to simplify and compute these determinants det                            and det     t    t  t  t t  t     . . Elimination reduces A to U . Then A  LU  A                                                                      LU . Find the determinants of L  U  A  U   L    and U   L   A . . If a  is i times j  show that det A  . Exception when A     . . If a  is i  j  show that det A  . Exception when n   or . . Compute the determinants of these matrices by row operations A      a      b c         B        a         b         c d              and C     a   a   a a   b   b a   b   c    . . What is wrong with this proof that projection matrices have det P  ? P  A  A  A    A  so  P    A    A   A   A     . . Calculus question Show that the partial derivatives of ln  det A  give A    f  a  b  c  d   ln  ad  bc  leads to   f   a  f   c  f   b  f   d   A   . .  MA TLAB  The Hilbert matrix hilb n has i  j entry equal to    i  j    .  Print ti determinants of hilb   hilb  ... hilb  .  Hilbert matrices are hard to work with! What are the pivots? .  MATLAB  What is a typical determinant experimentally of rand n and randn n for n        ? And what does  Inf  mean in MATLAB ?  . Using MATLAB  find the largest determinant of a  by  matrix of s and s. . If you know that det A   what is the determinant of B ? det A         row  row  row           det B         row   row  row   row  row   row          . Suppose the  by  matrix M has four equal rows all containing a  b  c  d . We know that det  M   . The problem is to find det  I  M  by any method det  I  M              a b c d a   b c d a b   c d a b c   d          . Partial credit if you find this determinant when a  b  c  d  .  Sudden death if you say that det  I  M   det I  det M . . Formulas for the Determinant The first formula has already appeared. Row operations produce the pivots in D  A If A is invertible then PA  LDU and det P   . The product rule gives det A   det L det D det U    product of the pivots  .  The sign   depends on whether the number of row exchanges is even or odd. The triangular factors have det L  det U   and det D  d   d  . In the  by  case the standard LDU factorization is  a   b c   d      c  a              b  a    . The product of the pivots is ad  bc .  That is the determinant of the diagonal matrix D . If the first step is a row exchange the pivots are c and   det A   c . Example . The        second difference matrix has pivots        ... in D                                     LDU  L                 n     n        U .   Its determinant is the product of its pivots. The numbers  ... n all cancel det A              n   n   n   . MA TLAB computes the determinant from the pivots.  But concentrating all information into the pivots makes it impossible to figure out how a change in one entry would affect the determinant.  We want to find an explicit expression for the determinant in terms of the n  entries. For n    we will be proving that ad  bc is correct.  For n    the determinant formula is again pretty well known it has six terms        a    a  a  a      a  a           a  a  a          a  a  a   a  a  a   a  a  a   a  a  a  .  Our goal is to derive these formulas directly from the defining properties  of det A . If we can handle n   and n   in an organized way you will see the pattern. To start each row can be broken down into vectors in the coordinate directions  a   b    a      b  and  c   d    c      d  . Then we apply the property of linearity first in row  and then in row  Separate into n     easy determinants      a   b c   d            a  c   d             b c   d            a  c             a   d             b c              b  d      .  Every row splits into n coordinate directions so this expansion has n  terms.  Most of those terms all but n !  n factorial will be automatically zero.  When two rows are in the same coordinate direction one will be a multiple of the other and      a  c                b  d        . We pay attention only when the rows point in different directions . The nonzero terms have to come in different columns . Suppose the first row has a nonzero term in column   the second row is nonzero in column   and finally the n th row in column v .  The column numbers    ... v are all different.  They are a reordering or permutation  of  the numbers    ... n . The  by  case produces !   determinants        a    a  a  a      a  a                 a  a  a                                      a  a  a                 a  a  a                 a  a  a                 a  a  a         .  All  but  these n !  determinants  are  zero  because  a  column  is  repeated.   There  are n choices for the first column   n   remaining choices for   and finally only one choice for the last column v .  All but one column will be used by that time when we snake down the rows of the matrix. In other words there are n ! ways to permute the numbers    ... n . The column numbers give the permutations Column numbers      v                                                  . Those are the !   permutations of         the first one is the identity. The determinant of A is now reduced to six separate and much simpler determinants. Factoring out the a   there is a term for every one of the six permutations det A  a  a  a                                            a  a  a                    a  a  a                    a  a  a                    a  a  a                   .  Every term is a product of n   entries a   with each row and column represented once . If the columns come in the order   ... v   that term is the product a    a  times the determinant of a permutation matrix P . The determinant of the whole matrix is the sum of these n ! terms and that sum is the explicit formula we are after  Big Formula det A       a   a    a   det P .  F or an n by n matrix this sum is taken over all n ! permutations   ... v  of the numbers   ... n  . The permutation gives the column numbers as we go down the matrix. The is appear in P at the same places where the a s appeared in A . It remains to find the determinant of P .  Row exchanges transform it to the identity matrix and each exchange reverses the sign of the determinant det P    or   for an even or odd number of row exchanges .          is odd so                            is even so                           requires one exchange and        requires two exchanges to recover        . These are two of the six  signs. For n   we only have      and       det A  a  a  det            a  a  det            a  a   a  a  or ad  bc . No one can claim that the big formula  is particularly simple.  Nevertheless it is possible to see why it has properties .  For A  I  every product of the a  will be zero except for the column sequence     ... n  . This term gives det I  . Property  will be checked in the next section because here we are most interested in property  The determinant should depend linearly on the first row a   a  ... a   . Look at all the terms a   a    a  involving a  .  The first column is   .  This leaves some permutation   ... v  of the remaining columns   ... n  .  We collect all these terms together as a  C   where the coefficient of a  is a smaller determinant with row  and column  removed  Cofactor of a  C     a    a   det P  det  submatrix of A  .  Similarly  the entry a  is multiplied by some smaller determinant C  . Grouping all the terms that start with the same a    formula  becomes Cofactors along row  det A  a  C   a  C     a   C   .  This shows that det A depends linearly on the entries a  ... a   of the first row. Example . For a  by  matrix this way of collecting terms gives det A  a              a             a             .  The cofactors C   C   C  are the  by  determinants in parentheses. Expansion of det A in Cofactors We want one more formula for the determinant. If this meant starting again from scratch it would be too much But the formula is already discoveredit is  and the only point is to identify the cofactors C   that multiply a   . We  know  that C   depends  on  rows   ... n .   Row    is  already  accounted  for  by a   . Furthermore a   also accounts for the j th column so its cofactor C   must depend  entirely on the other columns .  No row or column can be used twice in the same term. What we are really doing is splitting the determinant into the following sum Cofactor splitting        a  a  a  a  a  a  a  a  a                 a  a  a  a  a                 a  a  a  a  a                 a  a  a  a  a         . For a determinant of order n  this splitting gives n smaller determinants  minors  of order n   you see the three  by  submatrices.  The submatrix M   is formed by throwing away row  and column  j . Its determinant is multiplied by a   and by a plus or minus sign. These signs alternate as in det M    det M   det M   Cofactors of row  C           det M   . The second cofactor C  is a  a   a  a   which is det M  times  . This same tech nique works on every n by n matrix.  The splitting above confirms that C  is the deter minant of the lower right corner M  . There  is  a  similar  expansion  on  any  other  row  say  row i .   It  could  be  proved  by exchanging row i with row . Remember to delete row i and column  j of A for M   B The determinant of A is a combination of any row i times its cofactors det A by cofactors det A  a   C    a   C      a  C  .  The cofactor C   is the determinant of M  with the correct sign delete row i and column j C          det M  .  These formulas express det A as a combination of determinants of order n  . We could have defined the determinant by induction on n . A  by  matrix has det A  a   and then equation  defines the determinants of  by  matrices   by  matrices and n by n matrices.  We preferred to define the determinant by its properties which are much simpler to explain.   The explicit formula  and the cofactor formula  followed directly from these properties. There is one more consequence of det A  det A  .  We can expand in cofactors of a column of A  which is a row of A  . Down column j of A  det A  a   C    a   C      a  C  .  Example . The  by  second difference matrix A has only two nonzeros in row  Use cofactors A                                   .   C  comes from erasing row  and column  which leaves the      pattern C   det A   det                    For a     it is column  that gets removed and we need its cofactor C   C          det                      det          det A  . This left us with the  by  determinant. Altogether row  has produced  C   C   det A     det A    det A           The same idea applies to A  and A   and every A   Recursion by cofactors det A     det A      det A    .  This gives the determinant of increasingly bigger matrices. At every step the determinant of A  is n   from the previous determinants n and n        matrix det A     n    n     n   . The answer n   agrees with the product of pivots at the start of this section. Pr oblem Set . . For these matrices find the only nonzero term in the big formula  A                                                    and B                                                    . There  is  only  one  way  of  choosing  four  nonzero  entries  from  different  rows  and different columns. By deciding even or odd compute det A and det B . . Expand those determinants in cofactors of the first row.   Find the cofactors they include the signs         and the determinants of A and B . . True or false ? a  The determinant of S   AS equals the determinant of A . b  If det A   then at least one of the cofactors must be zero.  c  A matrix whose entries are s and s has determinant   or  . . a  Find the LU factorization the pivots and the determinant of the  by  matrix whose entries are a   smaller of i and j . Write out the matrix. b  Find the determinant if a   smaller of n  and n   where n    n    n    n   . Can you give a general rule for any n   n   n   n  ? . Let F  be the determinant of the     tridiagonal matrix  n by n  F   det                                . By expanding in cofactors along row  show that F   F     F    . This yields the Fibonacci sequence            ... for the determinants. . Suppose A  is the n by n tridiagonal matrix with is on the three diagonals A       A              A                                ... Let D  be the determinant of A   we want to find it. a  Expand in cofactors along the first row to show that D   D     D    . b  Starting from D    and D    find D   D  ... D  .  By noticing how these numbers cycle around with what period? find D  . . a  Evaluate this determinant by cofactors of row                                                            . b  Check by subtracting column  from the other columns and recomputing. . Compute the determinants of A   A   A  . Can you predict A  ? A             A                              A                                                     . Use row operations to produce zeros or use cofactors of row .   . How many multiplications to find an n by n determinant from a  the big formula ? b  the cofactor formula  building from the count for n  ? c  the product of pivots formula including the elimination steps? . In a  by  matrix does a  sign or  sign go with a  a  a  a  a  down the reverse diagonal? In other words is P             even or odd? The checkerboard pattern of  signs for cofactors does not give det P . . If A is m by n and B is n by m  explain why det   A  B    I   det AB .  Hint Postmultiply by  I  B   I  .  Do an example with m  n and an example with m  n .   Why does your second example automatically have det AB  ? . Suppose the matrix A is fixed except that a  varies from   to   . Give examples in which det A is always zero or never zero. Then show from the cofactor expansion  that otherwise det A   for exactly one value of a  . Problems  use the big formula with n ! terms  A     a   a    a  . . Compute the determinants of A  B  C from six terms. Independent rows? A                             B                             C                             . . Compute the determinants of A  B  C . Are their columns independent? A                             B                             C   A   B  . . Show that det A   regardless of the five nonzeros marked by x s A     x   x   x     x     x    .  What is the rank of A ?  . This problem shows in two ways that det A   the x s are any numbers A         x   x   x   x   x x   x   x   x   x        x   x        x   x        x   x        .  by  matrix  by  zero matrix Always singular  a  How do you know that the rows are linearly dependent? b  Explain why all  terms are zero in the big formula for det A . . Find two ways to choose nonzeros from four different rows and columns A                                                    B                                                    .  B has the same zeros as A . Is det A equal to    or    or    ? What is det B ? . Place the smallest number of zeros in a  by  matrix that will guarantee det A  . Place as many zeros as possible while still allowing det A   . . a  If a   a   a    how many of the six terms in det A will be zero? b  If a   a   a   a    how many of the  products a   a   a   a   are sure to be zero? . How many  by  permutation matrices have det P   ? Those are even permuta tions. Find one that needs four exchanges to reach the identity matrix. . If det A    at least one of the n ! terms in the big formula  is not zero.  Deduce that some ordering of the rows of A leaves no zeros on the diagonal.  Dont use P from elimination that PA can have zeros on the diagonal. . Prove that  is the largest determinant for a  by  matrix of s and  s. . How  many  permutations  of          are  even  and  what  are  they?   Extra  credit What are all the possible  by  determinants of I  P  ? Problems  use cofactors C          det M  . Delete row i  column j . . Find cofactors and then transpose. Multiply C   and C   by A and B ! A            B                             . . Find the cofactor matrix C and compare AC  with A    A                     A                               .   . The matrix B  is the      matrix A  except that b    instead of a   . Using cofactors of the last row of B   show that  B      B   B     B                             B                    . The recursion  B      B     B     is the same as for the A s. The difference is in the starting values    for n      . What are the pivots ? . B  is still the same as A  except for b   .  So use linearity in the first row where        equals        minus          B                   A                              A                             A              . Linearity in row  gives  B     A   A      . . The n by n determinant C  has s above and below the main diagonal C          C                     C                                      C                                                             . a  What are the determinants of C   C   C   C  ? b  By cofactors find the relation between C  and C    and C    . Find C  . . Problem  has s just above and below the main diagonal. Going down the matrix which order of columns if any gives all s?  Explain why that permutation is even for n       ... and odd for n       ... C    odd n  C     n     ...  C      n     ...  . . Explain why this Vandermonde determinant contains x  but not x  or x   V   det       a   a  a   b   b  b   c   c  c   x   x  x       . The determinant is zero at x    and . The cofactor of x  is V    b  a  c  a  c  b  . Then V    x  a  x  b  x  c  V  .  . Compute the determinants S   S   S  of these    tridiagonal matrices S         S                     S                                      . Make a Fibonacci guess for S  and verify that you are right. . Cofactors of those    matrices give S    S     S    .  Challenge Show that S  is  the  Fibonacci  number  F     by  proving  F       F    F     .   Keep  using Fibonaccis rule F   F     F    . . Change  to  in the upper left corner of the matrices in Problem .  Why does that subtract S    from the determinant S  ? Show that the determinants become the Fibonacci numbers    always F     . Problems  are about block matrices and block determinants. . With  by  blocks you cannot always use block determinants!      A   B  D        A  D  but      A   B C   D         A  D  C  B  . a  Why is the first statement true? Somehow B doesnt enter. b  Show by example that equality fails as shown when C enters. c  Show by example that the answer det  AD  CB  is also wrong. . With block multiplication A  LU has A   L  U  in the upper left corner A   A        L      U      . a  Suppose the first three pivots of A are    . What are the determinants of L   L   L  with diagonal s U   U   U   and A   A   A  ? b  If A   A   A  have determinants    find the three pivots. . Block  elimination  subtracts CA   times  the  first  row  A  B  from  the  second  row  C  D  . This leaves the Schur complement D  CA   B in the corner  I   CA   I  A   B C   D    A B  D  CA   B  . Take determinants of these matrices to prove correct rules for square blocks      A   B C   D        A    D  CA   B          AD  CB      .   . A  by  determinant has three products down to the right and three down to the left with minus signs.  Compute the six terms in the figure to find D .  Then explain without determinants why this matrix is or is not invertible . For A  in Problem  five of the !   terms in the big formula  are nonzero. Find those five terms to show that D    . . For the  by  tridiagonal matrix entries      find the five terms in the big formula that give det A          . . Find the determinant of this cyclic P by cofactors of row .  How many exchanges reorder     into    ? Is  P      or  ? P                                                    P                                                        I I   . . A    eyen  diagonesn     diagonesn      is the      matrix.  Change A      to  so det A  .  Predict the entries of A   based on n   and test the prediction for n  . .  MATLAB  The      matrices have determinant n  .  Compute  n    A   for n   and  and verify your guess for n  .  Inverses of tridiagonal matrices have the rank form uv  above the diagonal. . All Pascal matrices have determinant . If I subtract  from the n  n entry why does the determinant become zero? Use rule  or a cofactor. det                                       known det                                       explain . . Applications of Determinants This section follows through on four major applications inverse of A solving Ax  b volumes of boxes  and pivots .  They are among the key computations in linear algebra  done by elimination. Determinants give formulas for the answers. . Computation of A   . The  by  case shows how cofactors go into A     a   b c   d      ad  bc  d  b  c a    det A  C  C  C  C   . W e are dividing by the determinant and A is invertible exactly when det A is nonzero. The number C   d is the cofactor of a . The number C    c is the cofactor of b note the minus sign. That number C  goes in row  column ! The row a  b times the column C   C  produces ad  bc . This is the cofactor expan sion of det A . That is the clue we need A   divides the cofactors by det A . Cofactor matrix C is transposed A    C  det A means  A       C  det A .  Our goal is to verify this formula for A   . We have to see why AC    det A  I     a   a   . . . . . . a    a        C   C   . . . . . . C    C         det A   . . . . . .   det A    .  With cofactors C  ... C   in the first column and not the first row they multiply a  ... a   and give the diagonal entry det A .  Every row of A multiplies its cofactors  the cofactor expansion  to give the same answer det A on the diagonal. The critical question is Why do we get zeros off the diagonal ?  If we combine the entries a   from row  with the cofactors C   for row  why is the result zero? row  of A  row  of C a  C   a  C     a   C     .  The answer is We are computing the determinant of a new matrix B  with a new row . The first row of A is copied into the second row of B .  Then B has two equal rows and det B  .  Equation  is the expansion of det B along its row  where B has exactly the same cofactors as A because the second row is thrown away to find those cofactors. The remarkable matrix multiplication  is correct. That multiplication AC    det A  I immediately gives A   . Remember that the cofac tor from deleting row i and column j of A goes into row  j and column i of C  . Dividing by the number det A if it is not zero! gives A    C   det A . Example . The inverse of a sum matrix is a difference matrix A                             has A    C  det A                   . The minus signs enter because cofactors always include        .   .  The Solution of Ax  b . The multiplication x  A   b is just C  b divided bydet A . There is a famous way in which to write the answer  x  ... x    C Cramers rule  The j th component of x  A   b is the ratio x   det B  det A  where B      a  a  b  a   . . . . . . . . . . . . a   a   b  a     has b in column j .  Proof. Expand det B  in cofactors of its j th column which is b .  Since the cofactors ignore that column det B  is exactly the j th component in the product C  b  det B   b  C    b  C      b  C  . Dividing this by det A gives x  . Each component of x is a ratio of two determinants . That fact might have been recognized from Gaussian elimination but it never was. Example . The solution of x    x     x    x    has  and  in the first column for x  and in the second column for x   x                                             x                                          . The denominators are always det A . For  equations Cramers Rule would need  determinants. To my dismay I found in a book called Mathematics for the Millions that Cramers Rule was actually recommended and elimination was thrown aside To  deal  with  a  set  involving  the  four  variables u  v  w  z   we  first  have  to eliminate one of them in each of three pairs to derive three equations in three variables and then proceed as for the threefold lefthand set to derive values for two of them.  The reader who does so as an exercise will begin to realize how formidably laborious the method of elimination becomes when we have to deal with more than three variables. This consideration invites us to explore the possibility of a speedier method ... .  The Volume of a Box. The connection between the determinant and the volume is clearest when all angles are right angles the edges are perpendicular and the box is rectangular. Then the volume is the product of the edge lengths volume         .  We want to obtain the same        from det A  when the edges of that box are the rows of A . With right angles these rows are orthogonal and AA  is diagonal Rightangled box Orthogonal rows AA      row  . . . row n      r r o o w  w  n           . . .        . The   are  the  lengths  of  the  rows  the  edges.   and  the  zeros  off  the  diagonal  come because the rows are orthogonal. Using the product and transposing rules Rightangle case            det  AA     det A  det A     det A   . The square root of this equation says that the determinant equals the volume . The sign of det A will indicate whether the edges form a righthanded set of coordinates as in the usual x  y  z system or a lefthanded system like y  x  z . If the angles are not    the volume is not the product of the lengths.  In the plane Figure . the volume of a parallelogram equals the base  times the height h  The vector b  p of length h is the second row b   a   a    minus its projection p onto the first row.  The key point is this By rule  det A is unchanged when a multiple of row  is subtracted from row . We can change the parallelogram to a rectangle  where it is already proved that volume  determinant. In n dimensions  it takes longer to make each box rectangular  but the idea is the same. The volume and determinant are unchanged if we subtract from each row its pro jection onto the space spanned by the preceding rowsleaving a perpendicular height vector like pb . This GramSchmidt process produces orthogonal rows with volume  determinant. So the same equality must have held for the original rows.                                                         This completes the link between volumes and determinants but it is worth coming back one more time to the simplest case. We know that det              det      c     .   These determinants give the volumesor areas since we are in two dimensionsdrawn in Figure .. The parallelogram has unit base and unit height its area is also .                    .  A Formula for the Pivots. We can finally discover when elimination is possible without row exchanges.   The key observation is that the first k pivots are completely determined by the submatrix A  in the upper left corner of A . The remaining rows and columns of A have no effect on this corner of the problem  Elimination on A includes elimination on A  A      e  f g   h    i          e         a f  ec   a g h i    . Certainly the first pivot depended only on the first row and column The second pivot  ad  bc   a depends only on the  by  corner submatrix A  . The rest of A does not enter until the third pivot. Actually it is not just the pivots but the entire upperleft corners of L  D  and U  that are determined by the upperleft corner of A  A  LDU      c  a           a  ad  bc   a         b  a        . What we see in the first two rows and columns is exactly the factorization of the corner submatrix A  . This is a general rule if there are no row exchanges D If A is factored into LDU  the upper left corners satisfy A   L  D  U  . For every k  the submatrix A  is going through a Gaussian elimination of its own. The  proof  is  to  see  that  this  corner  can  be  settled  first  before  even  looking  at  other eliminations. Or use the laws for block multiplication  LDU      B C      E    F  G          L  D  F BD  U  BD  F  CEG  . Comparing the last matrix with A  the corner L  D  U  coincides with A  . Then det A   det L  det D  det U   det D   d  d   d  .  The product of the first k  pivots is the determinant of A  .   This is the same rule that we know already for the whole matrix.  Since the determinant of A    will be given by d  d   d     we can isolate each pivot d  as a ratio of determinants  Formula for pivots det A  det A     d  d    d  d  d    d     d  .  In our example above  the second pivot was exactly this ratio  ad  bc   a .   It is the determinant of A  divided by the determinant of A  .  By convention det A    so that the first pivot is a    a . Multiplying together all the individual pivots we recover d  d   d   det A  det A  det A  det A    det A  det A     det A  det A   det A . From equation  we can finally read off the answer to our original question The pivot entries are all nonzero whenever the numbers det A  are all nonzero E Elimination can be completed without row exchanges so P  I and A  LU  if and only if the leading submatrices A   A  ... A  are all nonsingular. That does it for determinants  except for an optional remark on property the sign reversal on row exchanges.  The determinant of a permutation matrix P was the only questionable point in the big formula. Independent of the particular row exchanges link ing P to I  is the number of exchanges always even or always odd? If so its determinant is well defined by rule  as either   or  . Starting from         a single exchange of  and  would achieve the natural order        .  So would an exchange of  and  then  and  and then  and .  In both sequences the number of exchanges is odd.  The assertion is that an even number of exchanges can never produce the natural order beginning with        . Here is a proof.  Look at each pair of numbers in the permutation and let N count the pairs in which the larger number comes first.  Certainly N   for the natural order        . The order        has N   since all pairs             and      are wrong. We will show that every exchange alters N by an odd number .  Then to arrive at N   the natural order takes a number of exchanges having the same evenness or oddness as N . When  neighbors  are  exchanged N changes  by    or  . Any  exchange  can  be achieved by an odd number of exchanges of neighbors .  This will complete the proof an odd number of odd numbers is odd.  To exchange the first and fourth entries below which happen to be  and  we use five exchanges an odd number of neighbors                                                            . We need   k exchanges of neighbors to move the entry in place k to place  .  Then   k   exchanges move the one originally in place  and now found in place    back down to place k .  Since    k      k    is odd the proof is complete.  The determinant not only has all the properties found earlier it even exists.   Problem Set . . Find the determinant and all nine cofactors C  of this triangular matrix A                             . Form C  and verify that AC    det A  I . What is A   ? . Use the cofactor matrix C to invert these symmetric matrices A                     and B                             . . Find x  y  and z by Cramers Rule in equation  ax  by   cx  dy   and x   y  z   x  y  z    x   z   . . a  Find the determinant when a vector x replaces column j of the identity consider x    as a separate case if M          x    x    x          then det M  . b  If Ax  b  show that AM is the matrix B  in equation  with b in column j . c  Derive Cramers rule by taking determinants in AM  B  . . a  Draw  the  triangle  with  vertices A        B          and C       .   By regarding it as half of a parallelogram explain why its area equals area  ABC     det          . b  Move the third vertex to C        and justify the formula area  ABC     det    x  y   x  y   x  y         det                    .  Hint Subtracting the last row from each of the others leaves det                     det                     det          . Sketch A         B          C        and their relation to A  B  C . . Explain in terms of volumes why det  A    det A for an n by n matrix A . . Predict in advance and confirm by elimination the pivot entries of A                             and B                             . . Find all the odd permutations of the numbers          .  They come from an odd number of exchanges and lead to det P   . . Suppose the permutation P takes            to            . a  What does P  do to            ? b  What does P   do to            ? . If P is an odd permutation explain why P  is even but P   is odd. . Prove that if you keep multiplying A by the same permutation matrix P  the first row eventually comes back to its original place. . If A is a  by  matrix with all  a     then det A  . Volumes or the big formula or pivots should give some upper bound on the determinant. Problems  are about Cramers Rule for x  A   b . . Solve these linear equations by Cramers Rule x   det B   det A  a  x    x    x    x    . b  x   x    x    x   x    x    x    . . Use Cramers Rule to solve for y only. Call the  by  determinant D  a ax  by   cx  dy   . b ax  by  cz   dx  ey  f z   gx  hy  iz   .   . Cramers Rule breaks down when det A  .  Example a has no solution whereas b has infinitely many. What are the ratios x   det B   det A ? a  x    x     x    x    . parallel lines b  x    x     x    x    . same line . Quick proof of Cramers rule . The determinant is a linear function of column . It is zero if two columns are equal. When   A   x     x     x    goes into column  to produce B   the determinant is               x     x     x            x               x  det A . a  What formula for x  comes from left side  right side? b  What steps lead to the middle equation? . If the right side b is the last column of A  solve the  by  system Ax  b .  Explain how each determinant in Cramers Rule leads to your solution x . Problems  are about A    C   det A . Remember to transpose C . . Find A   from the cofactor formula C   det A . Use symmetry in part b a A                             . b A                     . . If all the cofactors are zero how do you know that A has no inverse?  If none of the cofactors are zero is A sure to be invertible? . Find the cofactors of A and multiply AC  to find det A  A                              C                     and AC   . If you change that corner entry from  to  why is det A unchanged? . Suppose det A   and you know all the cofactors. How can you find A ? . From the formula AC    det A  I show that det C   det A     . . For professors only If you know all  cofactors of a  by  invertible matrix A  how would you find A ? . If all entries of A are integers and det A   or   prove that all entries of A   are integers. Give a  by  example.  . L is lower triangular and S is symmetric. Assume they are invertible L     a     b   c  d   e    f    S     a   b   d b   c   e d   e    f    . a  Which three cofactors of L are zero? Then L   is lower triangular. b  Which three pairs of cofactors of S are equal? Then S   is symmetric. . For n   the matrix C contains cof actors and each  by  cofactor contains terms and each term needs multiplications. Compare with     for the GaussJordan computation of A   . Problems  are about area and volume by determinants. . a  Find the area of the parallelogram with edges v       and w       . b  Find the area of the triangle with sides v  w  and v  w . Draw it. c  Find the area of the triangle with sides v  w  and w  v . Draw it. . A box has edges from        to                 and        . Find its volume and also find the area of each parallelogram face. . a  The corners of a triangle are             and      . What is the area? b  A new corner at       makes it lopsided four sides. Find the area. . The parallelogram with sides      and      has the same area as the parallelogram with sides      and      . Find those areas from  by  determinants and say why they must be equal. I cant see why from a picture. Please write to me if you do. . The Hadamard matrix H has orthogonal rows. The box is a hypercube! What is det H                                           volume of a hypercube in R  ? . If the columns of a  by  matrix have lengths L   L   L   L   what is the largest possible value for the determinant based on volume?  If all entries are  or   what are those lengths and the maximum determinant? . Show by a picture how a rectangle with area x  y  minus a rectangle with area x  y  produces the area x  y   x  y  of a parallelogram. . When the edge vectors      are perpendicular the volume of the box is    times    times    . The matrix A  A is . Find det A  A and det A .   . An n dimensional cube has how many corners? How many edges? How many  n    dimensional faces? The n cube whose edges are the rows of  I has volume . A hypercube computer has parallel processors at the corners with connections along the edges. . The triangle with corners                  has area   . The pyramid with four corners                                has volume . The pyramid in R  with five corners at          and the rows of I has what volume? Problems  are about areas dA and volumes dV in calculus. . Polar coordinates satisfy x  r cos  and y  r sin  . Polar area J dr d  includes J  J        x   r  x     y   r  y               cos   r sin  sin  r cos       . The two columns are orthogonal. Their lengths are . Thus J  . . Spherical coordinates      give x   sin  cos   y   sin  sin   z   cos  . Find the Jacobian matrix of  partial derivatives  x      x      x    are in row . Simplify its determinant to J    sin  . Then dV    sin  d  d  d  . . The matrix that connects r   to x  y is in Problem . Invert that matrix J          r   x  r   y     x     y            cos  ? ? ?       ? It is surprising that  r   x   x   r . The product JJ    I gives the chain rule  x  x   x  r  r  x   x      x   . . The triangle with corners             and      has area . When you rotate it by     the area is . The rotation matrix has determinant       cos   sin  sin  cos               ? ? ?       ? . Let P           Q           and R         .   Choose S so  that PQRS is  a parallelogram and compute its area. Choose T  U  V so that OPQRST UV is a tilted box and compute its volume. . Suppose  x  y  z           and        lie on a plane through the origin. What deter minant is zero? What equation does this give for the plane? . Suppose  x  y  z  is a linear combination of        and        .  What determinant is zero? What equation does this give for the plane of all combinations?  . If Ax      ...   show how Cramers Rule gives x  first column of A   . . VISA to AVIS This takes an odd number of exchanges IVSA AVSI AVIS. Count the pairs of letters in VISA and AVIS that are reversed from alphabetical order. The difference should be odd. Re view Exercises . Find the determinants of                                                   and                                   . . If B  M   AM  why is det B  det A ? Show also that det A   B  . . Starting with A  multiply its first row by  to produce B  and subtract the first row of B from the second to produce C . How is det C related to det A ? . Solve  u   v    u   v   by Cramers rule. . If the entries of A and A   are all integers how do you know that both determinants are  or  ? Hint  What is det A times det A   ? . Find all the cofactors and the inverse or the nullspace of             cos   sin  sin  cos    and  a   b a   b  . . What is the volume of the parallelepiped with four of its vertices at                           and         ? Where are the other four vertices? . How many terms are in the expansion of a  by  determinant and how many are sure to be zero if a   ? . If P  is an even permutation matrix and P  is odd deduce from P   P   P   P    P    P  that det  P   P    . . If det A   show that A can be connected to I by a continuous chain of matrices A  t  all with positive determinants.  The straight path A  t   A  t  I  A  does go from A     A to A     I  but in between A  t  might be singular. The problem is not so easy and solutions are welcomed by the author.   . Explain why the point  x  y  is on the line through      and      if det    x   y                      or x   y     . . In analogy with the previous exercise what is the equation for  x  y  z  to be on the plane through                 and        ? It involves a  by  determinant. . If the points  x  y  z           and        lie on a plane through the origin what determinant is zero? Are the vectors                         independent? . If every row of A has either a single    or a single    or one of each and is otherwise zero show that det A   or   or . . If C      and D       then CD   DC yields  equations Ax   CD  DC      is       a c b  b a  d  b c  a  d c  c b  d           u v w z                     . a  Show that det A   if a  d  . Solve for u  v  w  z  the entries of D . b  Show that det A   if ad  bc so C is singular. In all other cases CD   DC is only possible with D  zero matrix. . The circular shift permutes     ... n  into     ...   . What is the corresponding permutation matrix P  and depending on n  what is its determinant? . Find the determinant of A  eye  ones and if possible eyen  onesn .    .    Introduction This chapter begins the second half of linear algebra.  The first half was about Ax  b .  The new problem Ax   x will still be solved by simplifying a matrixmaking it diagonal if possible. The basic step is no longer to subtract a multiple of one row from another  Elimination changes the eigenvalues which we dont want. Determinants give a transition from Ax  b to Ax   x . In both cases the determinant leads  to  a  formal  solution  to  Cramers  rule  for x  A   b   and  to  the  polynomial det  A   I    whose roots will be the eigenvalues.   All matrices are now square  the eigenvalues  of  a  rectangular  matrix  make  no  more  sense  than  its  determinant.   The determinant can actually be used if n   or . For large n  computing  is more difficult than solving Ax  b . The first step is to understand how eigenvalues can be useful One of their applications is to ordinary differential equations. We shall not assume that the reader is an expert on differential equations! If you can differentiate x   sin x  and e   you know enough. As a specific example consider the coupled pair of equations dv d t   v   w  v      at t    dw d t   v   w  w      at t   .  This is an initialvalue problem .  The unknown is specified at time t   by the given initial values  and . The problem is to find v  t  and w  t  for later times t  . It is easy to write the system in matrix form.  Let the unknown vector be u  t   with initial value u    . The coefficient matrix is A  Vector unknown u  t    v  t  w  t    u          A          . The two coupled equations become the vector equation we want Matrix form du d t  Au with u  u    at t   .    This is the basic statement of the problem.   Note that it is a firstorder equationno higher derivatives appearand it is linear in the unknowns It also has constant coeffi cients  the matrix A is independent of time. How do we find u  t  ?  If there were only one unknown instead of two that question would be easy to answer. We would have a scalar instead of a vector equation Single equation du d t  au with u  u    at t   .  The solution to this equation is the one thing you need to know Pure exponential u  t   e  u    .  At the initial time t   u equals u    because e   .  The derivative of e  has the required factor a  so that du  dt  au .  Thus the initial condition and the equation are both satisfied. Notice the behavior of u for large times.  The equation is unstable if a   neutrally stable if a   or stable if a   the factor e  approaches infinity remains bounded or goes to zero. If a were a complex number a    i   then the same tests would be applied to the real part  . The complex part produces oscillations e     cos  t  i sin  t . Decay or growth is governed by the factor e   . So much for a single equation.  We shall take a direct approach to systems and look for solutions with the same exponential dependence on t just found in the scalar case v  t   e   y w  t   e   z  or in vector notation u  t   e   x .  This is the whole key to differential equations du  dt  Au  Look for pure exponential solutions . Substituting v  e   y and w  e   z into the equation we find  e   y   e   y   e   z  e   z   e   y   e   z . The factor e   is common to every term and can be removed.  This cancellation is the reason for assuming the same exponent  for both unknowns it leaves Eigenvalue problem  y   z   y  y   z   z .  That is the eigenvalue equation. In matrix form it is Ax   x . You can see it again if we use u  e   x a number e   that grows or decays times a fixed vector x . Substituting into du  dt  Au gives  e   x  Ae   x . The cancellation of e   produces Eigenvalue equation Ax   x .   Now we have the fundamental equation of this chapter.  It involves two unknowns  and x .   It  is  an  algebra  problem  and  differential equations  can  be forgotten!   The number  lambda is an eigenvalue of the matrix A  and the vector x is the associated eigenvector . Our goal is to find the eigenvalues and eigenvectors  s and x s and to use them. The Solution of Ax   x Notice that Ax   x is a nonlinear equation  multiplies x . If we could discover   then the equation for x would be linear. In fact we could write  Ix in place of  x  and bring this term over to the left side  A   I  x   .  The identity matrix keeps matrices and vectors straight  the equation  A    x   is shorter but mixed up. This is the key to the problem The vector x is in the nullspace of A   I . The number  is chosen so that A   I has a nullspace. Of course every matrix has a nullspace.  It was ridiculous to suggest otherwise but you see  the  point.   We  want  a nonzero eigenvector x   The  vector x    always  satisfies Ax   x  but it is useless in solving differential equations. The goal is to build u  t  out of exponentials e   x  and we are interested only in those particular values  for which there is a nonzero eigenvector x . To be of any use the nullspace of A   I must contain vectors other than zero. In short A   I must be singular . F or this the determinant gives a conclusive test. A The number  is an eigenvalue of A if and only if A   I is singular det  A   I    .  This is the characteristic equation. Each  is associated with eigenvectors x   A   I  x   or Ax   x .  In our example we shift A by  I to make it singular Subtract  I A   I              . Note that  is subtracted only from the main diagonal because it multiplies I . Determinant  A   I              or       . This is the characteristic polynomial .  Its roots where the determinant is zero are the eigenvalues.  They come from the general formula for the roots of a quadratic or from   factoring into                 .  That is zero if     or    as the general formula confirms Eigenvalues    b   b    ac  a          and  . Ther e are two eigenvalues because a quadratic has two roots .  Every  by  matrix A   I has   and no higher power of   in its determinant. The values     and    lead to a solution of Ax   x or  A   I  x  . A matrix with zero determinant is singular so there must be nonzero vectors x in its nullspace. In fact the nullspace contains a whole line of eigenvectors it is a subspace!        A    I  x          y z       . The solution the first eigenvector is any nonzero multiple of x   Eigenvector for   x       . The computation for   is done separately       A    I  x          y z       . The second eigenvector is any nonzero multiple of x   Eigenvector for   x       . Y ou might notice that the columns of A    I give x   and the columns of A    I are multiples of x  . This is special and useful for  by  matrices. In the  by  case I often set a component of x equal to  and solve  A   I  x   for the other components.  Of course if x is an eigenvector then so is  x and so is  x .  All vectors in the nullspace of A   I which we call the eigenspace  will satisfy Ax   x . In our example the eigenspaces are the lines through x        and x        . Before going back to the application the differential equation  we emphasize the steps in solving Ax   x  . Compute the determinant of A   I .  With  subtracted along the diagonal this determinant is a polynomial of degree n . It starts with      . . F ind the roots of this polynomial . The n roots are the eigenvalues of A . . For each eigenvalue solve the equation  A   I  x  .  Since the determinant is zero there are solutions other than x  . Those are the eigenvectors.  In the differential equation this produces the special solutions u  e   x .  They are the pure exponential solutions to du  dt  Au . Notice e   and e    u  t   e    x   e       and u  t   e    x   e       . These two special solutions give the complete solution.  They can be multiplied by any numbers c  and c   and they can be added together.  When u  and u  satisfy the linear equation du  dt  Au  so does their sum u   u   Complete solution u  t   c  e    x   c  e    x   This is superposition  and it applies to differential equations homogeneous and linear just as it applied to matrix equations Ax  .  The nullspace is always a subspace and combinations of solutions are still solutions. Now we have two free parameters c  and c   and it is reasonable to hope that they can be chosen to satisfy the initial condition u  u    at t   Initial condition c  x   c  x   u    or           c  c        .  The constants are c    and c    and the solution to the original equation is u  t    e        e       .  Writing the two components separately we have v      and w      Solution v  t    e     e    w  t    e     e   . The key was in the eigenvalues  and eigenvectors x .  Eigenvalues are important in themselves and not just part of a trick for finding u .  Probably the homeliest example is that of soldiers going over a bridge.  Traditionally they stop marching and just walk across.  If they happen to march at a frequency equal to one of the eigenvalues of the bridge it would begin to oscillate.  Just as a childs swing does you soon notice the natural frequency of a swing and by matching it you make the swing go higher.  An engineer tries to keep the natural frequencies of his bridge or rocket away from those of the wind or the sloshing of fuel. And at the other extreme a stockbroker spends his life trying to get in line with the natural frequencies of the market.  The eigenvalues are the most important feature of practically any dynamical system. Summary and Examples T o  summarize  this  introduction  has  shown  how  and x appear  naturally  and  auto matically when solving du  dt  Au .  Such an equation has pure exponential solutions      u  e   x  the eigenvalue gives the rate of growth or decay and the eigenvector x develops at this rate. The other solutions will be mixtures of these pure solutions and the mixture is adjusted to fit the initial conditions. The key equation was Ax   x .  Most vectors x will not satisfy such an equation. They change direction when multiplied by A   so that Ax is not a multiple of x .  This means  that only  certain  special  numbers  are  eigenvalues  and  only  certain  special vectors x are eigenvectors .  We can watch the behavior of each eigenvector and then combine these normal modes to find the solution.  To say the same thing in another way the underlying matrix can be diagonalized . The diagonalization in Section . will be applied to difference equations Fibonacci numbers and Markov processes and also to differential equations.  In every example we start by computing the eigenvalues and eigenvectors  there is no shortcut to avoid that.  Symmetric matrices are especially easy.  Defective matrices lack a full set of eigenvectors so they are not diagonalizable. Certainly they have to be discussed but we will not allow them to take over the book. We start with examples of particularly good matrices. Example . Everything is clear when A is a diagonal matrix  A            has        with x               with x       . On each eigenvector A acts like a multiple of the identity Ax    x  and Ax    x  . Other vectors like x       are mixtures x    x  of the two eigenvectors and when A multiplies x  and x  it produces the eigenvalues     and     A times x    x  is     x    x       . This is Ax for a typical vector x not an eigenvector.  But the action of A is determined by its eigenvectors and eigenvalues. Example . The eigenvalues of a projection matrix are  or ! P            has        with x               with x        . W e have    when x projects to itself and    when x projects to the zero vector. The column space of P is filled with eigenvectors and so is the nullspace. If those spaces have dimension r and n  r  then    is repeated r times and    is repeated n  r times  always n  s Four eigenvalues allowing repeats P                                                    has          .  There is nothing exceptional about   .  Like every other number zero might be an eigenvalue and it might not.  If it is then its eigenvectors satisfy Ax   x .  Thus x is in the nullspace of A .  A zero eigenvalue signals that A is singular not invertible its determinant is zero. Invertible matrices have all    . Example . The eigenvalues are on the main diagonal when A is triangular  det  A   I                                                  . The determinant is just the product of the diagonal entries. It is zero if         or      the eigenvalues were already sitting along the main diagonal. This example  in which the eigenvalues can be found by inspection  points to one main theme of the chapter To transform A into a diagonal or triangular matrix without changing its eigenvalues . We emphasize once more that the Gaussian factorization A  LU is not suited to this purpose.  The eigenvalues of U may be visible on the diagonal but they are not the eigenvalues of A . For most matrices there is no doubt that the eigenvalue problem is computationally more difficult than Ax  b .  With linear systems a finite number of elimination steps produced the exact answer in a finite time. Or equivalently Cramers rule gave an exact formula for the solution.  No such formula can give the eigenvalues or Galois would turn in his grave. For a  by  matrix det  A   I  involves   . Galois and Abel proved that there can be no algebraic formula for the roots of a fifthdegree polynomial. All they will allow is a few simple checks on the eigenvalues after they have been computed and we mention two good ones sum and product . B The sum of the n eigenvalues equals the sum of the n diagonal entries Trace of A          a     a  .  Furthermore the product of the n eigenvalues equals the determinant of A . The projection matrix P had diagonal entries      and eigenvalues   .   Then      agrees with    as it should.  So does the determinant which is     .  A singular matrix with zero determinant has one or more of its eigenvalues equal to zero. There should be no confusion between the diagonal entries and the eigenvalues.  For a  triangular  matrix  they  are  the  samebut  that  is  exceptional.   Normally  the  pivots diagonal entries and eigenvalues are completely different And for a  by  matrix the trace and determinant tell us everything  a   b c   d  has trace a  d  and determinant ad  bc   det  A   I   det      a   b c d             trace    determinant The eigenvalues are   trace    trace     det      . Those two  s add up to the trace Exercise  gives     trace for all matrices. Eigshow There is a MATLAB demo just type eigshow  displaying the eigenvalue problem for a  by  matrix. It starts with the unit vector x       . The mouse makes this vector move around the unit circle . At the same time the screen shows Ax  in color and also moving. Possibly Ax is ahead of x . Possibly Ax is behind x . Sometimes Ax is parallel to x . At that parallel moment Ax   x twice in the second figure.                                                           The eigenvalue  is the length of Ax  when the unit eigenvector x is parallel.  The builtin choices for A illustrate three possibilities   or  real eigenvectors. .  There are no real eigenvectors.   Ax stays behind or ahead of x .   This means the eigenvalues and eigenvectors are complex as they are for the rotation Q . .  There is only one line of eigenvectors unusual.  The moving directions Ax and x meet but dont cross. This happens for the last  by  matrix below. .  There are eigenvectors in two independent directions. This is typical! Ax crosses x at the first eigenvector x   and it crosses back at the second eigenvector x  . Suppose A is singular rank . Its column space is a line. The vector Ax has to stay on that line while x circles around. One eigenvector x is along the line. Another eigenvector appears when Ax   . Zero is an eigenvalue of a singular matrix. You can mentally follow x and Ax for these six matrices. How many eigenvectors and where? When does Ax go clockwise instead of counterclockwise with x ? A                                                           Pr oblem Set . . Find the eigenvalues and eigenvectors of the matrix A        . Verify that the trace equals the sum of the eigenvalues and the determinant equals their product. . With the same matrix A   solve the differential equation du  dt  Au  u         . What are the two pure exponential solutions? . If we shift to A   I  what are the eigenvalues and eigenvectors and how are they related to those of A ? B  A   I           . . Solve du  dt  Pu  when P is a projection du d t            u with u         . Part of u    increases exponentially while the nullspace part stays fixed. . Find the eigenvalues and eigenvectors of A                             and B                             . Check that         equals the trace and       equals the determinant. . Gi ve an example to show that the eigenvalues can be changed when a multiple of one row is subtracted from another.  Why is a zero eigenvalue not changed by the steps of elimination? . Suppose that  is an eigenvalue of A  and x is its eigenvector Ax   x . a  Sho w that this same x is an eigenvector of B  A   I  and find the eigenvalue. This should confirm Exercise . b  Assuming     show that x is also an eigenvector of A   and find the eigen value. . Sho w that the determinant equals the product of the eigenvalues by imagining that the characteristic polynomial is factored into det  A   I                       and making a clever choice of  .   . Show that the trace equals the sum of the eigenvalues in two steps.  First find the coefficient of        on the right side of equation . Next find all the terms in det  A   I   det      a    a   a   a  a     a   . . . . . . . . . a   a    a         that involve        .  They all come from the main diagonal!  Find that coefficient of        and compare. . a  Construct  by  matrices such that the eigenvalues of AB are not the products of the eigenvalues of A and B  and the eigenvalues of A  B are not the sums of the individual eigenvalues. b  Verify however that the sum of the eigenvalues of A  B equals the sum of all the individual eigenvalues of A and B  and similarly for products.  Why is this true? . The eigenvalues of A equal the eigenvalues of A  .   This is because det  A   I  equals det  A    I  . That is true because . Show by an example that the eigen vectors of A and A  are not the same. . Find the eigenvalues and eigenvectors of A         and A   a   b b   a  . . If B has eigenvalues    C has eigenvalues    and D has eigenvalues    what are the eigenvalues of the  by  matrix A       ? . Find the rank and all four eigenvalues for both the matrix of ones and the checker board matrix A                                                    and C                                                    . Which eigenvectors correspond to nonzero eigenvalues? . What are the rank and eigenvalues when A and C in the previous exercise are n by n ? Remember that the eigenvalue    is repeated n  r times. . If A is the  by  matrix of ones find the eigenvalues and the determinant of A  I .  . Choose the third row of the companion matrix A                         so that its characteristic polynomial  A   I  is            . . Suppose A has eigenvalues    with independent eigenvectors u  v  w . a  Give a basis for the nullspace and a basis for the column space. b  Find a particular solution to Ax  v  w . Find all solutions. c  Show that Ax  u has no solution.  If it had a solution then w ould be in the column space. . The powers A  of this matrix A approaches a limit as k    A   .  .  .  .    A    .  .  .  .    and A    .  .  .  .   . The matrix A  is halfway between A and A  . Explain why A      A  A   from the eigenvalues and eigenvectors of these three matrices. . Find the eigenvalues and the eigenvectors of these two matrices A            and A  I            . A  I has the eigen vectors as A . Its eigenvalues are by . . Compute the eigenvalues and eigenvectors of A and A    A            and A                   . A   has the eigen vectors as A .  When A has eigenvalues   and    its inverse has eigenvalues . . Compute the eigenvalues and eigenvectors of A and A   A           and A           . A  has the same as A . When A has eigenvalues   and    A  has eigenvalues . . a  If you know x is an eigenvector the way to find  is to .   b  If you know  is an eigenvalue the way to find x is to . . What do you do to Ax   x  in order to prove a b and c? a   is an eigenvalue of A   as in Problem . b    is an eigenvalue of A    as in Problem . c    is an eigenvalue of A  I  as in Problem . . From the unit vector u                construct the rank projection matrix P  uu  . a  Show that Pu  u . Then u is an eigenvector with   . b  If v is perpendicular to u show that Pv  zero vector. Then   . c  Find three independent eigenvectors of P all with eigenvalue   . . Solv e det  Q   I    by the quadratic formula to reach   cos   i sin   Q   cos   sin  sin  cos   rotates the xy plane by the angle  . Find the eigenvectors of Q by solving  Q   I  x  . Use i    . . Ev ery permutation matrix leaves x      ...   unchanged. Then   . Find two more  s for these permutations P                             and P                             . . If A has     and     then det  A   I                   . Find three matrices that have trace a  d   determinant  and     . . A  by  matrix B is known to have eigenvalues    This information is enough to find three of these a  the rank of B  b  the determinant of B  B  c  the eigenvalues of B  B  and d  the eigenvalues of  B  I    . . Choose the second row of A      so that A has eigenvalues  and . . Choose a  b  c  so that det  A   I        . Then the eigenvalues are     A                   a   b   c    .  . Construct any  by  Markov matrix M   positive entries down each column add to .  If e          verify that M  e  e .  By Problem     is also an eigenvalue of M .  Challenge  A  by  singular Markov matrix with trace   has eigenvalues   . . Find three  by  matrices that have       . The trace is zero and the determi nant is zero. The matrix A might not be  but check that A   . . This matrix is singular with rank . Find three  s and three eigenvectors A                                                . . Suppose A and B ha ve the same eigenvalues   ...   with the same independent eigenvectors x  ... x  .  Then A  B . Reason  Any vector x is a combination c  x     c  x  . What is Ax ? What is Bx ? . Re view Find the eigenvalues of A  B  and C  A                              B                              and C                             . . When a  b  c  d  show that      is an eigenvector and find both eigenvalues A   a   b c   d  . . When P exchanges rows  and  and columns  and  the eigenvalues dont change. Find eigenvectors of A and PAP for    A                             and PAP                             . . Challenge problem Is there a real  by  matrix other than I  with A   I ?   Its eigenvalues must satisfy    I .  They can be e      and e       .  What trace and determinant would this give? Construct A . . There are six  by  permutation matrices P . What numbers can be the determinants of P ? What numbers can be pivots ? What numbers can be the trace of P ? What four numbers can be eigenvalues of P ?   .    Diagonalization of a Matrix We start right off with the one essential computation.  It is perfectly simple and will be used in every section of this chapter. The eigenvectors diagonalize a matrix  C Suppose the n by n matrix A has n linearly independent eigenvectors. If these eigenvectors are the columns of a matrix S  then S   AS is a diagonal matrix  . The eigenvalues of A are on the diagonal of   Diagonalization S   AS             . . .        .  W e  call S the  eigenvector  matrix  and  the  eigenvalue  matrixusing  a  capital lambda because of the small lambdas for the eigenvalues on its diagonal. Proof. Put the eigenvectors x  in the columns of S  and compute AS by columns AS  A         x  x   x                    x    x     x         . Then the trick is to split this last matrix into a quite different product S        x    x     x         x  x   x              . . .        . It is crucial to keep these matrices in the right order.  If  came before S instead of after then   would multiply the entries in the first row.  We want   to appear in the first column. As it is S  is correct. Therefore AS  S   or S   AS    or A  S  S   .  S is invertible because its columns the eigenvectors were assumed to be independent. W e add four remarks before giving any examples or applications. Remark  . If the  matrix A has  no  repeated  eigenvaluesthe  numbers   ...   are distinctthen its n eigenvectors are automatically independent see D below.  There fore any matrix with distinct eigenvalues can be diagonalized . Remark  . The diagonalizing matrix S is not unique . An eigenvector x can be multiplied by a constant and remains an eigenvector.  We can multiply the columns of S by any nonzero constants and produce a new diagonalizing S . Repeated eigenvalues leave even more freedom in S .  For the trivial example A  I  any invertible S will do S   IS is is always diagonal   is just I . All vectors are eigenvectors of the identity.  Remark  . Other matrices S will not produce a diagonal  .  Suppose the first column of S is y .  Then the first column of S  is   y .  If this is to agree with the first column of AS  which by matrix multiplication is Ay  then y must be an eigenvector Ay    y . The order of the eigenvectors in S and the eigenvalues in  is automatically the same. Remark  . Not all matrices possess n linearly independent eigenvectors so not all ma trices are diagonalizable . The standard example of a defective matrix is A            . Its eigenvalues are        since it is triangular with zeros on the diagonal det  A   I   det            . All eigenvectors of this A are multiples of the vector                 x       or x   c   .    is a double eigenvalueits algebraic multiplicity is .  But the geometric multi plicity is there is only one independent eigenvector. We cant construct S . Here is a more direct proof that this A is not diagonalizable.  Since         would have to be the zero matrix But if   S   AS   then we premultiply by S and postmultiply by S    to deduce falsely that A  . There is no invertible S . That failure of diagonalization was not a result of   . It came from       Repeated eigenvalues A            and A         . Their eigenvalues are   and  . They are not singular! The problem is the shortage of eigenvectorswhich are needed for S . That needs to be emphasized Diagonalizability of A depends on enough eigenvectors. Invertibility of A depends on nonzero eigenvalues. There is no connection between diagonalizability  n independent eigenvector and in vertibility no zero eigenvalues.  The only indication given by the eigenvalues is this Diagonalization can fail only if there are repeated eigenvalues .  Even then it does not always fail. A  I has repeated eigenvalues    ...  but it is already diagonal!  There is no shortage of eigenvectors in that case. The test is to check for an eigenvalue that is repeated p times whether there are p independent eigenvectorsin other words whether A   I has rank n  p . To complete that circle of ideas we have to show that distinct eigenvalues present no problem.   D If eigenvectors x  ... x  correspond to different eigenvalues   ...    then those eigenvectors are linearly independent. Suppose first  that k    and  that  some  combination  of x  and x  produces  zero c  x   c  x   .  Multiplying by A  we find c    x   c    x   .  Subtracting   times the previous equation the vector x  disappears c         x    . Since       and x      we are forced into c   .  Similarly c     and the two vectors are independent only the trivial combination gives zero. This same argument extends to any number of eigenvectors If some combination pro duces zero multiply by A  subtract   times the original combination and x  disappears leaving a combination of x  ... x     which produces zero. By repeating the same steps this is really mathematical induction  we end up with a multiple of x  that produces zero. This forces c    and ultimately every c   . Therefore eigenvectors that come from distinct eigenvalues are automatically independent. A matrix with n distinct eigenvalues can be diagonalized. This is the typical case. Examples of Diagonalization The main point of this section is S   AS  A .  The eigenvector matrix S converts A into its eigenvalue matrix  diagonal. We see this for projections and rotations. Example . The projection A            has eigenvalue matrix       .  The eigen vectors go into the columns of S  S         and AS  S             . That last equation can be verified at a glance. Therefore S   AS   . Example . The eigenvalues themselves are not so clear for a rotation    rotation K         has det  K   I       . Ho w can a vector be rotated and still have its direction unchanged? Apparently it cantexcept for the zero vector which is useless.  But there must be eigenvalues and we must be able to solve du  dt  Ku . The characteristic polynomial     should still have two rootsbut those roots are not real . Y ou see the way out.  The eigenvalues of K are imaginary numbers     i and     i .   The  eigenvectors  are  also  not  real.   Somehow  in  turning  through      they  are  multiplied by i or  i   K    I  x     i     i  y z       and x      i   K    I  x    i    i  y z       and x     i  . The eigenvalues are distinct even if imaginary and the eigenvectors are independent. They go into the columns of S  S      i    i  and S   KS   i    i  . We are faced with an inescapable fact that complex numbers are needed even for real matrices .  If there are too few real eigenvalues there are always n complex eigen values.  Complex includes real when the imaginary part is zero.  If there are too few eigenvectors in the real world R   or in R   we look in C  or C  . The space C  contains all column vectors with complex components and it has new definitions of length and inner product and orthogonality. But it is not more difficult than R   and in Section . we make an easy conversion to the complex case. Powers and Products A  and AB There is one more situation in which the calculations are easy. The eigenvalue of A  are exactly    ...     and every eigenvector of A is also an eigenvector of A  .  We start from Ax   x  and multiply again by A  A  x  A  x   Ax    x .  Thus   is an eigenvalue of A   with the same eigenvector x .  If the first multiplication by A leaves the direction of x unchanged then so does the second. The same result comes from diagonalization by squaring S   AS    Eigenvalues of A   S   AS  S   AS     or S   A  S    . The matrix A  is diagonalized by the same S  so the eigenvectors are unchanged.  The eigenvalues are squared. This continues to hold for any power of A  E The eigenvalues of A  are    ...     and each eigenvector of A is still an eigenvector of A  . When S diagonalizes A  it also diagonalizes A       S   AS  S   AS    S   AS   S   A  S .  Each S   cancels an S  except for the first S   and the last S .   If A is invertible this rule also applies to its inverse the power k   . The eigen values of A   are     . That can be seen even without diagonalizing if    Ax   x then    x   A   x and   x  A   x . Example . If K is rotation through    then K  is rotation through   which means  I  and K   is rotation through     K          K            and K             . The eigenvalues of K are i and  i  their squares are   and   their reciprocals are   i   i and     i   i . Then K  is a complete rotation through    K             and also     i      i               . For a product of two matrices  we can ask about the eigenvalues of AB but we wont get a good answer.  It is very tempting to try the same reasoning hoping to prove what is not in general true . If  is an eigenvalue of A and  is an eigenvalue of B  here is the false proof that AB has the eigenvalue    False proof ABx  A  x   Ax    x . The mistake lies in assuming that A and B share the same eigenvector x . In general they do not We could have two matrices with zero eigenvalues while AB has    AB                                . The eigenvectors of this A and B are completely different which is typical. For the same reason the eigenvalues of A  B generally have nothing to do with    . This false proof does suggest what is true.  If the eigenvector is the same for A and B  then the eigenvalues multiply and AB has the eigenvalue   . But there is something more important.  There is an easy way to recognize when A and B share a full set of eigenvectors and that is a key question in quantum mechanics F Diagonalizable matrices share the same eigenvector matrix S if and only if AB  BA . Proof. If the same S diagonalizes both A  S   S   and B  S   S    we can multiply in either order AB  S   S   S   S    S     S   and BA  S   S   S   S    S     S   . Since          diagonal matrices always commute we have AB  BA .  In the opposite direction suppose AB  BA . Starting from Ax   x  we have ABx  BAx  B  x   Bx . Thus x and Bx are both eigenvectors of A  sharing the same  or else Bx  .  If we assume for convenience that the eigenvalues of A are distinctthe eigenspaces are all onedimensionalthen Bx must be a multiple of x . in other words x is an eigenvector of B as well as A . The proof with repeated eigenvalues is a little longer. Heisenberg s uncertainty principle comes from noncommuting matrices like posi tion P and momentum Q .  Position is symmetric momentum is skewsymmetric and together they satisfy QP  PQ  I .  The uncertainty principle follows directly from the Schwarz inequality  Qx    Px   Qx  Px  of Section .  x    x  x  x   QP  PQ  x    Qx  Px  . The  product  of  Qx    x  and  Px    x  momentum  and  position  errors  when  the wave function is x is at least   . It is impossible to get both errors small because when you try to measure the position of a particle you change its momentum. At the end we come back to A  S  S   .  That factorization is particularly suited to take powers of A  and the simplest case A  makes the point.  The LU factorization is hopeless when squared but S  S   is perfect.  The square is S   S    and the eigenvec tors are unchanged. By following those eigenvectors we will solve difference equations and differential equations. Pr oblem Set . . Factor the following matrices into S  S    A            and A            . . Find the matrix A whose eigenvalues are  and  and whose eigenvectors are     and      respectively.  Hint  A  S  S   . . Find all the eigenvalues and eigenvectors of A                             and write two different diagonalizing matrices S . . If a  by  upper triangular matrix has diagonal entries    how do you know it can be diagonalized? What is  ?   . Which of these matrices cannot be diagonalized? A           A          A             . . a  If A   I  what are the possible eigenvalues of A ? b  If this A is  by  and not I or  I  find its trace and determinant. c  If the first row is        what is the second row? . If A       find A  by diagonalizing A . . Suppose A  uv  is a column times a row a rank matrix. a  By multiplying A times u  show that u is an eigenvector. What is  ? b  What are the other eigenvalues of A and why? c  Compute trace  A  from the sum on the diagonal and the sum of  s. . Sho w by direct calculation that AB and BA have the same trace when A   a   b c   d  and B   q   r s   t  . Deduce that AB  BA  I is impossible except in infinite dimensions. . Suppose A has eigenvalues   . What is the trace of A  ? What is the determinant of  A     ? . If the eigenvalues of A are    which of the following are certain to be true? Give a reason if true or a counterexample if false a A is invertible. b A is diagonalizable. c A is not diagonalizable. . Suppose the only eigenvectors of A are multiples of x         . True or false a A is not invertible. b A has a repeated eigenvalue. c A is not diagonalizable. . Diagonalize the matrix A      and find one of its square rootsa matrix such that R   A . How many square roots will there be? . Suppose the eigenvector matrix S has S   S   . Show that A  S  S   is symmetric and has orthogonal eigenvectors. Problems  are about the eigenvalue and eigenvector matrices.  . Factor these two matrices into A  S  S    A            and A            . . If A  S  S   then A                and A                 . . If A has     with eigenvector x       and     with x        use S  S   to find A . No other matrix has the same  s and x s. . Suppose A  S  S   . What is the eigenvalue matrix for A   I ?  What is the eigen vector matrix? Check that A   I                 . . True or false If the n columns of S eigenvectors of A  are independent then a A is invertible. b A is diagonalizable. c S is invertible. d S is diagonalizable. . If the eigenvectors of A are the columns of I  then A is a matrix. If the eigen vector matrix S is triangular then S   is triangular and A is triangular. . Describe all matrices S that diagonalize this matrix A  A            . Then describe all matrices that diagonalize A   . . Write the most general matrix that has eigenvectors     and      . . Find the eigenvalues of A and B and A  B  A             B             A  B            . Eigenvalues of A  B are equal toare not equal to eigenvalues of A plus eigenval ues of B . . Find the eigenvalues of A  B  AB  and BA  A             B             AB             and BA            . Eigenvalues of AB are equal toare not equal to eigenvalues of A times eigenvalues of B . Eigenvalues of AB areare not equal to eigenvalues of BA .   Problems  are about the diagonalizability of A . . True or false If the eigenvalues of A are    then the matrix is certainly a  invertible. b  diagonalizable. c  not diagonalizable. . If the eigenvalues of A are  and  write everything you know about the matrices A and A  . . Complete these matrices so that det A  . Then trace   and    is repeated! Find an eigcnvector with Ax   x . These matrices will nothe diagonalizabie because there is no second line of eigenvectors. A       A          and A           . . The matrix A      is  not  diagonalizable  because  the  rank  of A   I is . Change one entry to make A diagonalizable. Which entries could you change? Problems  are about powers of matrices. . A   S   S   approaches the zero matrix as k   if and only if every  has absolute value less than . Does A    or B   ? A   .  .  .  .   and B   .  .  .  .   . . Recommended Find  and S to diagonalize A in Problem .  What is the limit of   as k   ?  What is the limit of S   S   ?  In the columns of this limiting matrix you see the . . Find  and S to diagonalize B in Problem . What is B  u  for these u  ? u        u         and u       . . Diagonalize A and compute S   S   to prove this formula for A   A            has A                         . . Diagonalize B and compute S   S   to prove this formula for B   B            has B               .  Problems  are new applications of A  S  S   . . Suppose that A  S  S   . Take determinants to prove that det A          prod uct of  s. This quick proof only works when A is . . The trace of S times  S   equals the trace of  S   times S . So the trace of a diago nalizable A equals the trace of   which is . . If A  S  S    diagonalize the block matrix B        .   Find its eigenvalue and eigenvector matrices. . Consider all  by  matrices A that are diagonalized by the same fixed eigenvector matrix S .  Show that the A s form a subspace  cA and A   A  have this same S . What is this subspace when S  I ? What is its dimension? . Suppose A   A . On the left side A multiplies each column of A . Which of our four subspaces contains eigenvectors with   ? Which subspace contains eigenvectors with   ? From the dimensions of those subspaces A has a full set of independent eigenvectors and can be diagonalized. . Suppose Ax   x .  If    then x is in the nullspace.  If     then x is in the column space. Those spaces have dimensions  n  r  r  n . So why doesnt every square matrix have n linearly independent eigenvectors? . Substitute A  S  S   into the product  A    I  A    I    A    I  and explain why this produces the zero matrix .  We are substituting the matrix A for the number  in the polynomial p     det  A   I  . The CayleyHamilton Theorem says that this product is always p  A   zero matrix  even if A is not diagonalizable. . T est the CayleyHamilton Theorem on Fibonaccis matrix A      .  The theorem predicts that A   A  I   since det  A   I  is      . . If A        then det  A   I  is    a    d  . Check the CayleyHamilton state ment that  A  aI  A  dI   zero matrix . . If A       and AB  BA  show that B      is also diagonal. B has the same eigen as A  but different eigen . These diagonal matrices B form a two dimensional subspace of matrix space. AB  BA   gives four equations for the unknowns a  b  c  d find the rank of the  by  matrix. . If A is  by . then AB  BA  zero matrix gives  equations for the  entries in B . Show that the  by  matrix is singular by noticing a simple nonzero solution B . . Find the eigenvalues and eigenvectors for both of these Markov matrices A and A  . Explain why A  is close to A   A   .  .  .  .   and A                     .     .    Difference Equations and Powers A  Difference equations u     Au  move forward in a finite number of finite steps.   A differential equation takes an infinite number of infinitesimal steps but the two theories stay absolutely in parallel. It is the same analogy between the discrete and the continuous that appears over and over in mathematics.  A good illustration is compound interest when the time step gets shorter. Suppose you invest  at  interest.  Compounded once a year the principal P is multiplied by .. This is a difference equation P     AP    .  P  with a time step of one year . After  years the original P    has been multiplied  times Yearly P     .    P  which is   .       . Now suppose the time step is reduced to a month. The new difference equation is p        .     p  . After  years or  months you have  more Monthly p      .     p  which is   .       . The next step is to compound every day on     days. This only helps a little Daily compounding    .          .  . Finally  to keep their employees really moving banks offer continuous compounding . The interest is added on at every instant and the difference equation breaks down. You can hope that the treasurer does not know calculus which is all about limits as  t  . The bank could compound the interest N times a year so  t    N  Continuously    .  N      e      .  . Or the bank can switch to a differential equationthe limit of the difference equation p        .   t  p  . Moving p  to the left side and dividing by  t  Discrete to continuous p     p   t  .  p  approaches d p d t  .  p .  The solution is p  t   e    p  .  After t   years this again amounts to ..  The principal stays finite even when it is compounded every instantand the improvement over compounding every day is only four cents. Fibonacci Numbers The main object of this section is to solve u     Au  .  That leads us to A  and powers of matrices . Our second example is the famous Fibonacci sequence  Fibonacci numbers                ....  You see the pattern Every Fibonacci number is the sum of the two previous F s Fibonacci equation F     F     F  .  That is the difference equation.  It turns up in a most fantastic variety of applications and deserves a book of its own. Leaves grow in a spiral pattern and on the apple or oak you find five growths for every two turns around the stem.  The pear tree has eight for every three turns and the willow is . The champion seems to be a sunflower whose seeds chose an almost unbelievable ratio of F   F     .  How  could  we  find  the  th  Fibonacci  number  without  starting  at F     and F    and working all the way out to F  ? The goal is to solve the difference equation F     F     F  . This can be reduced to a onestep equation u     Au  . Every step multiplies u    F     F   by a matrix A  F     F     F  F     F    becomes u               F    F    Au  .  The onestep system u     Au  is easy to solve It starts from u  .  After one step it produces u   Au  .  Then u  is Au   which is A  u  . Every step brings a multiplication by A  and after k steps there are k multiplications The solution to a difference equation u     Au  is u   A  u  . The real problem is to find some quick way to compute the powers A   and thereby find the th Fibonacci number. The key lies in the eigenvalues and eigenvectors G If A can be diagonalized A  S  S    then A  comes from    u   A  u    S  S    S  S      S  S    u   S   S   u  .  The columns of S are the eigenvectors of A .  Writing S   u   c  the solution becomes u   S   c     x   x           . . .          c  . . . c      c     x     c     x  .  After k steps u  is a combination of the n pure solutions   x . These formulas give two different approaches to the same solution u   S   S   u  . The first formula recognized that A  is identical with S   S    and we could stop there.                    But the second approach brings out the analogy with a differential equation The pure exponential  solutions e    x  are  now  the  pure  powers    x  .   The  eigenvectors x  are amplified by the eigenvalues   . By combining these special solutions to match u  that is where c came fromwe recover the correct solution u   S   S   u  . In any specific example like Fibonaccis the first step is to find the eigenvalues A   I           has det  A   I         Two eigenvalues         and         . The second row of A   I is       . To get  A   I  x   the eigenvector is x        The first Fibonacci numbers F    and F    go into u   and S   u   c  S   u                 gives c                                . Those are  the  constants  in u   c     x   c     x  .   Both  eigenvectors x  and x  have second component . That leaves F   c      c     in the second component of u   Fibonacci numbers F                           . This is the answer we wanted.  The fractions and square roots look surprising because Fibonaccis rule F     F     F  must produce whole numbers Somehow that formula for F  must give an integer.  In fact since the second term              is always less than    it must just move the first term to the nearest integer F   nearest integer to            . This is an enormous number and F  will be even bigger. The fractions are becoming insignificant and the ratio F   F  must be very close to           . . Since    is insignificant compared to     the ratio F     F  approaches   . That is a typical difference equation leading to the powers of A      .  it involved   because the eigenvalues did. If we choose a matrix with     and    . we can focus on the simplicity of the computation after A has been diagonalized  A          has    and   with x        and x        A   S   S   is                                                  .  The powers   and   appear in that last matrix A   mixed in by the eigenvectors. For the difference equation u     Au   we emphasize the main point.  Every eigen vector x produces a pure solution with powers of   One solution is u   x  u    x  u     x ... When the initial u  is an eigenvector x  this is the solution u     x .  In general u  is not an eigenvector.  But if u  is a combination of eigenvectors the solution u  is the same combination of these special solutions. H If u   c  x      c  x   then after k steps u   c     x     c     x  . Choose the c s to match the starting vector u   u      x   x        c  . . . c      Sc and c  S   u  .  Markov Matrices There was an exercise in Chapter  about moving in and out of California that is worth another look. These were the rules Each year   of the people outside California move in and   of the people inside California move out. We start with y  people outside and z  inside. At the end of the first year the numbers outside and inside are y  and z   Difference equation y   .  y   .  z  z   .  y   .  z  or  y  z     .  .  .  .   y  z   . This problem and its matrix have the two essential properties of a Markov process  .  The total number of people stays fixed Each column of the Markov matrix adds up to . Nobody is gained or lost. .  The numbers outside and inside can never become negative The matrix has no negative entries . The powers A  are all nonnegative.  We solve this Markov difference equation using u   S   S   u  . Then we show that the population approaches a steady state. First A has to be diagonalized A   I   .    .  .  .     has det  A   I       .    .                    and    .   A  S  S                 .         . To find A   and the distribution after k years change S  S   to S   S     y  z    A   y  z                  .          y  z     y   z           y    z   .           . Those two terms are c     x   c     x  .  The factor      is hidden in the first term.  In the long run the other factor  .    becomes extremely small. The solution approaches a limiting state u    y   z    Steady state  y  z     y   z         . The total population is still y   z   but in the limit   of this population is outside Cali fornia and   is inside. This is true no matter what the initial distribution may have been! If the year starts with   outside and   inside then it ends the same way  .  .  .  .               . or A u   u  . The steady state is the eigenvector of A corresponding to   .  Multiplication by A  from one time step to the next leaves u  unchanged. The theory of Markov processes is illustrated by that California example I A Markov matrix A has all a    with each column adding to . a     is an eigenvalue of A . b  Its eigenvector x  is nonnegativeand it is a steady state since Ax   x  . c  The other eigenvalues satisfy     . d  If A or any power of A has all positive entries these other     are below . The solution A  u  approaches a multiple of x  which is the steady state u  . T o find the right multiple of x   use the fact that the total population stays the same.  If California started with all  million people out it ended with  million out and  million in. It ends the same way if all  million were originally inside. We note that many authors transpose the matrix so its rows add to . Remark . Our description of a Markov process was deterministic populations moved in fixed proportions. But if we look at a single individual the fractions that move become  probabilities .  With probability    an individual outside California moves in.  If inside the probability of moving out is   . The movement becomes a random process  and A is called a transition matrix . The components of u   A  u  specify the probability that the individual is outside or inside the state.  These probabilities are never negative and add to everybody has to be somewhere.  That brings us back to the two fundamental properties of a Markov matrix Each column adds to  and no entry is negative. Why is    always an eigenvalue?  Each column of A  I adds up to     . Therefore the rows of A  I add up to the zero row  they are linearly dependent  and det  A  I   . Except for very special cases u  will approach the corresponding eigenvector  .  In the formula u   c     x     c     x   no eigenvalue can be larger than . Otherwise the probabilities u  would blow up.  If all other eigenvalues are strictly smaller than     then the first term in the formula will be dominant. The other    go to zero and u   c  x   u   steady state. This is an example of one of the central themes of this chapter  Given information about A  find information about its eigenvalues. Here we found    . Stability of u     Au  There is an obvious difference between Fibonacci numbers and Markov processes. The numbers F  become larger and larger while by definition any probability is between  and . The Fibonacci equation is unstable . So is the compound interest equation P      .  P    the principal keeps growing forever.  If the Markov probabilities decreased to zero that equation would be stable but they do not since at every stage they must add to . Therefore a Markov process is neutrally stable . We want to study the behavior of u     Au  as k   .  Assuming that A can be diagonalized u  will be a combination of pure solutions Solution at time k u   S   S   u   c     x     c     x  . The growth of u  is governed by the    . Stability depends on the eigenvalues  J The difference equation u     Au  is stable if all eigenvalues satisfy       neutrally stable if some       and all the other       and unstable if at least one eigenvalue has      . In the stable case the powers A  approach zero and so does u   A  u  .                         Example . This matrix A is certainly stable A           has eigenvalues  and   . The  s are on the main diagonal because A is triangular.  Starting from any u    and following the rule u     Au   the solution must eventually approach zero u        u         u         u         u            The larger eigenvalue     go verns the decay after the first step every u  is   u    . The real effect of the first step is to split u  into the two eigenvectors of A  u             and then u                      . Positive Matrices and Applications in Economics By developing the Markov ideas we can find a small gold mine  entirely optional  of matrix applications in economics. Example   Leontiefs inputoutput matrix  . This is one of the first great successes of mathematical economics.  To illustrate it we construct a consumption matrix in which a    gives the amount of product j that is needed to create one unit of product i  A     .      .   .  .  .  .  .     . steel food labor The first question is Can we produce y  units of steel y  units of food and y  units of labor?  We must start with larger amounts p   p   p   because some part is consumed by the production itself.  The amount consumed is Ap  and it leaves a net production of p  Ap . Problem To find a vector p such that p  Ap  y  or p   I  A    y . On the surface we are only asking if I  A is invertible. But there is a nonnegative twist to the problem. Demand and production y and p  are nonnegative. Since p is    A    y  the real question is about the matrix that multiplies y  When is  I  A    a nonnegative matrix ? Roughly speaking A cannot be too large. If production consumes too much nothing is left as output. The key is in the largest eigenvalue   of A  which must be below   If      I  A    fails to be nonnegative. If      I  A    fails to exist. If      I  A    is a converging sum of nonnegative matrices Geometric series  I  A     I  A  A   A    .  The  by  example has    .  and output exceeds input. Production can go on. Those are easy to prove once we know the main fact about a nonnegative matrix like A  Not only is the largest eigenvalue   positive but so is the eigenvector x  .  Then  I  A    has the same eigenvector with eigenvalue         . If   exceeds    that  last  number  is  negative.   The  matrix  I  A    will  take  the positive vector x  to a negative vector x         .  In that case  I  A    is definitely not nonnegative.  If     then I  A is singular.  The productive case is     when the powers of A go to zero stability and the infinite series I  A  A    converges. Multiplying this series by I  A leaves the identity matrixall higher powers cancelso  I  A    is a sum of nonnegative matrices We give two examples A            has     and the economy is lost A   .       .   has      and we can produce anything. The matrices  I  A    in those two cases are         and     . Leontiefs inspiration was to find a model that uses genuine data from the real econ omy.   The table for  contained  industries in the United States  with a trans actions table of consumption and production for each one.   The theory also reaches beyond  I  A     to decide natural prices and questions of optimization.  Normally la bor is in limited supply and ought to be minimized. And of course the economy is not always linear. Example   The prices in a closed inputoutput model  . The model is called closed when everything produced is also consumed. Nothing goes outside the system.  In that case A goes back to a Markov matrix . The columns add up to .  We might be talking about the value of steel and food and labor instead of the number of units The vector p represents prices instead of production levels. Suppose p  is a vector of prices.  Then A p  multiplies prices by amounts to give the value of each product.  That is a new set of prices which the system uses for the next set of values A  p  .  The question is whether the prices approach equilibrium.  Are there prices such that p  Ap  and does the system take us there? You recognize p as the nonnegative eigenvector of the Markov matrix A  with   . It is the steady state p   and it is approached from any starting point p  . By repeating a transaction over and over the price tends to equilibrium.     The PerronFrobenius theorem gives the key properties of a positive matrix not to be confused with a positive definite matrix which is symmetric and has all its eigen values positive. Here all the entries a  are positive. K If A is a positive matrix so is its largest eigenvalue    all other     . Every component of the corresponding eigenvector x  is also positive. Proof. Suppose A  . The key idea is to look at all numbers t such that Ax  tx for some nonnegative vector x other than x  .  We are allowing inequality in Ax  tx in order to have many positive candidates t .  For the largest value t  which is attained we will show that equality holds  Ax  t  x . Otherwise if Ax  t  x is not an equality multiply by A . Because A is positive that produces a strict inequality A  x  t  Ax . Therefore the positive vector y  Ax satisfies Ay  t  y  and t  could have been larger. This contradiction forces the equality Ax  t  x  and we have an eigenvalue.  Its eigenvector x is positive not just nonnegative because on the lefthand side of that equality Ax is sure to be positive. To see that no eigenvalue can be larger than t   suppose Az   z .  Since  and z may involve negative or complex numbers we take absolute values    z    Az  A  z  by the triangle inequality. This  z  is a nonnegative vector so    is one of the possible candidates t . Therefore    cannot exceed    which was t  . Example   Von Neumanns model of an expanding economy  . We go back to the  by  matrix A that gave the consumption of steel food and labor. If the outputs are s   f      then the required inputs are u      .      .   .  .  .  .  .        s  f        Au  . In economics the difference equation is backward!  Instead of u   Au  we have u   Au  .   If A is small as it is  then production does not consume everythingand the economy can grow.  The eigenvalues of A   will govern this growth.  But again there is a nonnegative twist since steel food and labor cannot come in negative amounts.  Von Neumann asked for the maximum rate t at which the economy can expand and still stay nonnegative  meaning that u   tu   . Thus the problem requires u   tAu  . It is like the PerronFrobenius theorem with A on the other side. As before equality holds when t reaches t  which is the eigenvalue associated with the positive eigenvector of A   . In this example the expansion factor is    x           and Ax     .   .   .  .  .  .  .                   .   .   .        x . W ith steelfoodlabor in the ratio  the economy grows as quickly as possible The maximum growth rate is     .  Pr oblem Set . . Prove that every third Fibonacci number in      .    ... is even. . Bernadelli studied a beetle which lives three years only. and propagates in as third year. They survive the first year with probability    and the second with probability    and then produce six females on the way out Beetle matrix A                       . Sho w that A   I  and follow the distribution of  beetles for six years. . For the Fibonacci matrix A       compute A   A   and A  . Then use the text and a calculator to find F  . . Suppose each Gibonacci number G    is the average of the two previous numbers G    and G  . Then G        G     G    G       G       G  G     G    is  G    G       A   G    G   . a  Find the eigenvalues and eigenvectors of A . b  Find the limit as n   of the matrices A   S   S   . c  If G    and G    show that the Gibonacci numbers approach   . . Diagonalize the Fibonacci matrix by completing S                               . Do the multiplication S   S       to find its second component.  This is the k th Fi bonacci number F                    . . The numbers    and    satisfy the Fibonacci rule F     F     F                  and                . Pro ve this by using the original equation for the  s multiply it by   .  Then any combination of    and    satisfies the rule. The combination F                    gives the right start of F    and F   .     . Lucas started with L    and L   .  The rule L     L     L  is the same so A is still Fibonaccis matrix. Add its eigenvectors x   x                                             L  L   . Multiplying by A    the  second  component  is L          .   Compute  the  Lucas number L  slowly by L     L     L   and compute approximately by    . . Suppose there is an epidemic in which every month half of those who are well be come sick and a quarter of those who are sick become dead.  Find the steady state for the corresponding Markov process    d    s    w                              d  s  w     . . Write the  by  transition matrix for a chemistry course that is taught in two sections if every week   of those in Section A and   of those in Section B drop the course and   of each section transfer to the other section. . Find the limiting values of y  and   k    if y     .  y   .  z  y    z     .  y   .  z  z    . Also find formulas for y  and z  from A   S   S   . . a  From the fact that column   column   column   so the columns are linearly dependent find one eigenvalue and one eigenvector of A  A     .  .  .  .  .  .  .  .  .     . b  Find the other eigenvalues of A it is Markov. c  If u           find the limit of A  u  as k   . . Suppose there are three major centers for MoveItYourself trucks. Every month half of those in Boston and in Los Angeles go to Chicago the other half stay here they are and the trucks in Chicago are split equally between Boston and Los Angeles Set up the  by  transition matrix A  and find the steady state u  corresponding to the eigenvalue   . . a  In what range of a and b is the following equation a Markov process? u     Au    a b   a   b  u   u       .  b  Compute u   S   S   u  for any a and b . c  Under what condition on a and b does u  approach a finite limit as k    and what is the limit? Does A have to be a Markov matrix? . Multinational companies in the Americas Asia and Europe have assets of  trillion. At the start  trillion are in the Americas and  trillion in Europe. Each year   the American money stays home and   goes to each of Asia and Europe. For Asia and Europe   stays home and   is sent to the Americas. a  Find the matrix that gives    Americas Asia Europe         A    Americas Asia Europe      . b  Find the eigenvalues and eigenvectors of A . c  Find the limiting distribution of the  trillion as the world ends. d  Find the distribution of the  trillion at year k . . If A is a Markov matrix show that the sum of the components of Ax equals the sum of the components of x .  Deduce that if Ax   x with     the components of the eigenvector add to zero. . The solution to du  dt  Au        u eigenvalues i and  i  goes around in a circle u   cos t  sin t  . Suppose we approximate du  dt by forward backward and centered differences F  B  C   F  u     u   Au  or u      I  A  u  this is Eulers method.  B  u     u   Au    or u      I  A    u  backward Euler.  C  u     u     A  u     u   or u      I    A     I    A  u  . Find the eigenvalues of I  A   I  A     and  I    A     I    A  . For which difference equation does the solution u  stay on a circle? . What values of  produce instability in v       v   w    w       v   w   ? . Find the largest a  b  c for which these matrices are stable or neutrally stable  a  .  .  .     b .   .     c .  .  c  . . Multiplying term by term check that  I  A  I  A  A      I .  This series rep resents  I  A    .   It is nonnegative when A is nonnegative  provided it has a finite     sum the condition for that is    .  Add up the infinite series and confirm that it equals  I  A     for the consumption matrix A                             which has     . . F or A           find the powers A  including A   and show explicitly that their sum agrees with  I  A    . . Explain by mathematics or economics why increasing the consumption matrix A must increase t     and slow down the expansion. . What are the limits as k   the steady states of the following?  .  .  .  .          .  .  .  .          .  .  .  .    . Problems  are about A  S  S   and A   S   S   . Diagonalize A and compute S   S   to prove this formula for A   A            has A                         . . Diagonalize B and compute S   S   to prove this formula for B   B            has B               . . The eigenvalues of A are  and  the eigenvalues of B are  and  A            and B            . Find a matrix square root of A from R  S   S    Why is there no real matrix square root of B ? . If A and B have the same  s with the same full set of independent eigenvectors their factorizations into are the same. So A  B . . Suppose A and B have the same full set of eigenvectors  so that A  S   S   and B  S   S   . Prove that AB  BA . . a  When do the eigenvectors for    span the nullspace   A  ? b  When do all the eigenvectors for     span the column space   A  ?  . The powers A  approach zero if all       and they blow up if any      .  Peter Lax gives four striking examples in his book Linear Algebra . A            B          C          D     .        A      B   I C    C  D       Find the eigenvalues   e   of B and C to show that B   I and C    I . . Differential Equations and e  Wherever you find a system of equations rather than a single equation matrix theory has a part to play. For difference equations the solution u   A  u  depended on the owen of A . For differential equations the solution u  t   e  u    depends on the exponential of A . To define this exponential. and to understand it we turn right away to an example Differential equation du d t  Au          u .  The first step is always to find the eigenvalues  and   and the eigenvectors A              and A                . Then several approaches lead to u  t  . Probably the best is to match the general solution to the initial vector u    at t  . The general solution is a combination of pure exponential solutions.  These are so lutions of the special form ce   x  where  is an eigenvalue of A and x is its eigenvec tor. These pure solutions satisfy the differential equation since d  dt  ce   x   A  ce   x  . They were our introduction to eigenvalues at the start of the chapter.  In this  by  example there are two pure exponentials to be combined Solution u  t   c  e    x   c  e    x  or u         e   e     c  c   .  At time zero when the exponentials are e    u    determines c  and c   Initial condition u     c  x   c  x          c  c    Sc . You recognize S  the matrix of eigenvectors. The constants c  S   u    are the same as they were for difference equations. Substituting them back into equation  the solution     is u  t          e   e     c  c    S  e   e     S   u    .  Here is the fundamental formula of this section  Se   S   u    solves the differential equation just as S   S   u  solved the difference equation u  t   Se   S   u    with         and e     e   e     .  There are two more things to be done with this example.   One is to complete the mathematics by giving a direct definition of the exponential of a matrix .  The other is to give a physical interpretation of the equation and its solution.   It is the kind of differential equation that has useful applications. The exponential of a diagonal matrix  is easy e   just has the n numbers e   on the  diagonal.   For  a  general  matrix A   the  natural  idea  is  to  imitate  the  power  series e     x  x   !  x   !   . If we replace x by At and  by I  this sum is an n by n matrix Matrix exponential e   I  At   At   !   A t   !    .  The series always converges and its sum e  has the right properties  e   e     e          e   e     I  and d d t  e    Ae  .  From the last one u  t   e  u    solves the differential equation.  This solution must be the same as the form Se   S   u    used for computation. To prove directly that those solutions agree remember that each power  S  S     telescopes into A   S   S   be cause S   cancels S . The whole exponential is diagonalized by S  e   I  S  S   t  S   S   t  !  S   S   t  !     S  I   t    t   !    t   !     S    Se   S   . Example . In equation  the exponential of A         has         e   Se   S           e   e                 e    e    e    e    e    e    e    e     . At t   we get e   I . The infinite series e  gives the answer for all t  but a series can be hard to compute. The form Se   S   gives the same answer when A can be diagonalized it requires n independent eigenvectors in S . This simpler form leads to a combination of n exponentials e   x which is the best solution of all  L If A can be diagonalized A  S  S    then du  dt  Au has the solution u  t   e  u     Se   S   u    .  The columns of S are the eigenvectors x  ... x  of A . Multiplying gives u  t      x   x        e    . . . e       S   u     c  e    x     c  e    x   combination of e   x .  The constants c  that match the initial conditions u    are c  S   u    . This gives a complete analogy with difference equations and S  S   u  . In both cases we assumed that A could be diagonalized. since otherwise it has fewer than n eigenvec tors and we have not found enough special solutions.  The missing Solutions do exist but they are more complicated than pure exponentials e   x .  They involve generalized eigenvectors and factors like te   . To compute this defective case we can use the Jor dan form in Appendix B and find e  . The formula u  t   e  u    remains completely correct . The matrix e  is never singular .  One proof is to look at its eigenvalues if  is an eigenvalue of A  then e   is the corresponding eigenvalue of e  and e   can never be zero. Another approach is to compute the determinant of the exponential det e   e    e     e     e     .  Quick proof that e  is invertible Just recognize e   as its inverse . This invertibility is fundamental for differential equations. If n solutions are linearly independent at t   they remain linearly independent forever . If the initial vectors are v  ... v   we can put the solutions e  v into a matrix  e  v   e  v    e   v   v   . The determinant of the lefthand side is the Wronskian . It never becomes zero because it is the product of two nonzero determinants.  Both matrices on the righthand side are invertible. Remark . Not all differential equations come to us as a firstorder system du  dt  Au . We may start from a single equation of higher order like y    y    y   . To convert to a  by  system introduce v  y  and w  v  as additional unknowns along with y itself. Then these two equations combine with the original one to give u   Au  y   v v   w w    w   v or u                        y v w     Au .      v w  S  S  S  S  concentration    We are back to a firstorder system.  The problem can be solved two ways.  In a course on differential equations you would substitute y  e   into y    y    y               e     or           e     .  The three pure exponential solutions are y  e    y  e   and y  e   . No eigenvectors are involved. In a linear algebra course we find the eigenvalues of A  det  A   I             I                         .  Equations  and  are the same! The same three exponents appear       and   .  This is a general rule which makes the two methods consistent the growth rates of the solutions stay fixed when the equations change form.  It seems to us that solving the thirdorder equation is quicker. The physical significance of du  dt         u is easy to explain and at the same time genuinely important.  This differential equation describes a process of diffusion . Divide an infinite pipe into four segments Figure ..  At time t   the middle seg ments contain concentrations v    and w    of a chemical. At each time t  the diffusion rate between two adjacent segments is the difference in concentrations .  Within each segment the concentration remains uniform zero in the infinite segments. The process is continuous in time but discrete in space the unknowns are v  t  and w  t  in the two inner segments S  and S  . The concentration v  t  in S  is changing in two ways. There is diffusion into S   and into or out of S  . The net rate of change is dv  dt  and dw  dt is similar Flow rate into S  dv d t   w  v    v  Flow rate into S  dw d t     w  v  w  . This law of diffusion exactly matches our example du  dt  Au  u   v w  and du d t     v  w v   w           u . The eigenvalues   and   will govern the solution.  They give the rate at which the concentrations decay and   is the more important because only an exceptional set of  starting conditions can lead to superdecay at the rate e     In fact those conditions must come from the eigenvector       .   If the experiment admits only nonnegative concentrations superdecay is impossible and the limiting rate must be e   . The solution that decays at this slower rate corresponds to the eigenvector      .  Therefore the two concentrations will become nearly equal typical for diffusion as t   . One more comment on this example  It is a discrete approximation with only two unknowns to the continuous diffusion described by this partial differential equation Heat equation  u  t    u  x  . That heat equation is approached by dividing the pipe into smaller and smaller segments of length   N . The discrete system with N unknowns is governed by d d t      u    u                                    u    u        Au .  This is the finite difference matrix with the     pattern. The right side Au approaches the second derivative d  u  dx   after a scaling factor N  comes from the flow problem. In the limit as N     we reach the heat equation  u   t    u   x  .   Its solutions are still combinations of pure exponentials but now there are infinitely many.  Instead of eigenvectors from Ax   x  we have eigenfunctions from d  u  dx    u .  Those are u  x   sin n  x with    n    . Then the solution to the heat equation is u  t        c  e       sin n  x . The constants c  are determined by the initial condition.  The novelty is that the eigen vectors are functions u  x   because the problem is continuous and not discrete. stability of differential equations Just as for difference equations.   the eigenvalues decide how u  t  behaves as t   . As  long  as A can  be  diagonalized  there  will  be n pure  exponential  solutions  to  the differential equation and any specific solution u  t  is some combination u  t   Se   S   u   c  e    x     c  e    x  . Stability is governed by those factors e    . If they all approach zero then u  t  approaches zero  if they all stay bounded then u  t  stays bounded if one of them blows up then except for very special starting conditions the solution will blow up.  Furthermore the size of e   depends only on the real part of  . It is only the real parts of the eigenvalues that govern stability  If   a  ib  then e    e  e   e   cos bt  i sin bt  and the magnitude is  e     e  .     This decays for a   it is constant for a   and it explodes for a  . The imaginary part is producing oscillations but the amplitude comes from the real part. M The differential equation du  dt  Au is stable and e    whenever all Re     neutrally stable when all Re     and Re     and unstable and e   is unbounded if any eigenvalue has Re    . In some texts the condition Re    is called asymptotic stability because it guarantees decay for large times t . Our argument depended on having n pure exponential solutions but even if A is not diagonalizable and there are terms like te    the result is still true All solutions approach zero if and only if all eigenvalues have Re   . Stability is especially easy to decide for a  by  system which is very common in applications. The equation is du d t   a   b c   d  u . and we need to know when both eigenvalues of that matrix have negative real parts. Note again that the eigenvalues can be complex numbers. The stability tests are Re     The trace a  d must be negative. Re     The determinant ad  bc must be positive. When the eigenvalues are real those tests guarantee them to be negative. Their product is the determinant it is positive when the eigenvalues have the same sign. Their sum is the trace it is negative when both eigenvalues are negative. When the eigenvalues are a complex pair x  iy   the tests still succeed.   The trace is their sum  x which is   and the determinant is  x  iy  x  iy   x   y   . Figure . shows the one stable quadrant trace   and determinant  . It also shows the parabolic boundary line between real and complex eigenvalues.  The reason for the parabola is in the quadratic equation for the eigenvalues det  a   b c d         trace    det    .  The quadratic formula for  leads to the parabola  trace      det     and       trace    trace      det   .  Abo ve the parabola the number under the square root is negativeso  is not real. On the parabola the square root is zero and  is repeated.  Below the parabola the square roots are real. Every symmetric matrix has real eigenvalues  since if b  c  then  trace      det    a  d      ad  b     a  d     b    .                                                 For complex eigenvalues b and c have opposite signs and are sufficiently large. Example . One from each quadrant only  is stable                                      On the boundaries of the second quadrant the equation is neutrally stable. On the hori zontal axis one eigenvalue is zero because the determinant is      . On the vertical axis above the origin both eigenvalues are purely imaginary because the trace is Zero. Crossing those axes are the two ways that stability is lost. The n by n case is more difficult. A test for Re     came from Routh and Hurwitz who found a series of inequalities on the entries a  .   I do not think this approach is much good for a large matrix the computer can probably find the eigenvalues with more certainty than it can test these inequalities.   Lyapunovs idea was to find a weighting matrix W  so  that  the  weighted  length  Wu  t   is  always  decreasing .   If  there  exists such a W  then  Wu  will decrease steadily to zero and after a few ups and downs u must get there too stability.  The real value of Lyapunovs method is for a nonlinear equationthen stability can be proved without knowing a formula for u  t  . Example . du  dt        u sends u  t  around a circle starting from u          . Since trace   and det   we have purely imaginary eigenvalues                 so    i and  i . The eigenvectors are     i  and    i  . and the solution is u  t     e      i     e      i  .     That is correct but not beautiful. By substituting cos t  i sin t for e  and e    real num bers will reappear  The circling solution is u  t    cos t  sin t  . Starting from a different u      a  b   the solution u  t  ends up as u  t    a cos t  b sin t b cos t  a sin t    cos t  sin t sin t cos t  a b  .  There we have something important!  The last matrix is multiplying u     so it must be the exponential e  .  Remember that u  t   e  u    .  That matrix of cosines and sines is our leading example of an orthogonal matrix . The columns have length  their inner product is zero and we have a confirmation of a wonderful fact If A is skewsymmetric  A    A  then e  is an orthogonal matrix. A    A gi ves a conservative system. No energy is lost in damping or diffusion A    A   e     e    and  e  u       u     . That last equation expresses an essential property of orthogonal matrices.  When they multiply a vector the length is not changed.  The vector u    is just rotated and that describes the solution to du  dt  Au  It goes around in a circle . In this very unusual case e  can also be recognized directly from the infinite series. Note that A        has A    I  and use this in the series for e   I  At   At      A t                      t         t                       cos t  sin t sin t cos t  Example . The diffusion equation is stable A          has     and    . Example . If we close off the infinite segments nothing can escape du d t          u or dv  dt  w  v dw  dt  v  w . This is a continuous Markov process .  Instead of moving every year the particles move every instant.  Their total number v  w is constant.  That comes from adding the two equations on the righthand side the derivative of v  w is zero. A  discrete  Markov  matrix  has  its  column  sums  equal  to    .   A continuous Markov matrix for differential equations has its column sums equal to    . A is a discrete Markov matrix if and only if B  A  I is a continuous Markov matrix.  The     steady state for both is the eigenvector for   . It is multiplied by     in difference equations and by e     in differential equations and it doesnt move. In the example the steady state has v  w . Example  . In  nuclear  engineering  a  reactor  is  called critical when  it  is  neutrally stable the fission balances the decay. Slower fission makes it stable or subcritical  and eventually it runs down. Unstable fission is a bomb. SecondOrder Equations The laws of diffusion led to a firstorder system du  dt  Au . So do a lot of other appli cations in chemistry in biology and elsewhere but the most important law of physics does not.  It is Newtons law F  ma  and the acceleration a is a second derivative.  In ertial terms produce secondorder equations we have to solve d  u  dt   Au instead of du  dt  Au  and the goal is to understand how this switch to second derivatives alters the solution  . It is optional in linear algebra but not in physics. The comparison will be perfect if we keep the same A  d  u d t   Au          u .  Two initial conditions get the system startedthe displacement u    and the veloc ity u     . To match these conditions there will be  n pure exponential solutions. Suppose we use  rather than   and write these special solutions as u  e    x . Sub stituting this exponential into the differential equation it must satisfy d  d t   e    x   A  e    x   or    x  Ax .  The vector x must be an eigenvector of A  exactly as before . The corresponding eigen value  is  now      so  the  frequency  is  connected  to  the  decay  rate  by  the  law             .  Every special solution e   x of the firstorder equation leads to two special solutions e    x of the secondorder equation.  and the two exponents are       . This breaks down only when    which has just one square root if the eigenvector is x  the two special solutions are x and tx . F or a genuine diffusion matrix the eigenvalues  are all negative and the frequencies  are all real Pure diffusion is converted into pure oscillation . The factors e    produce neutral stability the solution neither grows or decays and the total energy stays precisely constant. It just keeps passing around the system. The general solution to d  u  dt   Au  if A has negative eigenvalues   ...   and if         is u  t    c  e      d  e      x   cdots   c  e      d  e      x  .  As always the constants are found from the initial conditions. This is easier to do at the expense of one extra formula by switching from oscillating exponentials to the more familiar sine and cosine u  t    a  cos   t  b  sin   t  x     a  cos   t  b  sin   t  x  .  The initial displacement u    is easy to keep separate t   means that sin  t   and cos  t   leaving only u     a  x     a  x   or u     Sa  or a  S   u    . Then differentiating u  t  and setting t  . the b s are determined by the initial velocity u      b    x     b    x  . Substituting the a s and b s into the formula for u  t   the equation is solved. The matrix A         has      and     . The frequencies are     and     . If the system starts from rest u       the terms in b sin  t will disappear Solution from u         u  t     cos t        cos   t      . Ph ysically two masses are connected to each other and to stationary wails by three identical springs Figure .. The first mass is held at v      the second mass is held at w      and at t   we let go.  Their motion u  t  becomes an average of two pure oscillations corresponding to the two eigenvectors.  In the first mode x         the masses move together and the spring in the middle is never stretched Figure .a. The frequency     is the same as for a single spring and a single mass. In the faster mode x         with frequency   the masses move oppositely but with equal speeds. The general solution is a combination of these two normal modes. Our particular solution is half of each. As  time  goes  on  the  motion  is  almost  periodic.   If  the  ratio      had  been  a fraction like  the masses would eventually return to u          and begin again. A combination of sin  t and sin  t would have a period of   . But   is irrational. The  best we can say is that the masses will come arbitrarily close to      and also      . Like a billiard ball bouncing forever on a perfectly smooth table the total energy is fixed. Sooner or later the masses come near any state with this energy. Again we cannot leave the problem without drawing a parallel to the continuous case. As the discrete masses and springs merge into a solid rod the second differences given by the      matrix A turn into second derivatives.  This limit is described by the celebrated wave equation   u   t     u   x  . Pr oblem Set . . Following the first example in this section  find the eigenvalues and eigenvectors and the exponential e   for A          . . For the previous matrix write the general solution to du  dt  Au  and the specific solution that matches u          .  What is the steady state as t   ?  This is a continuous Markov process    in a differential equation corresponds to    in a difference equation since e    . . Suppose the time direction is reversed to give the matrix  A  du d t          u with u       . Find u  t  and show that it blows up instead of decaying as t   .   Diffusion is irreversible and the heat equation cannot run backward. . If P is a projection matrix show from the infinite series that e   I   .  P . . A diagonal matrix like       satisfies the usual rule e        e   e    because the rule holds for each diagonal entry. a  Explain why e        e  e   using the formula e   Se   S   . b  Show that e     e  e  is not true for matrices from the example A            B         use series for e  and e  .     . The higher order equation y   y   can be written as a firstorder system by intro ducing the velocity y  as another unknown d d t  y y     y  y     y   y  . If this is du  dt  Au  what is the  by  matrix A ?  Find its eigenvalues and eigen vectors and compute the solution that starts from y      y      . . Convert y    to a firstorder system du  dt  Au  d d t  y y     y               y y   . This  by  matrix A has only one eigenvector and cannot be diagonalized. Compute e  from the series I  At   and write the solution e  u    starting from y      y      . Check that your  y  y   satisfies y   . . Suppose the rabbit population r and the wolf population w are governed by dr d t   r   w dw d t  r  w . a  Is this system stable neutrally stable or unstable? b  If initially r   and w   what are the populations at time t ? c  After a long time what is the proportion of rabbits to wolves? . Decide the stability of u   Au for the following matrices a A            . b A         . c A         . d A            . . Decide on the stability or instability of dv  dt  w  dw  dt  v .  Is there a solution that decays? . From their trace and determinant at what time t do the following matrices change between stable with real eigenvalues stable with complex eigenvalues and unstable? A       t     A         t      A    t    t  .  . Find the eigenvalues and eigenvectors for du d t  Au                     u . Why do you know  without computing  that e  will be an orthogonal matrix and  u  t     u    u    u   will be constant? . For the skewsymmetric equation du d t  Au      c  b  c  a b  a        u  u  u      a  write out u    u    u   and confirm that u   u   u   u   u   u   . b  deduce that the length u    u    u   is a constant. c  find the eigenvalues of A . The solution will rotate around the axis w   a  b  c   because Au is the cross prod uct u  w which is perpendicular to u and w . . What are the eigenvalues  and frequencies   and the general solution of the fol lowing equation? d  u d t           u . . Solve the secondorder equation d  u d t             u with u         and u          . . In most applications the secondorder equation looks like Mu   Ku   with a mass matrix multiplying the second derivatives. Substitute the pure exponential u  e    x and find the generalized eigenvalue problem that must be solved for the frequency  and the vector x . . W ith a friction matrix F in the equation u   F u   Au   substitute a pure expo nential u  e   x and find a quadratic eigenvalue problem for  . . F or equation  in the text with    and   find the motion if the first mass is hit at t   u          and u           . . Every  by  matrix with trace zero can be written as A   a b  c b  c  a  . Show that its eigenvalues are real exactly when a   b   c  .     . By backsubstitution or by computing eigenvectors solve du d t                             u with u              . . Find  s and x s so that u  e   x solves du d t            u . What combination u  c  e    x   c  e    x  starts from u           ? . Solv e Problem  for u  t    y  t   z  t  by backsubstitution First solve dz d t  z  starting from z       . Then solve dy d t   y   z  starting from y      . The solution for y will be a combination of e   and e  . . Find A to change y    y    y into a vector equation for u  t    y  t   y   t   du d t   y  y       y y    Au . What are the eigenvalues of A ? Find them also by substituting y  e   into the scalar equation y    y    y . . A door is opened between rooms that hold v      people and w      people. The movement between rooms is proportional to the difference v  w  dv d t  w  v and dw d t  v  w . Show that the total v  w is constant  people. Find the matrix in du  dt  Au  and its eigenvalues and eigenvectors. What are v and w at t  ? . Reverse the diffusion of people in Problem  to du  dt   Au  dv d t  v  w and dw d t  w  v . The total v  w still remains constant. How are the  s changed now that A is changed to  A ? But show that v  t  gro ws to infinity from v     .  . The solution to y    is a straight line y  C  Dt . Convert to a matrix equation d d t  y y              y y   has the solution  y y    e   y    y      . This matrix A cannot be diagonalized. Find A  and compute e   I  At    A  t     .   Multiply your e  times  y     y     to check the straight line y  t   y     y     t . . Substitute y  e   into y    y    y to show that    is a repeated root.  This is trouble we need a second solution after e   . The matrix equation is d d t  y y             y y   . Show that this matrix has      and only one line of eigenvectors. Trouble here too . Show that the second solution is y  te   . . Figure out how to write my   by   ky   as a vector equation Mu   Au . . a  Find two familiar functions that solve the equation d  y  dt    y .  Which one starts with y      and y      ? b  This secondorder equation y    y produces a vector equation u   Au  u   y y   du d t   y  y             y y    Au . Put y  t  from part a into u  t    y  y   . This solves Problem  again. . A particular solution to du  dt  Au  b is u   A   b  if A is invertible. The solutions to du  dt  Au give u  . Find the complete solution u   u  to a du d t   u   . b du d t            u      . . If c is not an eigenvalue of A  substitute u  e  v and find v to solve du  dt  Au  e  b .  This u  e  v is a particular solution.  How does it break down when c is an eigenvalue? . Find a matrix A to illustrate each of the unstable regions in Figure . a     and    . b     and    . c  Comple x  s with real part a  . Pr oblems  are about the matrix exponential e  .     . Write five terms of the infinite series for e  . Take the t derivative of each term. Show that you have four terms of Ae  . Conclusion e  u    solves u   Au . . The matrix B        has B   . Find e  from a short infinite series. Check that the derivative of e  is Be  . . Starting from u     the solution at time T is e  u    . Go an additional time t to reach e   e  u    .  This solution at time t  T can also be written as . Conclusion e  times e  equals . . Write A       in the form S  S   . Find e  from Se   S   . . If A   A  show that the infinite series produces e   I  e     A . For A      in Problem  this gives e   . Generally e  e  is different from e  e  . They are both different from e    . Check this using Problems  and  A            B         A  B            . . Write A      as S  S   .   Multiply Se   S   to  find  the  matrix  exponential e  . Check e   I when t  . . Put A      into the infinite series to find e  . First compute A   e               t  t                e    . . Give two reasons why the matrix exponential e  is never singular a  Write its inverse. b  Write its eigenvalues. If Ax   x then e  x  x . . Find a solution x  t   y  t  of the first system that gets large as t   .  To avoid this instability a scientist thought of exchanging the two equations! dx  dt   x   y dy  dt    x   y becomes dy  dt    x   y dx  dt   x   y . Now the matrix        is stable. It has   . Comment on this craziness. . From this general solution to du  dt  Au  find the matrix A  u  t   c  e        c  e       .  .    Complex Matrices It is no longer possible to work only with real vectors and real matrices In the first half of this book when the basic problem was Ax  b  the solution was real when A and b were real. Complex numbers could have been permitted. but would have contributed nothing new. Now we cannot avoid them. A real matrix has real coefficients in det  A   I   but the eigenvalues as in rotations may be complex. W e now introduce the space C  of vectors with n complex components. Addition and matrix multiplication follow the same rules as before. Length is computed differently . The old way the vector in C  with components    i  would have zero length    i    not good. The correct length squared is     i    . This change to  x     x        x    forces a whole series of other changes. The inner product the transpose the definitions of symmetric and orthogonal matrices all need to be modified for complex numbers.  The new definitions coincide with the old when the vectors and matrices are real.  We have listed these changes in a table at the end of the section. and we explain them as we go. That table virtually amounts to a dictionary for translating real into complex.   We hope it will be useful to the reader.  We particularly want to find out about symmetric matrices and Hermitian  matrices  Where  are  their  eigenvalues  and  what  is  special about their eigenvectors ? For practical purposes those are the most important questions in the theory of eigenvalues. We call attention in advance to the answers . Ev ery symmetric matrix and Hermitian matrix has real eigenvalues. . Its eigenvectors can be chosen to be orthonormal. Strangely  to prove that the eigenvalues are real we begin with the opposite possibility and that takes us to complex numbers complex vectors and complex matrices. Complex Numbers and Their Conjugates Probably the reader has already met complex numbers  a review is easy to give.  The important ideas are the complex conjugate   x and the absolute value  x  . Everyone knows that whatever i is it satisfies the equation i    .  It is a pure imaginary number and so are its multiples ib  b is real. The sum a  ib is a complex number and it is plotted in a natural way on the complex plane Figure .. The real numbers a and the imaginary numbers ib are special cases of complex num bers they lie on the axes. Two complex numbers are easy to add Complex addition  a  ib  c  id    a  c  i  b  d  .                                                                     Multiplying a  ib times c  id uses the rule that i     Multiplication  a  ib  c  id   ac  ibc  iad  i  bd   ac  bd  i  bc  ad  . The complex conjugate of a  ib is the number a  ib . The sign of the imaginary part is reversed .  It is the mirror image across the real axis any real number is its own conjugate since b  . The conjugate is denoted by a bar or a star  a  ib    a  ib  a  ib . It has three important properties .  The conjugate of a product equals the product of the conjugates  a  ib  c  id    ac  bd   i  bc  ad    a  ib   c  id  .  .  The conjugate of a sum equals the sum of the conjugates  a  c   i  b  d    a  c   i  b  d    a  ib    c  id  . .  Multiplying any a  ib by its conjugate a  ib produces a real number a   b   Absolute value  a  ib  a  ib   a   b   r  .  This distance r is the absolute value  a  ib    a   b  . Finally  trigonometry connects the sides a and b to the hypotenuse r by a  r cos  and b  r sin  . Combining these two equations moves us into polar coordinates Polar form a  ib  r  cos   i sin    re   .   The most important special case is when r  .  Then a  ib is e    cos   i sin  .  It falls on the unit circle in the complex plane. As  varies from  to    this number e   circles around zero at the constant radial distance  e      cos    sin    . Example . x     i times its conjugate x     i is the absolute value squared x x      i     i      x   so r   x    . To divide by    i  multiply numerator and denominator by its conjugate    i    i    i    i    i    i    i     i  . In polar coordinates multiplication and division are easy re   times Re   has absolute value rR and angle    . r e   divided by Re   has absolute value r  R and angle    . Lengths and Transposes in the Complex Case W e return to linear algebra and make the conversion from real to complex. By definition the complex vector space C  contains all vectors x with n complex components  Complex vector x       x  x  . . . x       with components x   a   ib  . Vectors x and y are still added component by component.  Scalar multiplication cx is now  done  with  complex  numbers c .   The  vectors v  ... v  are  linearly dependent if some nontrivial combination gives c  v   ...  c  v    the c  may now be complex. The unit coordinate vectors are still in C   they are still independent and they still form a basis. Therefore C  is a complex vector space of dimension n . In the new definition of length each x   is replaced by its modulus  x     Length squared  x     x        x    .  Example . x    i  and  x     y     i    i  and  y     . For real vectors there was a close connection between the length and the inner product  x    x  x .  This connection we want to preserve.  The inner product must be modified to match the new definition of length  and we conjugate the first vector in the inner product . Replacing x by x  the inner product becomes Inner product x  y  x  y      x  y  .    If we take the inner product of x      i   i  with itself we are back to  x    Length squared x  x     i    i     i   i         and  x     . Note that y  x is different from x  y  we have to watch the order of the vectors. This  leaves  only  one  more  change  in  notation  condensing  two  symbols  into  one. Instead of a bar for the conjugate and a T for the transpose those are combined into the conjugate transpose . For vectors and matrices a superscript H or a star combines both operations. This matrix A   A   A  is called  A Hermitian  A Hermitian A   A  has entries  A     A   .  You have to listen closely to distinguish that name from the phrase  A is Hermitian which means that A equals A  . If A is an m by n matrix then A  is n by m  Conjugate transpose      i  i   i            i   i    i    . This symbol A  gives official recognition to the fact that  with complex entries  it is very seldom that we want only the transpose of A . It is the conjugate transpose A  that becomes appropriate and x  is the row vector  x    x   . N .  The inner product of x and y is x  y . Orthogonal vectors have x  y  . .  The squared length of x is  x    x  x   x        x    . .  Conjugating  AB    B  A  produces  AB    B  A  . Hermitian Matrices We spoke in earlier chapters about symmetric matrices A  A  . With complex entries this idea of symmetry has to be extended. The right generalization is not to matrices that equal their transpose but to matrices that equal their conjugate transpose .  These are the Hermitian matrices and a typical example is A  Hermitian matrix A       i    i    A  .  The diagonal entries must be real  they are unchanged by conjugation. Each offdiagonal entry is matched with its mirror image across the main diagonal and    i is the conju gate of    i . In every case a   a   . Our main goal is to establish three basic properties of Hermitian matrices.   These properties apply equally well to symmetric matrices. A real symmetric matrix is cer tainly Hermitian .  For real matrices there is no difference between A  and A  . The eigenvalues of A are real as we now prove.  Property  If A  A   then for all complex vectors x  the number x  Ax is real. Every entry of A contributes to x  Ax . Try the  by  case with x   u  v   x  Ax   u v       i    i   u v    uu   vv      i  uv      i  u v  r eal  real  sum of complex conjugates  . For a proof in general.  x  Ax   is the conjugate of the  by  matrix x  Ax  but we actually get the same number back again  x  Ax    x  A  x   x  Ax . So that number must be real. Property  If A  A   every eigenvalue is real. Proof. Suppose Ax   x . The trick is to multiply by x   x  Ax   x  x .  The lefthand side is real by Property  and the righthand side x  x   x   is real and positive because x   . Therefore   x  Ax  x  x must be real. Our example has    and      A   I              i    i                     i                     .  Note . This proof of real eigenvalues looks correct for any real matrix False proof Ax   x gives x  Ax   x  x  so   x  Ax x  x is real . There must be a catch The eigenvector x might be complex .  It is when A  A  that we can be sure  and x stay real.  More than that the eigenvectors are perpendicular  x  y   in the real symmetric case and x  y   in the complex Hermitian case. Pr operty  Two eigenvectors of a real symmetric matrix or a Hermitian ma trix if they come from different eigenvalues are orthogonal to one another. The proof starts with Ax    x  Ay    y  and A  A      x   y   Ax   y  x  Ay  x     y  .  The outside numbers are   x  y    x  y  since the  s are real. Now wc use the assump tion        which forces the conclusion that x  y  . In our example  A   I  x       i    i    x  x         x      i   A  I  y       i    i   y  y         y     i    .   These two eigenvectors are orthogonal x  y        i     i      . Of course any multiples x   and y   are equally good as eigenvectors. MATLAB picks    x  and    y    so  that x   and y   are unit  vectors  the  eigenvectors are normalized to have length .  They are now orthonormal .  If these eigenvectors are chosen to be the columns of S  then we have S   AS   as always. The diagonalizing matrix can be chosen with orthonormal columns when A  A  . In case A is  real  and  symmetric  its  eigenvalues  are  real  by  Property  .   Its  unit eigenvectors are orthogonal by Property .  Those eigenvectors are also real they solve  A   I  x  . These orthonormal eigenvectors go into an orthogonal matrix Q  with Q  Q  I and Q   Q   .   Then S   AS   becomes  specialit  is Q   AQ   or A  Q  Q    Q  Q  . We can state one of the great theorems of linear algebra O A real symmetric matrix can be factored into A  Q  Q  . Its orthonormal eigenvectors are in the orthogonal matrix Q and its eigenvalues are in  . In geometry or mechanics this is the principal axis theorem . It gives the right choice of axes for an ellipse.  Those axes are perpendicular and they point along the eigen vectors of the corresponding matrix.   Section . connects symmetric matrices to n  dimensional ellipses. In mechanics the eigenvectors give the principal directions along which there is pure compression or pure tensionwith no shear. In  mathematics  the  formula A  Q  Q  is  known  as  the spectral  theorem .   If  we multiply  columns  by  rows  the  matrix A becomes  a  combination  of  onedimensional projectionswhich are the special matrices xx  of rank  multiplied by   A  Q  Q          x   x              . . .          x    . . .  x          x  x      x  x        x  x   .  Our  by  example has eigenvalues  and  Example . A                                    combination of two projections. The eigenvectors with length scaled to  are x           and x          . Then the matrices on the righthand side are x  x   and x  x   columns times rowsand they are projections onto the line through x  and the line through x  . All symmetric matrices are combinations of onedimensional projectionswhich are symmetric matrices of rank .  Remark . If A is real and its eigenvalues happen to be real then its eigenvectors are also real.  They solve  A   I  x   and can be computed by elimination.  But they will not be orthogonal unless A is symmetric A  Q  Q  leads to A   A . If A is real all complex eigenvalues come in conjugate pairs Ax   x and A x   x . If a  ib is an eigenvalue of a real matrix so is a  ib . If A  A  then b  . Strictly speaking the spectral theorem A  Q  Q  has been proved only when the eigenvalues of A are distinct.  Then there are certainly n independent eigenvectors and A can be safely diagonalized.  Nevertheless it is true see Section . that even with repeated eigenvalues a symmetric matrix still has a complete set of orthonormal eigen vectors . The extreme case is the identity matrix which has    repeated n timesand no shortage of eigenvectors. T o finish the complex case we need the analogue of a real orthogonal matrixand you can guess what happens to the requirement Q  Q  I . The transpose will be replaced by the conjugate transpose. The condition will become U  U  I . The new letter U reflects the new name A complex matrix with orthonormal columns is called a unitary matrix . Unitary Matrices May we propose two analogies? A Hermitian or symmetric matrix can be compared to a real number. A unitary or orthogonal matrix can be compared to a number on the unit circle a complex number of absolute value . The  s are real if A   A  and they are on the unit circle if U  U  I . The eigenvectors can be scaled to unit length and made orthonormal.  Those statements  are  not  yet  proved  for  unitary  including  orthogonal  matrices. Therefore we go directly to the three properties of U that correspond to the earlier Prop erties  of A . Remember that U has orthonormal columns Unitary matrix U  U  I  UU   I  and U   U   . This leads directly to Property    that multiplication by U has no effect on inner prod ucts angles or lengths. The proof is on one line just as it was for Q  Property    U x    U y   x  U  U y  x  y and lengths are preserved by U  Length unchanged  U x    x  U  U x   x   .  Property   Every eigenvalue of U has absolute value     . This follows  directly  from U x   x   by  comparing  the  lengths  of  the  two  sides  U x    x  by Property    and always   x      x  . Therefore     .            Property   Eigenvectors corresponding to different eigenvalues are orthonor mal. Start with U x    x and U y    y  and take inner products by Property    x  y   U x    U y      x      y       x  y . Comparing the left to the right       or x  y  . But Property   is       so we cannot also have      . Thus x  y   and the eigenvectors are orthogonal. Example . U   cos t  sin t sin t cos t  has eigenvalues e  and e   . The orthogonal eigenvectors are x      i  and y     i  . Remember to take conjugates in x  y    i   . After division by   the y are orthonormal. Here is the most important unitary matrix by far. Example . U    n           w  w            w     w             Fourier matrix  n . The complex number w is on the unit circle at the angle      n .  It equals e      .  Its powers are spaced evenly around the circle.  That spacing assures that the sum of all n powers of w all the n th roots of is zero. Algebraically the sum   w    w    is  w       w    . And w    is zero! row  of U  times column  of U is  n    w  w      w      w    w     . ro w i of U  times column j of U is  n    W  W      W      W    W     . In the second case W  w    .  Every entry of the original F has absolute value .  The factor  n shrinks the columns of U into unit vectors. The fundamental identity of the finite Fourier transform is U  U  I . Thus U is a unitary matrix.  Its inverse looks the same except that w is replaced by w    e     w . Since U is unitary its inverse is found by transposing which changes nothing and conjugating which changes w to w . The inverse of this U is U . U x can be computed quickly by the Fast Fourier Transform as found in Section .. By Property   of unitary matrices the length of a vector x is the same as the length of U x .  The energy in state space equals the energy in transform space.  The energy is the sum of  x     and it is also the sum of the energies in the separate frequencies.  The vector x      ...   contains equal amounts of every frequency component and its Discrete Fourier Transform U x      ...     n also has length .  Example . P                                                    . This is an orthogonal matrix so by Property   it must have orthogonal eigenvectors. They are the columns of the Fourier matrix! Its eigenvalues must have absolute value . They are the numbers   w ... w    or   i  i   i  in this  by  ease. It is a real matrix but its eigenvalues and eigenvectors are complex. One final note SkewHermitian matrices satisfy K    K  just as skewsymmetric matrices satisfy K    K . Their properties follow immediately from their close link to Hermitian matrices If A is Hermitian then K  iA is skewHermitian. The eigenvalues of K are purely imaginary instead of purely real we multiply i .  The eigenvectors are not changed. The Hermitian example on the previous pages would lead to K  iA    i    i     i  i    K  . The diagonal entries are multiples of i allowing zero.  The eigenvalues are  i and  i . The eigenvectors are still orthogonal and we still have K  U  U  with a unitary U instead of a real orthogonal Q  and with  i and  i on the diagonal of  . This section is summarized by a table of parallels between real and complex. Real versus Complex R   n real components  C   n complex components length  x    x      x    length  x     x        x    transpose A    A   Hermitian transpose A    A    AB    B  A    AB    B  A  inner product x  y  x  y     x  y   inner product x  y  x  y      x  y   Ax   y  x   A  y    Ax   y  x   A  y  orthogonality x  y    orthogonality x  y   symmetric matrices A   A  Hermitian matrices A   A A  Q  Q    Q  Q  real    A  U  U    U  U  real   skewsymmetric K    K  skewHermitian K    K orthogonal Q  Q  I or Q   Q    unitary U  U  I or U   U    Qx    Qy   x  y and  Qx    x     U x    U y   x  y and  U x    x  The columns rows and eigenvectors of Q and U are orthonormal and every        Problem Set . . For the complex numbers    i and   i  a  find their positions in the complex plane. b  find their sum and product. c  find their conjugates and their absolute values. Do the original numbers lie inside or outside the unit circle? . What can you say about a  the sum of a complex number and its conjugate? b  the conjugate of a number on the unit circle? c  the product of two numbers on the unit circle? d  the sum of two numbers on the unit circle? . If x    i and y     i  find x  x x    x  and x  y . Check that the absolute value  xy  equals  x  times  y   and the absolute value    x  equals  divided by  x  . . Find a and b for the complex numbers a  ib at the angles           on the unit circle.  Verify by direct multiplication that the square of the first is the second and the cube of the first is the third. . a  If x  r e   what are x   x    and x in polar coordinates? Where are the complex numbers that have x    x ? b  At t   the complex number e        equals one. Sketch its path in the complex plane as t increases from  to   . . Find the lengths and the inner product of x      i  i  and y      i  i  . . Write out the matrix A  and compute C  A  A if A    i  i      . What is the relation between C and C  ? Does it hold whenever C is constructed from some A  A ? . a  With the preceding A  use elimination to solve Ax  . b  Show that the nullspace you just computed is orthogonal to   A   and not to the usual row space   A   .  The four fundamental spaces in the complex case are   A  and   A  as before and then   A   and   A   .  . a  How is the determinant of A  related to the determinant of A ? b  Prove that the determinant of any Hermitian matrix is real. . a  How many degrees of freedom are there in a real symmetric matrix a real diag onal matrix and a real orthogonal matrix?  The first answer is the sum of the other two because A  Q  Q  . b  Show that  by  Hermitian matrices A and also unitary U have  real degrees of freedom columns of U can be multiplied by any e   . . Write P  Q and R in the form   x  x      x  x   of the spectral theorem P             Q           R         . . Give a reason if true or a counterexample if false a  If A is Hermitian then A  iI is invertible. b  If Q is orthogonal. then Q    I is invertible. c  If A is real then A  iI is invertible. . Suppose A is a symmetric  by  matrix with eigenvalues   . a  What properties can be guaranteed for the corresponding unit eigenvectors u  v  w ? b  In terms of u  v  w  describe the nullspace left nullspace row space and column space of A . c  Find a vector x that satisfies Ax  v  w . Is x unique? d  Under what conditions on b does Ax  b have a solution? e  If u  v  w are the columns of S  what are S   and S   AS ? . In the list below which classes of matrices contain A and which contain B ? A                                                    and B                                                    . Orthogonal invertible projection permutation Hermitian rank   diagonalizable Markov . Find the eigenvalues of A and B . . What is the dimension of the space S of all n by n real symmetric matrices?  The spectral theorem says that every symmetric matrix is a combination of n projection matrices. Since the dimension exceeds n  how is this difference explained? . Write one significant fact about the eigenvalues of each of the following.   a  A real symmetric matrix. b  A stable matrix all solutions to du  dt  Au approach zero. c  An orthogonal matrix. d  A Markov matrix. e  A defective matrix nondiagonalizable. f  A singular matrix. . Show that if U and V are unitary so is UV . Use the criterion U  U  I . . Show  that  a  unitary  matrix  has  det U     but  possibly  det U is  different  from det U  . Describe all  by  matrices that are unitary. . Find a third column so that U is unitary. How much freedom in column ? U         i         i           . . Diagonalize the  by  skewHermitian matrix K       whose entries are all   . Compute e    Se   S    and verify that e  is unitary. What is the derivative of e  at t  ? . Describe all  by  matrices that are simultaneously Hermitian unitary and diagonal. How many are there? . Every matrix Z can be split into a Hermitian and a skewHermitian part Z  A  K  just as a complex number z is split into a  ib  The real part of z is half of z  z  and the real part of Z is half of Z  Z  . Find a similar formula for the imaginary part K  and split these matrices into A  K  Z     i    i    and Z   i i  i   i  . . Show that the columns of the  by  Fourier matrix F in Example  are eigenvectors of the permutation matrix P in Example . . For the permutation of Example  write out the circulant matrix C  c  I  c  P  c  P   c  P  .   Its eigenvector matrix is again the Fourier matrix.   Write out also the four components of the matrixvector product Cx  which is the convolution of c   c   c   c   c   and x   x   x   x   x   . . For a circulant C  F  F    why is it faster to multiply by F    then   then F the convolution rule than to multiply directly by C ? . Find the lengths of u     i    i     i  and v   i  i  i  . Also find u  v and v  u .  . Prove that A  A is always a Hermitian matrix Compute A  A and AA   A   i  i  i    i  . . If Az   then A  Az  .  If A  Az   multiply by z  to prove that Az  .  The nullspaces of A and A  A are . A  A is an invertible Hermitian matrix when the nullspace of A contains only z  . . When you multiply a Hermitian matrix by a real number c  is cA still Hermitian? If c  i  show that iA is skewHermitian. The  by  Hermitian matrices are a subspace provided that the scalars are real numbers. . Which classes of matrices does P belong to orthogonal invertible Hermitian uni tary factorizable into LU  factorizable into QR ? P                             . . Compute P   P   and P  in Problem . What are the eigenvalues of P ? . Find the unit eigenvectors of P in Problem  and put them into the columns of a unitary matrix U . What property of P makes these eigenvectors orthogonal? . Write down the  by  circulant matrix C   I   P   P  .  It has the same eigen vectors as P in Problem . Find its eigenvalues. . If U is unitary and Q is a real orthogonal matrix show that U   is unitary and also U Q is unitary. Start from U  U  I and Q  Q  I . . Diagonalize A real  s and K imaginary  s to reach U  U   A      i i     K       i   i i  . Diagonalize this orthogonal matrix to reach Q  U  U  . Now all  s are  Q   cos   sin  sin  cos   . . Diagonalize this unitary matrix V to reach V  U  U  . Again all      V         i   i    .   . If v  ... v  is an orthonormal basis for C   the matrix with those columns is a matrix. Show that any vector z equals  v   z  v     v   z  v  . . The functions e   and e   are orthogonal on the interval   x    because their complex inner product is      . . The vectors v     i     w   i      and z  are an orthogonal basis for . . If A  R  iS is a Hermitian matrix are the real matrices R and S symmetric? . The complex dimension of C  is . Find a nonreal basis for C  . . Describe all  by  matrices that are Hermitian and also unitary.  Do the same for  by  matrices. . How are the eigenvalues of A  square matrix related to the eigenvalues of A ? . If u  u   show that I   uu  is Hermitian and also unitary. The rank matrix uu  is the projection onto what line in C  ? . If A  iB is a unitary matrix  A and B are real show that Q        is an orthogonal matrix. . If A  iB is a Hermitian matrix  A and B are real show that       is symmetric. . Prove that the inverse of a Hermitian matrix is again a Hermitian matrix. . Diagonalize this matrix by constructing its eigenvalue matrix  and its eigenvector matrix S  A      i   i    A  . . A matrix with orthonormal eigenvectors has the form A  U  U    U  U  . Prove that AA   A  A . These are exactly the normal matrices . . Similarity Transformations Virtually every step in this chapter has involved the combination S   AS . The eigenvec tors of A went into the columns of S  and that made S   AS a diagonal matrix called  .  When A was symmetric we wrote Q instead of S  choosing the eigenvectors to be orthonormal. In the complex case when A is Hermitian we write U it is still the matrix of eigenvectors. Now we look at all combinations M   AM  formed with any invertible M on the right and its inverse on the left . The invertible eigenvector matrix S may fail to exist the defective case or we may not know it or we may not want to use it. First a new word The matrices A and M   AM are similar. Going from one to the other is a similarity transformation .  It is the natural step for differential equations  or  matrix  powers  or  eigenvaluesjust  as  elimination  steps  were  natural  for Ax  b . Elimination multiplied A on the left by L     but not on the right by L .   So U is not similar to A  and the pivots are not the eigenvalues. A whole family of matrices M   AM is similar to A  and there are two questions .  What do these similar matrices M   AM have in common? .  With a special choice of M  what special form can be achieved by M   AM ? The final answer is given by the Jordan form  with which the chapter ends. These combinations M   AM arise in a differential or difference equation  when a change of variables u  Mv introduces the new unknown v  du d t  Au becomes M dv d t  AMv  or dv d t  M   AMv u     Au  becomes Mv     AMv   or v     M   AMv  . The new matrix in the equation is M   AM .  In the special case M  S  the system is uncoupled because   S   AS is diagonal. The eigenvectors evolve independently. This is the maximum simplification but other M s are also useful.  We try to make M   AM easier to work with than A . The family of matrices M   AM includes A itself by choosing M  I .  Any of these similar matrices can appear in the differential and difference equations by the change u  Mv  so they ought to have something in common and they do Similar matrices share the same eigenvalues . P Suppose that B  M   AM .  Then A and B have the same eigenvalues . Every eigenvector x of A corresponds to an eigenvector M   x of B . Start from Ax   x and substitute A  MBM    Same eigenvaluc MBM   x   x which is B  M   x     M   x  .  The eigenvalue of B is still  . The eigenvector has changed from x to M   x . W e can also check that A   I and B   I have the same determinant Product of matrices B   I  M   AM   I  M    A   I  M Product rule det  B   I   det M   det  A   I  det M  det  A   I  . The polynomials det  A   I  and det  B   I  are equal.  Their rootsthe eigenvalues of A and B are the same. Here are matrices B similar to A .   Example . A      has eigenvalues  and . Each B is M   AM  If M    b       then B    b       triangular with    and  . If M            then B             projection with    and  . If M   a   b c   d   then B  an arbitrary matrix with    and  . In this case we can produce any B that has the correct eigenvalues.  It is an easy case because the eigenvalues  and  are distinct.  The diagonal A was actually   the out standing member of this family of similar matrices the capo .  The Jordan form will worry about repeated eigenvalues and a possible shortage of eigenvectors.  All we say no is that every M   AM has the same number of independent eigenvectors as A each eigenvector is multiplied by M   . The  first  step  is  to  look  at  the  linear  transformations  that  lie  behind  the  matrices. Rotations reflections and projections act on n dimensional space.  The transformation can happen without linear algebra but linear algebra turns it into matrix multiplication. Change of Basis  Similarity Transformation The similar matrix B  M   AM is closely connected to A  if we go back to linear trans formations.  Remember the key idea Every linear transformation is represented by a matrix .  The matrix depends on the choice of basis! If we change the basis by M  we change the matrix A to a similar matrix B . Similar  matrices  represent  the  same  transformation T with  respect  so  different bases .  The algebra is almost straightforward.  Suppose we have a basis v  ... v  .  The j th column of A comes from applying T to v   T v   combination of the basis vectors  a   v     a  v  .  For a new basis V  ... V    the new matrix B is constructed in the same way T V   combination of the V s  b   V     b  V  .  But also each V must be a combination of the old basis vectors V    m  v  .  That matrix M is really representing the identity transformation ! when the only thing happening is the change of basis  T is I . The in verse matrix M   also represents the identity transformation. when the basis is changed from the v s back to the V s. Now the product rule gives the result we want Q The matrices A and B that represent the same linear transformation T with respect to two different bases the v s and the V s are similar   T     B    I     M    T     A  I     M .   I think an example is the best way to explain B  M   AM .  Suppose T is projection onto the line L at angle  . This linear transformation is completely described without the help of a basis. But to represent T by a matrix we do need a basis. Figure . offers two choices the standard basis v         v        and a basis V   V  chosen especially for T .                                                                              In fact T V   V  since V  is already on the line L  and T V    since V  is perpen dicular to the line. In that eigenvector basis the matrix is diagonal Elgenvector basis B   T                . The other thing is the change of basis matrix M . For that we express V  as a combination v  cos   v  sin  and  put  those  coefficients  into  column  .   Similarly V  or IV    the transformation is the identity is  v  sin   v  cos   producing column  Change of basis M   I       c  s s c  . The inverse matrix M   which is here the transpose goes from v to V . Combined with B and M  it gives the projection matrix in the standard basis of v s Standard basis A  MBM     c  cs cs   s   . We can summarize the main point. The way to simplify that matrix A in fact to diag onalize itis to find its eigenvectors. They go into the columns of M or S  and M   AM is diagonal. The algebraist says the same thing in the language of linear transformations Choose a basis consisting of eigenvectors .  The standard basis led to A  which was not simple. The right basis led to B  which was diagonal. We emphasize again that M   AM does not arise in solving Ax  b .  There the basic operation was to multiply A on the left side only! by a matrix that subtracts a multiple   of one row from another.  Such a transformation preserved the nullspace and row space of A  it normally changes the eigenvalues. Eigenvalues are actually calculated by a sequence of simple similarities . The matrix goes gradually toward a triangular form and the eigenvalues gradually appear on the main diagonal.  Such a sequence is described in Chapter .  This is much better than trying to compute det  A   I   whose roots should be the eigenvalues. For a large matrix it is numerically impossible to concentrate all that information into the polynomial and then get it out again. Triangular Forms with a Unitary M Our first move beyond the eigenvector matrix M  S is a little bit crazy  Instead of a more general M  we go the other way and restrict M to be unitary . M   AM can achieve a triangular form T under this restriction.  The columns of M  U are orthonormal in the real case we would write M  Q .  Unless the eigenvectors of  are orthogonal a diagonal U   AU is impossible.  But Schurs lemma in R is very usefulat least to the theory. The rest of this chapter is devoted more to theory than to applications. The Jordan form is independent of this triangular form. R There is a unitary matrix M  U such that U   AU  T is triangular. The eigenvalues of A appear along the diagonal of this similar matrix T . Proof. Every matrix say  by  has at least one eigenvalue   .  In the worst case it could be repeated four times. Therefore A has at least one unit eigenvector x   which we place in the first column of  U .  At this stage the other three columns are impossible to determine so we complete the matrix in any way that leaves it unitary and call it U  . The GramSchmidt process guarantees that this can be done. Ax     x  column  means that the product U    AU  starts in the right form AU   U                             leads to U    AU                              . No w work with the  by  submatrix in the lower righthand corner.  It has a unit eigenvector x   which becomes the first column of a unitary matrix M   If U                M        then U     U    AU   U                               .  At the last step an eigenvector of the  by  matrix in the lower righthand corner goes into a unitary M   which is put into the corner of U   Triangular U     U    U    AU  U   U                                 T . The product U  U  U  U  is still a unitary matrix and U   AU  T . This lemma applies to all matrices with no assumption that A is diagoalizable.  We could use it to prove that the powers A  approach zero when all        and the expo nentials e  approach zero when all Re    even without the full set of eigenvectors which was assumed in Sections . and .. Example . A         has the eigenvalue    twice. The only line of eigenvectors goes through      . After dividing by   this is the first column of U  and the triangular U   AU  T has the eigenvalues on its diagonal U   AU                                                        T .  Diagonalizing Symmetric and Hermitian Matrices This triangular form will show that any symmetric or Hermitian matrixwhether its eigenvalues are distinct or not has a complete set of orthonormal eigenvectors.  We need a unitary matrix such that U   AU is diagonal .  Schurs lemma has just found it. This triangular T must be diagonal because it is also Hermitian when A  A   T  T   U   AU    U  A   U      U   AU . The diagonal matrix U   AU represents a key theorem in linear algebra. S Spectral Theorem Every real symmetric A can be diagonalized by an orthogonal matrix Q . Every Hermitian matrix can be diagonalized by a unitary U   real  Q   AQ   or A  Q  Q   complex  U   AU   or A  U  U  The columns of Q or U  contain orthonormal eigenvectors of A . Remark  . In the real symmetric case the eigenvalues and eigenvectors are real at every step. That produces a real unitary U an orthogonal matrix.   Remark  . A is the limit of symmetric matrices with distinct eigenvalues.  As the limit approaches the eigenvectors stay perpendicular. This can fail if A   A   A         cos     sin   has eigenvectors     and  cos  sin   . As    the only eigenvector of the nondiagonalizable matrix     is     . Example . The spectral theorem says that this A  A  can be diagonalized A                             with repeated eigenvalues        and      .    has a plane of eigenvectors and we pick an orthonormal pair x  and x   x               and x            and x                for      . These are the columns of Q . Splitting A  Q  Q  into  columns times  rows gives A                                                                                                           . Since       those first two projections x  x   and x  x   each of rank  combine to give a projection P  of rank  onto the plane of eigenvectors. Then A is                               P     P                                                    .  Every Hermitian matrix with k different eigenvalues has a spectral decomposition into A    P       P   where P  is the projection onto the eigenspace for   . Since there is a full set of eigenvectors the projections add up to the identity. And since the eigenspace are orthogonal two projections produce zero P  P   . W e are very close to answering an important question so we keep going For which matrices is T   ? Symmetric skewsymmetric and orthogonal T s are all diagonal! Hermitian skewHermitian and unitary matrices are also in this class. They correspond to numbers on the real axis  the imaginary axis  and the unit circle .  Now we want the whole class corresponding to all complex numbers. The matrices are called normal. T The matrix N is normal if it commutes with N   NN   N  N .   For such matrices  and no others  the triangular T  U   NU is the diagonal  . Normal matrices are exactly those that have a complete set of orthonormal eigenvectors .  Symmetric and Hermitian matrices are certainly normal  If A  A   then AA  and A  A both equal A  .  Orthogonal and unitary matrices are also normal UU  and U  U both equal I . Two steps will work for any normal matrix .  If N is normal then so is the triangular T  U   NU  T T   U   NUU  N  U  U   NN  U  U   N  NU  U  N  UU   NU  T  T . .  A triangular T that is normal must be diagonal! See Problems  at the end of this section. Thus if N is normal the triangular T  U   NU must be diagonal. Since T has the same eigenvalues as N  it must be  .  The eigenvectors of N are the columns of U  and they are orthonormal.  That is the good case.  We turn now from the best possible matrices  normal  to the worst possible  defective . Normal N           Defective A            . The Jordan Form This  section  has  done  its  best  while  requiring M to  be  a  unitary  matrix U .   We  got M   AM into a triangular form T .   Now we lift this restriction on M .   Any matrix is allowed and the goal is to make M   AM as nearly diagonal as possible . The result of this supreme effort at diagonalization is the Jordan form J . If A has a full set of eigenvectors we take M  S and arrive at J  S   AS   . Then the Jordan form coincides with the diagonal  .  This is impossible for a defective nondiagonalizable matrix. For every missing eigenvector the Jordan form will have a  just above its main diagonal .  The eigenvalues appear on the diagonal because J is triangular.  And distinct eigenvalues can always be decoupled. It is only a repeated  that may or may not! require an offdiagonal  in J . U If A has s independent eigenvectors it is similar to a matrix with s blocks Jordan form J  M   AM     J  . . . J     .  Each Jordan block J  is a triangular matrix that has only a single eigenvalue   and only one eigenvector Jordan block J                       .    The same   will appear in several blocks if it has several independent eigen vectors.  Two matrices are similar if and only if they share the same Jordan form J . Man y  authors  have  made  this  theorem  the  climax  of  their  linear  algebra  course. Frankly I think that is a mistake. It is certainly true that not all matrices are diagonaliz able and the Jordan form is the most general case. For that very reason its construction is both technical and extremely unstable.   A slight change in A can put back all the missing eigenvectors and remove the offdiagonal is. Therefore the right place for the details is in the appendix and the best way to start on the Jordan form is to look at some specific and manageable examples. Example . T            and A         and B            all lead to J            . These four matrices have eigenvalues  and  with only one eigenvector so J con sists of one block .  We now check that.  The determinants all equal .  The traces the sums down the main diagonal are . The eigenvalues satisfy      and     . For T  B  and J  which are triangular the eigenvalues are on the diagonal. We want to show that these matrices are similar they all belong to the same family. T From T to J  the job is to change  to . and a diagonal M will do it M   T M                                       J . B From B to J  the job is to transpose the matrix. A permutation does that P   BP                                          J . A From A to J  we go first to T as in equation . Then change  to  U   AU             T and then M   T M             J . Example . A                             and B                             . Zero is a triple eigenvalue for A and B  so it will appear in all their Jordan blocks. There can be a single  by  block or a  by  and a  by I block or three I by I blocks. Then A and B have three possible Jordan forms J                               J                         J                      .   The only eigenvector of A is        . Its Jordan form has only one block and A must be similar to J  .  The matrix B has the additional eigenvector         and its Jordan form is J  with two blocks As for J   zero matrix  it is in a family by itself the only matrix similar to J  is M    M  .  A count of the eigenvectors will determine J when there is nothing more complicated than a triple eigenvalue. Example . Application to difference and differential equations  powers and expo nentials . If A can be diagonalized the powers of A  S  S   are easy A   S   S   . In every case we have Jordans similarity A  MJM    so now we need the powers of J  A    MJM    MJM      MJM     MJ  M   . J is blockdiagonal and the powers of each block can be taken separately  J                                 k       k  k           k            .  This block J  will  enter  when  is  a  triple  eigenvalue  with  a  single  eigenvector.   Its exponential is in the solution to the corresponding differential equation Exponential e        e   te     t  e    e   te     e      .  Here I  J  t   J  t    !   produces    t    t   !    e   on the diagonal. The third column of this exponential comes directly from solving du  dt  J  u  d d t    u  u  u                              u  u  u     starting from u            . This can be solved by backsubstitution since J  is triangular. The last equation du   dt   u  yields u   e   .  The equation for u  is du   dt   u   u   and its solution is te   . The top equation is du   dt   u   u   and its solution is   t  e   .  When  has multi plicity m with only one eigenvector the extra factor t appears m   times. These powers and exponentials of J are a part of the solutions u  and u  t  . The other part is the M that connects the original A to the more convenient matrix J  if u     Au  then u   A  u   MJ  M   u  if du  dt  Au then u  t   e  u     Me  M   u    . When M and J are S and  the diagonalizable case those are the formulas of Sections .  and  ..   Appendix  B  returns  to  the  nondiagonalizable  case  and  shows  how  the Jordan form can be reached. I hope the following table will be a convenient summary.   Similarity Transformations . A is diagonalizable  The columns of S are eigenvectors and S   AS   . . A is arbitrary  The columns of M include generalized eigenvectors of A  and the Jordan form M   AM  J is block diagonal . . A is arbitrary  The unitary U can be chosen so that U   AU  T is triangular . . A is normal  AA   A  A  then U can be chosen so that U   AU   . Special cases of normal matrices all with orthonormal eigenvectors a  If A  A  is Hermitian then all   are real. b  If A  A  is real symmetric then  is real and U  Q is orthogonal. c  If A   A  is skewHermitian then all   are purely imaginary. d  If A is orthogonal or unitary then all       are on the unit circle. Pr oblem Set . . If B is similar to A and C is similar to B  show that C is similar to A . Let B  M   AM and C  N   BN . Which matrices are similar to I ? . Describe in words all matrices that are similar to        and find two of them. . Explain why A is never similar to A  I . . Find a diagonal M  made up of s and  s to show that A                                  is similar to B                            . . Show if B is invertible that BA is similar to AB . . a  If CD   DC and D is invertible show that C is similar to  C . b  Deduce that the eigenvalues of C must come in plusminus pairs. c  Show directly that if Cx   x  then C  Dx      Dx  . . Consider any A and a Givens rotation M in the  plane A     a   b   c d   e    f g   h    i     M     cos   sin   sin  cos         . Choose the rotation angle  to produce zero in the      entry of M   AM .  Note . This zeroing is not so easy to continue because the rotations that produce zero in place of d and h will spoil the new zero in the corner.  We have to leave one diagonal below the main one and finish the eigenvalue calculation in a different way. Otherwise if we could make A diagonal and see its eigenvalues we would be finding the roots of the polynomial det  A   I  by using only the square roots that determine cos  and that is impossible. . What matrix M changes the basis V         V        to the basis v         v        ?  The columns of M come from expressing V  and V  as combinations  m  v  of the v s. . For the same two bases express the vector      as a combination c  V   c  V  and also as d  v   d  v  . Check numerically that M connects c to d  Mc  d . . Confirm  the  last  exercise   If V   m  v   m  v  and V   m  v   m  v    and m  c   m  c   d  and m  c   m  c   d   the vectors c  V   c  V  and d  v   d  v  are the same. This is the change of basis formula Mc  d . . If the transformation T is a reflection across the   line in the plane find its matrix with respect to the standard basis v         v         and also with respect to V         V         . Show that those matrices are similar. . The identity  transformation takes  every  vector  to  itself T x  x .   Find  the  corre sponding matrix if the first basis is v         v        and the second basis is w         w        . It is not the identity matrix! . The derivative of a  bx  cx  is b   cx   x  . a  Write the  by  matrix D such that D    a b c        b  c     . b  Compute D  and interpret the results in terms of derivatives. c  What are the eigenvalues and eigenvectors of D ? . Show that every number is an eigenvalue for T f  x   d f  dx  but the transformation T f  x      f  t  dt has no eigenvalues here    x   . . On the space of  by  matrices let T be the transformation that transposes every matrix . Find the eigenvalues and eigenmatrices for A    A . . a  Find an orthogonal Q so that Q   AQ   if A                             and                              .   Then find a second pair of orthonormal eigenvectors x   x  for   . b  V erify that P  x  x    x  x   is the same for both pairs. . Prove that every unitary matrix A is diagonalizable in two steps i  If A is unitary and U is too then so is T  U   AU . ii  An upper triangular T that is unitary must be diagonal. Thus T   . Any unitary matrix A distinct eigenvalues or not has a complete set of orthonormal eigenvectors. All eigenvalues satisfy     . . Find a normal matrix  NN   N  N  that is not Hermitian skewHermitian unitary or diagonal. Show that all permutation matrices are normal. . Suppose T is a  by  upper triangular matrix with entries t  . Compare the entries of T T  and T  T  and show that if they are equal then T must be diagonal. All normal triangular matrices are diagonal. . If N is normal show that  Nx    N  x  for every vector x . Deduce that the i th row of N has the same length as the i th column. Note  If N is also upper triangular this leads again to the conclusion that it must be diagonal. . Prove that a matrix with orthonormal eigenvectors must be normal as claimed in T  If U   NU  A  or N  U  U   then NN   N  N . . Find a unitary U and triangular T so that U   AU  T  for A          and A                             . . If A has eigenvalues    what are the eigenvalues of A  A  I  A   I  ? . a  Show by direct multiplication that every triangular matrix T  say  by  satisfies its own characteristic equation  T    I  T    I  T    I   . b  Substituting U   A U for T  deduce the famous CayleyHamilton theorem Every matrix satisfies its own characteristic equation . For  by  this is  A    I  A    I  A    I   . . The characteristic polynomial of A      is     a  d    ad  bc  .  By direct substitution verify CayleyHamilton A    a  d  A  ad  bc  I  . . If a     above the main diagonal and a    elsewhere find the Jordan form say  by  by finding all the eigenvectors. . Show by trying for an M and failing that no two of the three Jordan forms in equa tion  are similar J    M   J  M  J    M   J  M  and J    M   J  M .  . Solve u   Ju by backsubstitution solving first for u   t   du d t  Ju           u  u   with initial value u         . Notice te   in the first component u   t  . . Compute A  and e  if A  MJM    A                                     . . Show that A and B are similar by finding M so that B  M   AM  a A            and B            . b A            and B          . c A            and B            . . Which of these matrices A  to A  are similar? Check their eigenvalues.                                                             . . There are sixteen  by  matrices whose entries are s and s.  Similar matrices go into the same family.  How many families?  How many matrices total  in each family? . a  If x is in the nullspace of A  show that M   x is in the nullspace of M   AM . b  The nullspaces of A and M   AM have the same vectorsbasisdimension. . If A and B have the exactly the same eigenvalues and eigenvectors does A  B ? With n independent eigenvectors we do have A  B . Find A   B when      repeated but there is only one line of eigenvectors  x     . Pr oblems  are about the Jordan form. . By direct multiplication find J  and J  when J   c   c  . Guess the form of J  . Set k   to find J  . Set k    to find J   .   . If J is the  by  Jordan block with    find J  and count its eigenvectors and find its Jordan form two blocks. . The text solved du  dt  Ju for a  by  Jordan block J .   Add a fourth equation dw  dt   w  x . Follow the pattern of solutions for z  y  x to find w . . These Jordan matrices have eigenvalues    . They have two eigenvectors find them. But the block sizes dont match and J is not similar to K  J                                  and K                                      . F or any matrix M   compare JM with MK .   If they are equal  show that M is not invertible. Then M   JM  K is impossible. . Prove in three steps that A  is always similar to A we know that the  s are the same the eigenvectors are the problem a  F or A  one block find M   permutation so that M    J  M   J   . b  For A  any J  build M  from blocks so that M    JM   J  . c  For any A  MJM    Show that A  is similar to J  and so to J and to A . . Which pairs are similar? Choose a  b  c  d to prove that the other pairs arent  a   b c   d    b   a d   c    c   d a   b    d   c b   a  . . True or false with a good reason a  An invertible matrix cant be similar to a singular matrix. b  A symmetric matrix cant be similar to a nonsymmetric matrix. c A cant be similar to  A unless A  . d A  I cant be similar to A  I . . Prove that AB has the same eigenvalues as BA . . If A is  by  and B is  by  AB and BA have different sizes. Nevertheless  I  A  I  AB  B   I    A  I      B   BA   G . a  What sizes are the blocks of G ? They are the same in each matrix. b  This equation is M   F M  G  so F and G have the same  eigenvalues. F has the eigenvalues of AB plus  zeros G has the eigenvalues of BA plus  zeros. AB has the same eigenvalues as BA plus zeros.  . Why is each of these statements true? a  If A is similar to B  then A  is similar to B  . b A  and B  can be similar when A and B are not similar try     . c      is similar to     . d     is not similar to     . e  If we exchange rows  and  of A   and then exchange columns  and  the eigenvalues stay the same . Properties of Eigenvalues and Eigenvectors How are the properties of a matrix reflected in its eigenvalues and eigenvectors?  This question is fundamental throughout Chapter . A table that organizes the key facts may be helpful. For each class of matrices here are the special properties of the eigenvalues   and eigenvectors x  . Symmetric A   A real  s orthogonal x   x    Orthogonal Q   Q   all      orthogonal x   x    Sk ewsymmetric A    A imaginary  s orthogonal x   x    Complex Hermitian A   A real  s orthogonal x   x    P ositive definite x  Ax   all    orthogonal Similar matrix B  M   AM   B     A  x  B   M   x  A  Projection P  P   P      column space nullspace Reflection I   uu       ...  u  u  Rank matrix uv    v  u   ...  u  v  Inverse A       A  eigenvectors of A Shift A  cI   A  c eigenvectors of A Stable powers A    all      Stable exponential e    all Re    Markov m         m        steady state x   Cyclic permutation P   I    e      x        ...       Diagonalizable S  S   diagonal of  columns of S are independent Symmetric Q  Q  diagonal of  real    columns of Q are orthonormal Jordan J  M   AM diagonal of J each block gives  eigenvector Every matrix A  U  V  rank  A   rank    eigenvectors of A  A  AA  in V  U   Review Exercises . Find the eigenvalues and eigenvectors and the diagonalizing matrix S  for A            and B          . . Find the determinants of A and A   if A  S         S   . . If A has eigenvalues  and  corresponding to the eigenvectors     and       how can you tell in advance that A is symmetric?  What are its trace and determi nant? What is A ? . In the previous problem what will be the eigenvalues and eigenvectors of A  ? What is the relation of A  to A ? . Does there exist a matrix A such that the entire family A  cI is invertible for all complex numbers c ? Find a real matrix with A  rI invertible for all real r . . Solve for both initial values and then find e   du d t            u if u         and if u         . . Would you prefer to have interest compounded quarterly at  per year or annu ally at ? . True or false with counterexample if false a  If B is formed from A by exchanging two rows then B is similar to A . b  If a triangular matrix is similar to a diagonal matrix it is already diagonal. c  Any two of these statements imply the third A is Hermitian A is unitary A   I . d  If A and B are diagonalizable so is AB . . What happens to the Fibonacci sequence if we go backward in time and how is F   related to F  ? The law F     F     F  is still in force so F    . . Find the general solution to du  dt  Au if A                   .  Can you find a time T at which the solution u  T  is guaranteed to return to the initial value u    ? . If P is the matrix that projects R  onto a subspace S  explain why every vector in S is an eigenvector and so is every vector in S  .  What are the eigenvai Note the connection to P   P  which means that     . . Sho w that every matrix of order   is the sum of two singular matrices. . a  Show that the matrix differential equation dX  dt  AX  X B has the solution X  t   e  X    e  . b  Prove that the solutions of dX  dt  AX  X A keep the same eigenvalues for all time. . If  the  eigenvalues  of A are    and    with  eigenvectors      and        find  the solutions to du  dt  Au and u     Au   starting from u       . . Find the eigenvalues and eigenvectors of A       i  i  i   i     . What property do you expect for the eigenvectors and is it true? . By trying to solve  a   b c   d  a   b c   d              A show that A has no square root.  Change the diagonal entries of A to  and find a square root. . a  Find the eigenvalues and eigenvectors of A        . b  Solv e du  dt  Au starting from u          . c  If v  t   income to stockbrokers and w  t   income to client and they help each other by dv  dt   w and dw  dt    v  what does the ratio v  w approach as t   ? . True or false with reason if true and counterexample if false a  For  every  matrix A   there  is  a  solution  to du  dt  Au starting  from u       ...   . b  Every invertible matrix can be diagonalized. c  Every diagonalizable matrix can be inverted. d  Exchanging the rows of a  by  matrix reverses the signs of its eigenvalues.   e  If eigenvectors x and y correspond to distinct eigenvalues then x  y  . . If K is a skewsymmetric matrix show that Q   I  K  I  K    is an orthogonal matrix. Find Q if K       . . If K    K skewHermitian the eigenvalues are imaginary and the eigenvectors are orthogonal. a  How do you know that K  I is invertible? b  How do you know that K  U  U  for a unitary U ? c  Why is e   unitary? d  Why is e  unitary? . If M is the diagonal matrix with entries d  d   d   what is M   AM ?  What are its eigenvalues in the following case? A                             . . If A    I  what are the eigenvalues of A ?  If A is a real n by n matrix show that n must be even and give an example. . If Ax    x and A  y    y all real show that x  y  . . A variation on the Fourier matrix is the sine matrix S        sin  sin   sin   sin   sin   sin   sin   sin   sin      with     . V erify that S   S   .  The columns are the eigenvectors of the tridiagonal      matrix. . a  Find a nonzero matrix N such that N   . b  If Nx   x  show that  must be zero. c  Pro ve that N called a nilpotent matrix cannot be symmetric. . a  Find  the  matrix P  aa   a  a that  projects  any  vector  onto  the  line  through a         . b  What is the only nonzero eigenvalue of P  and what is the corresponding eigen vector? c  Solve u     Pu   starting from u          . . Suppose the first row of A is   and its eigenvalues are i   i . Find A .  . a  For which numbers c and d does A have real eigenvalues and orthogonal eigen vectors? A             d   c           . b  For which c and d can we find three orthonormal vectors that are combinations of the columns dont do it!? . If the vectors x  and x  are in the columns of S  what are the eigenvalues and eigen vectors of A  S           S   and B  S           S   ? . What is the limit as k   the Markov steady state of  .  .  .  .     a b  ?    .    Minima Maxima and Saddle Points Up to now we have hardly thought about the signs of the eigenvalues .  We couldnt ask whether  was positive before it was known to be real.  Chapter  established that every symmetric matrix has real eigenvalues. Now we will find a test that can be applied directly to A  without computing its eigenvalues which will guarantee that all those eigenvalues are positive .  The test brings together three of the most basic ideas in the book pivots  determinants  and eigenvalues . The signs of the eigenvalues are often crucial.  For stability in differential equations we needed negative eigenvalues so that e   would decay. The new and highly important problem is to recognize a minimum point .   This arises throughout science and engi neering and every problem of optimization.  The mathematical problem is to move the second derivative test F    into n dimensions. Here are two examples F  x  y       x  y    y sin y  x  f  x  y    x    xy  y  . Does either F  x  y  or  f  x  y  have a minimum at the point x  y  ? Remark  . The zeroorder terms F        and f        have no effect on the an swer. They simply raise or lower the graphs of F and f . Remark  . The linear terms give a necessary condition To have any chance of a mini mum the first derivatives must vanish at x  y    F  x    x  y    x    and  F  y    x  y   y cos y  sin y    f  x   x   y   and  f  y   x   y   . All zero . Thus  x  y        is a stationary point for both functions.  The surface z  F  x  y  is tangent to the horizontal plane z   and the surface z  f  x  y  is tangent to the plane z  .  The question is whether the graphs go above those planes or not  as we move away from the tangency point x  y  .  Remark  .  The second derivatives at      are decisive   F  x      x     F  x  y    F  y  x     F  y     y sin y   cos y     f  x      f  x  y    f  y  x     f  y    . These second derivatives    contain the answer.  Since they are the same for F and f  they must contain the same answer.  The two functions behave in exactly the same way near the origin. F has a minimum if and only if f has a minimum . I am going to show that those functions dont! Remark  . The higherdegree terms in F have no effect on the question of a local min imum but they can prevent it from being a global minimum.  In our example the term  x  must sooner or later pull F toward   .  For f  x  y   with no higher terms all the action is at      . Every quadratic form f  ax    bxy  cy  has a stationary point at the origin where  f   x   f   y  .  A local minimum would also be a global minimum The surface z  f  x  y  will then be shaped like a bowl resting on the origin Figure ..  If the stationary point of F is at x    y    the only change would be to use the second derivatives at     Quadratic part of F f  x  y   x     F  x       xy   F  x  y      y     F  y       .  This f  x  y  beha ves near      in the same way that F  x  y  behaves near      .                  The third derivatives are drawn into the problem when the second derivatives fail to give a definite decision.  That happens when the quadratic part is singular.  For a true minimum f is allowed to vanish only at x  y  .  When f  x  y  is strictly positive at all other points the bowl goes up it is called positive definite .   Definite versus Indefinite Bowl versus Saddle The problem comes down to this  For a function of two variables x and y  what is the correct replacement for the condition   F   x   ? With only one variable the sign of the second derivative decides between a minimum or a maximum.  Now we have three second derivatives F   F   F    and F  .   These three numbers like      must determine whether or not F as well as f  has a minimum. What conditions on a  b  and c ensure that the quadratic f  x  y   ax    bxy  cy  is positive definite ? One necessary condition is easy i If ax    bxy  cy  is positive definite then necessarily a  . We look at x   y   where ax    bxy  cy  is equal to a .  This must be positive. Translating back to F  that means that   F   x   .  The graph must go up in the x direction. Similarly fix x   and look in the y direction where f    y   cy   ii If f  x  y  is positive definite then necessarily c  . Do  these  conditions a    and c    guarantee  that f  x  y  is  always  positive?   The answer is no . A large cross term  bxy can pull the graph below zero. Example . f  x  y   x    xy  y  . Here a   and c   are both positive. But f is not positive definite because f        . The conditions a   and c   ensure that f  x  y  is positive on the x and y axes.  But this function is negative on the line x  y  because b    overwhelms a and c . Example . In our original f the coefficient  b   was positive.  Does this ensure a minimum?  Again the answer is no  the sign of b is of no importance! Even though its second derivatives are positive  x    xy  y  is not positive definite . Neither F nor f has a minimum at      because f               . It is the size of b  compared to a and c  that must be controlled. We now want a necessary and sufficient condition for positive definiteness. The simplest technique is to complete the square Express f  x  y  using squares f  ax    bxy  cy   a  x  b a y     c  b  a  y  .  The first term on the right is never negative when the square is multiplied by a  . But  this  square  can  be  zero  and  the  second  term  must  then  be  positive.   That  term has coefficient  ac  b    a .  The last requirement for positive definiteness is that this coefficient must be positive iii  If ax    bxy  cy  stays positive then necessarily ac  b  . Test for a minimum The conditions a   and ac  b  are just right. They guarantee c  . The right side of  is positive and we have found a minimum  A ax    bxy  cy  is positive definite if and only if a   and ac  b  . Any f  x  y  has a minimum at a point where  F   x   F   y   with  F   x    and   F   x    F   y      F   x  y   .  T est for a maximum Since f has a maximum whenever  f has a minimum we just reverse the signs of a  b  and c .  This actually leaves ac  b  unchanged The quadratic form is negative definite if and only if a   and ac  b  . The same change applies for a maximum of F  x  y  . Singular case ac  b   The second term in equation  disappears to leave only the first squarewhich is either positive semidefinite  when a   or negative semidef inite  when a  . The prefix semi allows the possibility that f can equal zero as it will at the point x  b  y   a . The surface z  f  x  y  degenerates from a bowl into a valley. For f   x  y    the valley runs along the line x  y  . Saddle Point ac  b   In one dimension F  x  has a minimum or a maximum or F   . In two dimensions a very important possibility still remains The combination ac  b  may be negative . This occurred in both examples when b dominated a and c . It also occurs if a and c have opposite signs. Then two directions give opposite resultsin one direction f increases in the other it decreases.  It is useful to consider two special cases Saddle points at      f    xy and f   x   y  and ac  b     . In the first b   dominates a  c  . In the second a   and c    have opposite sign.  The saddles  xy and x   y  are practically the same if we turn one through   we get the other. They are also hard to draw. These quadratic forms are indefinite  because they can take either sign.  So we have a stationary point that is neither a maximum or a minimum.  It is called a saddle point . The surface z  x   y  goes down in the direction of the y axis where the legs fit if you still ride a horse.  In case you switched to a car think of a road going over a mountain pass. The top of the pass is a minimum as you look along the range of mountains but it is a maximum as you go along the road. Higher Dimensions Linear Algebra Calculus would be enough to find our conditions F    and F  F   F   for a minimum. But linear algebra is ready to do more because the second derivatives fit into a symmetric matrix A .  The terms ax  and cy  appear on the diagonal .  The cross derivative  bxy is   split between the same entry b above and below. A quadratic f  x  y  comes directly from a symmetric  by  matrix! x  Ax in R  ax    bxy  cy    x   y   a   b b   c  x y  .  This identity please multiply it out is the key to the whole chapter.  It generalizes immediately to n dimensions  and it is a perfect shorthand for studying maxima and minima.  When the variables are x  ... x    they go into a column vector x . For any symmetric matrix A  the product x  Ax is a pure quadratic form f  x  ... x    x  Ax in R   x  x   x        a  a   a   a  a   a        a   a    a            x  x   x                  a  x  x  .  The  diagonal  entries a  to a  multiply x   to x   .   The  pair a   a  combines  into  a  x  x  . Then f  a  x     a  x  x     a  x   . There are no higherorder terms or lowerorder termsonly secondorder. The func tion is zero at x    ...    and its first derivatives are zero. The tangent is flat this is a stationary point.  We have to decide if x   is a minimum or a maximum or a saddle point of the function f  x  Ax . Example . f   x    xy  y  and A             saddle point . Example . f   xy and A             saddle point . Example . A is  by  for  x     x  x    x     x  x    x    f   x  x  x                         x  x  x      minimum at        . Any  function F  x  ... x   is  approached  in  the  same  way.   At  a  stationary  point all first derivatives are zero. A is the second derivative matrix with entries a     F   x   x  .  This automatically equals a     F   x   x   so A is symmetric. Then F has a minimum when the pure quadratic x  Ax is positive definite . These secondorder terms control F near the stationary point Taylor series F  x   F    x   grad F    x  Ax  higher order terms .  At a stationary point grad F    F   x  ...  F   x   is a vector of zeros. The second derivatives in x  Ax take the graph up or down or saddle. If the stationary point is at x   instead of  F  x  and all derivatives are computed at x  .  Then x changes to x  x  on the righthand side. The next section contains the tests to decide whether x  Ax is positive the bowl goes up from x  .  Equivalently the tests decide whether the matrix A is positive defi nite which is the main goal of the chapter. Pr oblem Set . . The quadratic f  x    xy   y  has a saddle point at the origin despite the fact that its coefficients are positive. Write f as a difference of two squares . . Decide for or against the positive definiteness of these matrices and write out the corresponding f  x  Ax  a           . b         . c           . d         . The determinant in b is zero along what line is f  x  y   ? . If a  by  symmetric matrix passes the tests a   ac  b    solve the quadratic equation det  A   I    and show that both eigenvalues are positive. . Decide between a minimum maximum or saddle point for the following functions. a F       e   x    x sin y   y  at the point x  y  . b F   x    x  cos y  with stationary point at x   y   . . a  F or which numbers b is the matrix A        positive definite? b  Factor A  LDL  when b is in the range for positive definiteness. c  Find the minimum value of    x    bx y   y    y for b in this range. d  What is the minimum if b  ? . Suppose the positive coefficients a and c dominate b in the sense that a  c   b . Find an example that has ac  b   so the matrix is not positive definite. . a  What  by  symmetric matrices A  and A  correspond to f  and f  ? f   x    x    x     x  x    x  x    x  x  f   x     x     x     x  x    x  x    x  x  . b  Show that f  is a single perfect square and not positive definite.  Where is f  equal to ? c  Factor A  into LL   Write f   x  A  x as a sum of three squares. . If A      is positive definite test A        for positive definiteness.   . The quadratic f  x   x      x    x      x   is positive.  Find its matrix A  factor it into LDL   and connect the entries in D and L to    in f . . If R       write out R  and check that it is positive definite unless R is singular. . a  If A      is Hermitian  complex b  find its pivots and determinant. b  Complete the square for x  Ax . Now x    x  x   can be complex a  x     Re b x  x   c  x     a  x    b  a  x      x    . c  Sho w that a   and ac   b   ensure that A is positive definite. d  Are the matrices          and          positive definite? . Decide whether F  x  y    x   y has a minimum at the point x  y   after showing that the first derivatives are zero at that point. . Under what conditions on a  b  c is ax    bxy  cy   x   y  for all x  y ? Problems  are about tests for positive definiteness. . Which of A   A   A   A  has two positive eigenvalues? Test a   and ac  b   dont compute the eigenvalues. Find an x so that x  A  x  . A             A             A           A           . . What is the quadratic f  ax    bxy  cy  for each of these matrices? Complete the square to write f as a sum of one or two squares d         d        . A            and A            . . Show that f  x  y   x    xy   y  does not have a minimum at      even though it has positive coefficients.  Write f as a difference of squares and find a point  x  y  where f is negative. .  Important  If A has independent columns  then A  A is square and symmetric and invertible Section .. Rewrite x  A  Ax to show why it is positive except when x  . Then A  A is positive definite. . Test to see if A  A is positive definite in each case A             A                     and A                  .  . Find the  by  matrix A and its pivots rank eigenvalues and determinant  x  x  x      A       x  x  x        x   x    x    . . For F   x  y     x   x  y  y  and F   x  y   x   xy  x  find the second derivative matrices A  and A   A     F   x    F   x  y   F   y  x   F   y   . A  is positive definite so F  is concave up   convex.  Find the minimum point of F  and the saddle point of F  look where first derivatives are zero. . The graph of z  x   y  is a bowl opening upward. The graph of z  x   y  is a saddle .  The graph of z   x   y  is a bowl opening downward.  What is a test on F  x  y  to have a saddle at      ? . Which values of c give a bowl and which give a saddle point for the graph of z   x    xy  cy  ? Describe this graph at the borderline value of c . . Tests for Positive Definiteness Which symmetric matrices have the property that x  Ax   for all nonzero vectors x ? There are four or five different ways to answer this question and we hope to find all of them.  The previous section began with some hints about the signs of eigenvalues.  but that gave place to the tests on a  b  c  b   a   b b   c  is positive definite when a      and ac  b    . From those conditions both eigenvalues are positive . Their product     is determinant ac  b    so the eigenvalues are either both positive or both negative.  They must be positive because their sum is the trace a  c  . Looking at a and ac  b   it is even possible to spot the appearance of the pivots . They turned up when we decomposed x  Ax into a sum of squares Sum of squares ax    bxy  cy   a  x  b a y    ac  b  a y  .  Those coefficients a and  ac  b    a are  the  pivots  for  a    by    matrix.   For  larger matrices the pivots still give a simple test for positive definiteness x  Ax stays positive when n independent squares are multiplied by positive pivots .   One more preliminary remark. The two parts of this hook were linked by the chapter on determinants.  Therefore we ask what part determinants play. It is not enough to require that the determinant of A is positive .  If a  c    and b  .  then det A   but A   I  negative definite.   The determinant test is applied not only to A itself giving ac  b    but also to the  by  submatrix a in the upper lefthand corner. The natural generalization will involve all n of the upper left submatrices of A  A    a    A    a  a  a  a    A      a  a  a  a  a  a  a  a  a        A   A . Here is the main theorem on positive definiteness and a reasonably detailed proof B Each of the following tests is a necessary and sufficient condition for the real symmetric matrix A to be positive definite  I x  kx   for all nonzero real vectors x . II  All the eigenvalues of A satisfy    . III  All the upper left submatrices A  have positive determinants. IV  All the pivots without row exchanges satisfy d   . Proof. Condition  I  defines  a  positive  definite  matrix.   Our  first  step  shows  that  each eigenvalue will be positive If Ax   x  then x  Ax  x   x    x   . A positive definite matrix has positive eigenvalues since x  Ax  . Now  we  go  in  the  other  direction.   If  all      we  have  to  prove x  Ax    for every vector x not just the eigenvectors.  Since symmetric matrices have a full set of orthonormal eigenvectors any x is a combination c  x     c  x  . Then Ax  c  Ax     c  Ax   c    x     c    x  . Because of the orthogonality x   x    and the normalization x   x    x  Ax   c  x      c  x     c    x     c    x    c        c     .  If every     then equation  shows that x  Ax  . Thus condition II implies condi tion I. If condition I holds  so does condition III  The determinant of A is the product of the eigenvalues.  And if condition I holds we already know that these eigenvalues are positive.  But we also have to deal with every upper left submatrix A  .  The trick is to look at all nonzero vectors whose last n  k components are zero x  Ax   x      A      x     x   A  x    .  Thus A  is positive definite.  Its eigenvalues not the same   !  must be positive.  Its determinant is their product so all upper left determinants are positive. If condition III holds so does condition IV  According to Section . the k th pivot d  is the ratio of det A  to det A    . If the determinants are all positive so are the pivots. If condition IV holds  so does condition I  We are given positive pivots  and must deduce that x  Ax  . This is what we did in the  by  case by completing the square. The pivots were the numbers outside the squares. To see how that happens for symmetric matrices of any size we go back to elimination on a symmetric matrix  A  LDL  . Example . Positive pivots     and    A                                                                        LDL  . I want to split x  Ax into x  LDL  x  If x     u v w     then L  x                        u v w        u    v v    w w    . So x  Ax is a sum of squares with the pivots     and   as coefficients x  Ax   L  x   D  L  x     u    v       v    w       w   . Those positive pivots in D multiply perfect squares to make x  Ax positive. Thus condi tion IV implies condition I and the proof is complete. It is beautiful that elimination and completing the square are actually the same. Elim ination removes x  from all later equations.  Similarly the first square accounts for all terms in x  Ax involving x  .  The sum of squares has the pivots outside. The multipliers   are inside ! You can see the numbers    and    inside the squares in the example. Every diagonal entry a  must be positive . As we know from the examples however it is far from sufficient to look only at the diagonal entries. The  pivots d  are  not  to  be  confused  with  the  eigenvalues.   For  a  typical  positive definite matrix they are two completely different sets of positive numbers In our  by  example probably the determinant test is the easiest Determinant test det A     det A     det A   det A   . The pivots are the ratios d    d      d     . Ordinarily the eigenvalue test is the longest computation. For this A we know the  s are all positive Eigenvalue test                     .   Even though it is the hardest to apply to a single matrix eigenvalues can be the most useful test for theoretical purposes. Each test is enough by itself . Positive Definite Matrices and Least Squares I hope you will allow one more test for positive definiteness.  It is already close.  We connected positive definite matrices to pivots Chapter  determinants Chapter  and eigenvalues Chapter .  Now we see them in the leastsquares problems in Chapter  coming from the rectangular matrices of Chapter . The rectangular matrix will be R and the leastsquares problem will be Rx  b . It has m equations with m  n square systems are included. The leastsquare choice  x is the solution of R  R  x  R  b .  That matrix AR  R is not only symmetric but positive definite as we now showprovided that the n columns of R are linearly independent C The symmetric matrix A is positive definite if and only if V  There is a matrix R with independent columns such that A  R  R . The key is to recognize x  Ax as x  R  Rx   Rx    Rx  .   This squared length  Rx   is positive unless x   because R has independent columns. If x is nonzero then Rx is nonzero. Thus x  R  Rx   and R  R is positive definite. It remains to find an R For which A  R  R . We have almost done this twice already Elimination A  LDL    L  D   DL   . So take R   DL  . This Cholesky decomposition has the pivots split evenly between L and L  . Eigenvalues A  Q  Q    Q      Q   . So take R    Q  .  A third possibility is R  Q   Q   the symmetric positive definite square root of A . There are many other choices square or rectangular and we can see why. If you multiply any R by a matrix Q with orthonormal columns  then  QR    QR   R  Q  QR  R  IR  A . Therefore QR is another choice. Applications  of  positive  definite  matrices  are  developed  in  my  earlier  book Intro duction to Applied Mathematics and also the new Applied Mathematics and Scientific Computing see www.wellesleycambridge.com .  We mention that Ax   Mx arises constantly in engineering analysis.  If A and M are positive definite this general ized problem is parallel to the familiar Ax   x  and   . M is a mass matrix for the finite element method in Section .. Semidefinite Matrices The tests for semidefiniteness will relax x  Ax      d   and det   to allow zeros to appear. The main point is to see the analogies with the positive definite case.  D Each of the following tests is a necessary and sufficient condition for a symmetric matrix A to be positive semidefinite  I   x  Ax   for all vectors x this defines positive semidefinite. II    All the eigenvalues of A satisfy    . III    No principal submatrices have negative determinants. IV    No pivots are negative. V    There is a matrix R  possibly with dependent columns such that A  R  R . The diagonalization A  Q  Q  leads to x  Ax  x  Q  Q  x  y   y . If A has rank r  there are r nonzero  s and r perfect squares in y   y    y        y   . Note . The novelty is that condition III  applies to all the principal submatrices not only those in the upper lefthand corner.  Otherwise we could not distinguish between two matrices whose upper left determinants were all zero           is positive semidefinite and        is negative semidefinite. A row exchange comes with the same column exchange to maintain symmetry. Example . A                       is positive semi definite by all five tests I   x  Ax   x   x     x   x     x   x      zero if x   x   x  . II    The eigenvalues are            a zero eigenvalue. III    det A   and smaller determinants are positive. IV   A                                                                missing pivot. V   A  R  R with dependent columns in R                                                                   in the nullspace. Remark . The conditions for semidefiniteness could also be deduced from the origin con ditions IV for definiteness by the following trick  Add a small multiple of the identity giving a positive definite matrix A   I .  Then let  approach zero.  Since the determi nants and eigenvalues depend continuously on   they will be positive until the very last moment. At    they must still be nonnegative.   My class often asks about unsymmetric positive definite matrices.  I never use that term. One reasonable definition is that the symmetric part    A  A   should be positive definite. That guarantees that the real parts of the eigenvalues are positive . But it is not necessary A      has    but    A  A        is indefinite. If Ax   x  then x  Ax   x  x and x  A  x   x  x . Adding   x   A  A   x   Re   x  x   so that Re   . Ellipsoids in n Dimensions Throughout this book geometry has helped the matrix algebra.  A linear equation pro duced a plane.  The system Ax  b gives an intersection of planes.  Least squares gives a perpendicular projection. The determinant is the volume of a box. Now for a positive definite matrix and its x  Ax  we finally get a figure that is curved. It is an ellipse in two dimensions and an ellipsoid in n dimensions. The equation to consider is x  Ax  .  If A is the identity matrix this simplifies to x    x      x    .  This is the equation of the unit sphere in R  .  If A   I  the sphere gets smaller. The equation changes to  x       x    . Instead of     ...    it goes through       ...   .  The center is at the origin because if x satisfies x  Ax   so does the opposite vector  x . The important step is to go from the identity matrix to a diagonal matrix  Ellipsoid For A             the equation is x  Ax   x    x      x     . Since the entries are unequal and positive! the sphere changes to an ellipsoid. One solution is x          along the first axis.  Another is x         .  The major axis has the farthest point x         . It is like a football or a rugby ball but not quite those are closer to x    x      x    . The two equal coefficients make them circular in the x   x  plane and much easier to throw! Now comes the final step to allow nonzeros away from the diagonal of A . Example . A      and x  Ax   u    uv   v   .  That ellipse is centered at u  v   but the axes are not so clear.  The offdiagonal s leave the matrix positive definite but they rotate the ellipseits axes no longer line up with the coordinate axes Figure ..  We will show that the axes of the ellipse point toward the eigenvector of A .  Because A  A   those eigenvectors and axes are orthogonal.  The major axis of the ellipse corresponds to the smallest eigenvalue of A . To  locate  the  ellipse  we  compute      and    .   The  unit  eigenvectors  are           and        . Those  are  at    angles  with  the u  v axes  and  they  are lined up with the axes of the ellipse.  The way to see the ellipse properly is to rewrite                                                      x  Ax   New squares  u    uv  v    u    v        u    v       .     and    are outside the squares.  The eigenvectors are inside.  This is different from completing the square to   u    v      v   with the pivots outside. The first square equals  at             at the end of the major axis.  The minor axis is onethird as long since we need      to cancel the . Any ellipsoid x  Ax   can be simplified in the same way. The key step is to diago nalize A  Q  Q  .  We straightened the picture by rotating the axes.  Algebraically the change to y  Q  x produces a sum of squares x  Ax   x  Q    Q  x   y   y    y        y     .  The major axis has y        along the eigenvector with the smallest eigenvalue. The other axes are along the other eigenvectors. Their lengths are      ...      . Notice  that  the  s  must  be  positive the  matrix  must  be  positive  definite or  these square roots are in trouble.  An indefinite equation y     y     describes a hyperbola and not an ellipse.  A hyperbola is a crosssection through a saddle and an ellipse is a crosssection through a bowl. The change from x to y  Q  x rotates the axes of the space  to match the axes of the ellipsoid.  In the y variables we can see that it is an ellipsoid because the equation becomes so manageable E Suppose A  Q  Q  with    . Rotating y  Q  x simplifies x  Ax   x  Q  Q  x    y   y    and   y        y     . This is the equation of an ellipsoid.  Its axes have lengths      ...      from the center. In the original x space they point along the eigenvectors of A .   The Law of Inertia For  elimination  and  eigenvalues  matrices  become  simpler  by  elementary  operations The essential thing is to know which properties of the matrix stay unchanged.  When a multiple of one row is subtracted from another  the row space  nullspace.  rant and determinant all remain the same.  For eigenvalues the basic operation was a similarity transformation A  S   AS or A  M   AM . The eigenvalues are unchanged and also the Jordan form. Now we ask the same question for symmetric matrices What are the elementary operations and their invariants for x  Ax ? The basic operation on a quadratic form is to change variables.  A new vector y is related to x by some nonsingular matrix x  Cy . The quadratic form becomes y  C  ACy . This shows the fundamental operation on A  Congruence transformation A  C  AC for some nonsingular C .  The symmetry of A is preserved since C  AC remains symmetric.  The real question is What other properties are shared by A and C  AC ?  The answer is given by Sylvesters law of inertia . F C  AC has the same number of positive eigenvalues negative eigenvalues and zero eigenvalues as A . The signs of the eigenvalues and not the eigenvalues themselves are preserved by a congruence transformation.  In the proof we will suppose that A is nonsingular.  Then C  AC is also nonsingular and there are no zero eigenvalues to worry about. Otherwise we can work with the nonsingular A   I and A   I  and at the end let   . Proof. W e want to borrow a trick from topology.  Suppose C is linked to an orthogonal matrix Q by a continuous chain of nonsingular matrices C  t  . At t   and t   C     C and C     Q .  Then the eigenvalues of C  t   AC  t  will change gradually as t goes from  to  from the eigenvalues of C  AC to the eigenvalues of Q  AQ . Because C  t  is never singular none of these eigenvalues can touch zero not to mention cross over it!. Therefore the number of eigenvalues to the right of zero and the number to the left is the same for C  AC as for Q  AQ . And A has exactly the same eigenvalues as the similar matrix Q   AQ  Q  AQ . One good choice for Q is to apply GramSchmidt to the columns of C . Then C  QR  and the chain of matrices is C  t   tQ    t  QR . The family C  t  goes slowly through GramSchmidt from QR to Q . It is invertible because Q is invertible and the triangular factor tI    t  R has positive diagonal. That ends the proof. Example . Suppose A  I .  Then C  AC  C  C is positive definite.  Both I and C  C have n positive eigenvalues confirming the law of inertia. Example . If A         then C  AC has a negative determinant det C  AC   det C   det A  det C     det C     .  Then C  AC must have one positive and one negative eigenvalue like A . Example . This application is the important one G For any symmetric matrix A  the signs of the pivots agree with the signs of the eigenvalues .  The eigenvalue matrix  and the pivot matrix D have the same number of positive entries negative entries and zero entries. We will assume that A allows the symmetric factorization A  LDL  without row ex changes.  By the law of inertia A has the same number of positive eigenvalues as D . But the eigenvalues of D are just its diagonal entries the pivots.  Thus the number of positive pivots matches the number of positive eigenvalues of A . That is both beautiful and practical.   It is beautiful because it brings together for symmetric matrices two parts of this book that were previously separate pivots and eigenvalues . It is also practical because the pivots can locate the eigenvalues A has positive pivots A   I has a negative pivot A                     A   I                             . A has positive eigenvalues by our test. But we know that   is smaller than  because subtracting  dropped it below zero. The next step looks at A  I  to see if    . It is because A  I has a negative pivot. That interval containing  is cut in half at every step by checking the signs of the pivots. This was almost the first practical method of computing eigenvalues. It was dominant about  after one important improvementto make A tridiagonal first.  Then the pivots are computed in  n steps instead of   n  . Elimination becomes fast and the search for eigenvalues by halving the intervals becomes simple.  The current favorite is the QR method in Chapter . The Generalized Eigenvalue Problem Physics engineering and statistics are usually kind enough to produce symmetric ma trices in their eigenvalue problems. But sometimes Ax   x is replaced by Ax   Mx . There are two matrices rather than one . An example is the motion of two unequal masses in a line of springs m  d  v d t    v  w   m  d  w d t   v   w   or  m    m   d  u d t           u   .  When the masses were equal m   m    this was the old system u   Au  .  Now it is Mu   Au   with a mass matrix M . The eigenvalue problem arises when we look   for exponential solutions e    x  Mu   Au   becomes M  i    e    x  Ae    x   .  Canceling e     and writing  for    this is an eigenvalue problem Generalized problem Ax   Mx         x    m    m   x .  There is a solution when A   M is singular. The special choice M  I brings back the usual det  A   I   . We work out det  A   M  with m    and m    det                            gives        . F or the eigenvector x           the two masses oscillate togetherbut the first mass only moves as far as      . . In the fastest mode the components of x            ha ve opposite signs and the masses move in opposite directions. This time the smaller mass goes much further. The underlying theory is easier to explain if M is split into R  R .  M is assumed to be positive definite.  Then the substitution y  Rx changes Ax   Mx   R  Rx into AR   y   R  y . Writing C for R    and multiplying through by  R      C   this becomes a standard eigenvalue problem for the single symmetric matrix C  AC  Equivalent problem C  ACy   y .  The eigenvalues   are the same as for the original Ax   Mx . and the eigenvectors are related by y   Rx  . The properties of C  AC lead directly to thc properties of Ax   Mx  when A  A  and M is positive definite .  The eigenvalues for Ax   Mx are real because C  AC is symmetric. .  The  s have the same signs as the eigenvalues of A  by the law of inertia. . C  A C has orthogonal eigenvectors y  . So the eigenvectors of Ax   Mx have  M orthogonality x   Mx   x   R  Rx   y   y    .  A and M are being simultaneously diagonalized .  If S has the x  in its columns  then S  AS   and S  MS  I .   This is a congruence transformation  with S  on the left and not a similarity transformation with S   .  The main point is easy to summarize As long as M is positive definite the generalized eigenvalue problem Ax    Mx behaves exactly like Ax   x .  Problem Set . . For what range of numbers a and b are the matrices A and B positive definite? A     a      a      a    B             b            . . Decide for or against the positive definiteness of A                        B                      C                              . . Construct an indefinite matrix with its largest entries on the main diagonal A      b  b b  b  b   b     with  b    can have det A   . . Show from the eigenvalues that if A is positive definite so is A  and so is A   . . If A and B are positive definite then A  B is positive definite . Pivots and eigenvalues are not convenient for A  B . Much better to prove x   A  B  x  . . From the pivots eigenvalues and eigenvectors of A       write A as R  R in three ways  L  D   DL     Q      Q    and  Q   Q   Q   Q   . . If A  Q  Q  is symmetric positive definite then R  Q   Q  is its symmetric pos itive definite square root .  Why does R have positive eigenvalues?  Compute R and verify R   A for A        and A             . . If A is symmetric positive definite and C is nonsingular prove that B  C  AC is also symmetric positive definite. . If A  R  R prove the generalized Schwarz inequality  x  Ay     x  Ax  y  Ay  . . The ellipse u    v    corresponds to A      . Write the eigenvalues and eigen vectors and sketch the ellipse. . Reduce  the  equation   u      uv   v     to a  sum  of  squares  by  finding  the eigenvalues of the corresponding A  and sketch the ellipse.   . In three dimensions   y      y      y     represents an ellipsoid when all    . Describe all the different kinds of surfaces that appear in the positive semidefinite case when one or more of the eigenvalues is zero. . Write down the five conditions for a  by  matrix to be negative definite   A is positive definite with special attention to condition III How is det   A  related to det A ? . Decide whether the following matrices are positive definite negative definite semidef inite or indefinite A                              B                                 C   B  D  A   . Is there a real solution to  x    y    z    xy   xz   yz  ? . Suppose A is symmetric positive definite and Q is an orthogonal matrix.   True or false a Q  AQ is a diagonal matrix. b Q  AQ is symmetric positive definite. c Q  AQ has the same eigenvalues as A . d e   is symmetric positive definite. . If A is positive definite and a  is increased prove from cofactors that the determinant is increased. Show by example that this can fail if A is indefinite. . From A  R  R .  show for positive definite matrices that det A  a  a   a  .  The length squared of column j of R is a  . Use determinant  volume. . Lyapunov test for stability of M  Suppose AM  M  A   I with positive definite A . If Mx   x show that Re A  .  Hint  Multiply the first equation by x  and x . . Which  by  symmetric matrices A produce these functions f  x  Ax ? Why is the first matrix positive definite but not the second one? a f    x    x    x    x  x   x  x   . b f    x    x    x    x  x   x  x   x  x   . . Compute the three upper left determinants to establish positive definiteness.  Verify that their ratios give the second and third pivots. A                             .  . A positive definite matrix cannot have a zero or even worse a negative number on its diagonal. Show that this matrix fails to have x  Ax    x  x  x                                 x  x  x     is not positive when  x   x   x         . . A  diagonal  entry a  of  a  symmetric  matrix  cannot  be  smaller  than  all  s.   If  it were then A  a  I would have eigen values and would be positive definite. But A  a  I has a on the main diagonal. . Give a quick reason why each of these statements is true a  Every positive definite matrix is invertible. b  The only positive definite projection matrix is P  I . c  A diagonal matrix with positive diagonal entries is positive definite. d  A symmetric matrix with a positive determinant might not be positive definite! . For which s and t do A and B have all    and are therefore positive definite? A     s       s       s    and B     t      t      t    . . Y ou may have seen the equation for an ellipse as             . What are a and b when the equation is written as   x     y   ?  The ellipse  x    y    has halfaxes with lengths a   and b  . . Dra w the tilted ellipse x   xy  y    and find the halflengths of its axes from the eigenvalues of the corresponding A . . With positive pivots in D  the factorization A  LDL  becomes L  D  DL  . Square roots of the pivots give D   D  D . Then C  L  D yields the Cholesky factor ization A  CC   which is symmetrized LU  From C            find A . From A          find C . . In the Cholesky factorization A  CC   with C  L  D  the square roots of the pivots are on the diagonal of C . Find C lower triangular for A                             and A                             .   . The symmetric factorization A  LDL  means that x  Ax  x  LDL  x   x   y   a   b b   c  x y    x   y     b  a   a    ac  b    a   b  a    x y  . The lefthand side is ax    bxy  cy  .  The righthand side is a  x    y    y  . The second pivot completes the square! Test with a   b   c  . . Without multiplying A                           find a the determinant of A . b    the eigenvalues of A . c    the eigenvectors of A .    d    a reason why A is symmetric positive definite. . For the semidefinite matrices A                       rank  and B                             rank   write x  Ax as a sum of two squares and x  Bx as one square. . Apply any three tests to each of the matrices A                             and B                              to decide whether they are positive definite positive semidefinite or indefinite. . For C        and A       confirm that C  AC has eigenvalues of the same signs as A .   Construct  a chain  of  nonsingular matrices C  t  linking C to an  orthogonal Q .  Why is it impossible to construct a nonsingular chain linking C to the identity matrix? . If the pivots of a matrix are all greater than  are the eigenvalues all greater than ? Test on the tridiagonal      matrices. . Use the pivots of A    I to decide whether A has an eigenvalue smaller than    A    I      .      .      .     . . An  algebraic  proof  of  the  law  of  inertia  starts  with  the  orthonormal  eigenvectors x  ... x  of A corresponding to eigenvalues    . and the orthonormal eigenvectors y  ... y  of C  AC corresponding to eigenvalues    .  a  To prove that the p  q vectors x  ... x   Cy  ... Cy  are independent assume that some combination gives zero a  x     a  x   b  Cy     b  Cy   z  say  . Show that z  Az    a        a     and z  Az    b        b    . b  Deduce that the a s and b s are zero proving linear independence.  From that deduce p  q  n . c  The same argument for the n  p negative  s and the n  q positive  s gives n  p  n  q  n .  We again assume no zero eigenvalueswhich are handled separately.  Show that p  q  n   so the number p of positive  s equals the number n  q of positive  swhich is the law of inertia. . If C is nonsingular show that A and C  AC have the same rank.  Thus they have the same number of zero eigenvalues. . Find by experiment the number of positive negative and zero eigenvalues of A   I B B    when the block B of order   n  is nonsingular. . Do A and C  AC always satisfy the law of inertia when C is not square? . In  equation    with m     and m     verify  that  the  normal  modes  are M  orthogonal x   Mx   . . Find the eigenvalues and eigenvectors of Ax   Mx          x            x . . If the symmetric matrices A and M are indefinite Ax   Mx might not have real eigenvalues. Construct a  by  example. . A gr oup of nonsingular matrices includes AB and A   if it includes A and B . Prod ucts and inverses stay in the group. Which of these sets are groups? Positive definite symmetric matrices A orthogonal matrices Q all exponentials e  of a fixed matrix A matrices P with positive eigenvalues matrices D with determinant .  Invent a group containing only positive definite matrices.   .    Singular Value Decomposition A great matrix factorization has been saved for the end of the basic course. U  V  joins with LU from elimination and QR from orthogonalization Gauss and GramSchmidt. Nobodys name is attached A  U  V  is known as the SVD or the singular value decomposition .   We want to describe it  to prove it  and to discuss its applications which are many and growing. The SVD is closely associated with the eigenvalueeigenvector factorization Q  Q  of a positive definite matrix. The eigenvalues are in the diagonal matrix  . The eigenvector matrix Q is orthogonal  Q  Q  I  because eigenvectors of a symmetric matrix can be chosen to be orthonormal. For most matrices that is not true and for rectangular matrices it is ridiculous eigenvalues undefined. But now we allow the Q on the left and the Q  on the right to be any two orthogonal matrices U and V  not necessarily transposes of each other. Then every matrix will split into A  U  V  . The diagonal but rectangular matrix  has eigenvalues from A  A  not from A ! Those positive entries also called sigma will be   ...   . They are the singular values of A . They fill the first r places on the main diagonal of  when A has rank r . The rest of  is zero. W ith rectangular matrices the key is almost always to consider A  A and AA  . Singular Value Decomposition Any m by n matrix A can be factored into A  U  V    orthogonal  diagonal  orthogonal  . The columns of U  m by m  are eigenvectors of AA   and the columns of V  n by n  are eigenvectors of A  A .  The r singular values on the diagonal of   m by n  are the square roots of the nonzero eigenvalues of both AA  and A  A . Remark  . For positive definite matrices  is  and U  V  is identical to Q  Q  .  For other  symmetric  matrices  any  negative  eigenvalues  in  become  positive  in  .   For complex matrices  remains real but U and V become unitary the complex version of orthogonal. We take complex conjugates in U  U  I and V  V  I and A  U  V  . Remark  . U and V give orthonormal bases for all four fundamental subspaces  first r columns of U  column space of A last m  r columns of U  left nullspace of A first r columns of V  row space of A last n  r columns of V  nullspace of A Remark  . The SVD chooses those bases in an extremely special way.  They are more than  just  orthonormal. When  A  multiplies  a  column  v  of V   it  produces   times  a column of U . That comes directly from AV  U   looked at a column at a time.  Remark  . Eigenvectors of AA  and A  A must go into the columns of U and V  AA    U  V   V   U    U   U  and similarly A  A  V    V  .  U must be the eigenvector matrix for AA  . The eigenvalue matrix in the middle is    which is m by m with    ...    on the diagonal. From the A  A  V    V   the V matrix must be the eigenvector matrix for A  A . The diagonal matrix    has the same    ...     but it is n by n . Remark  . Here is the reason that Av     u  . Start with A  Av      v   Multiply by A AA  Av      Av   This says that Av  is an eigenvector of AA  ! We just moved parentheses to  AA   Av   . The length of this eigenvector Av  is    because v  A  Av      v   v  gives  Av        . So the unit eigenvector is Av      u  . In other words AV  U  . Example . This A has only one column rank r  . Then  has only     SVD A                                                     U        V     . A  A is  by  whereas AA  is  by . They both have eigenvalue  whose square root is the  in  . The two zero eigenvalues of AA  leave some freedom for the eigenvectors in columns  and  of U . We kept that matrix orthogonal. Example . Now A has rank  and AA           with    and               U  V                                                     . Notice   and  . The columns of U are left singular vectors unit eigenvectors of AA  . The columns of V are right singular vectors unit eigenvectors of A  A . Application of the SVD We will pick a few important applications after emphasizing one key point. The SVD is terrific for numerically stable computations.  because U and V are orthogonal matrices. They never change the length of a vector. Since  U x    x  U  U x   x    multiplication by U cannot destroy the scaling.   Of course  could multiply by a large  or more commonly divide by a small   and overflow the computer. But still  is as good as possible . It reveals exactly what is large and what is small. The ratio      is the condition number of an invertible n by n matrix.  The availability of that information is another reason for the popularity of the SVD. We come back to this in the second application. .  Image processing Suppose a satellite takes a picture and wants to send it to Earth. The picture may contain  by  pixelsa million little squares each with a definite color. We can code the colors and send back  numbers. It is better to find the essential information inside the  by  matrix and send only that. Suppose we know the SVD. The key is in the singular values in  . Typically some  s are significant and others are extremely small.  If we keep  and throw away  then we send only the corresponding  columns of U and V .  The other  columns are multiplied in U  V  by the small  s that are being ignored. We can do the matrix multiplication as columns times rows  A  U  V   u    v    u    v      u    v   .  An y matrix is the sum of r matrices of rank .  If only  terms are kept we send  times  numbers instead of a million  to  compression. The pictures are really striking as more and more singular values are included.  At first you see nothing and suddenly you recognize everything. The cost is in computing the SVDthis has become much more efficient but it is expensive for a big matrix. .  The effective rank The rank of a matrix is the number of independent rows and the number of independent columns.  That can be hard to decide in computations!  In exact arithmetic counting the pivots is correct. Real arithmetic can be misleadingbut discarding small pivots is not the answer. Consider the following  is small        and         and         . The first has rank  although roundoff error will probably produce a second pivot. Both pivots will be small how many do we ignore?  The second has one small pivot but we cannot pretend that its row is insignificant. The third has two pivots and its rank is  but its effective rank ought to be . We go to a more stable measure of rank.  The first step is to use A  A or AA   which are symmetric but share the same rank as A .   Their eigenvaluesthe singular values squaredare not misleading.  Based on the accuracy of the data we decide on a toler ance like    and count the singular values above itthat is the effective rank.  The examples above have effective rank  when  is very small. . Polar decomposition Every nonzero complex number z is a positive number r times  a number e   on the unit circle z  re   .  That expresses z in polar coordinates.  If we think of z as a  by  matrix r corresponds to a positive definite matrix and e   corresponds to an orthogonal matrix .  More exactly since e   is complex and satisfies e    e     it forms a  by  unitary matrix  U  U  I . We take the complex conjugate as well as the transpose for U  . The SVD extends this polar factorization to matrices of any size Every real square matrix can be factored into A  QS  where Q is orthogonal and S is symmetric positive semidefinite .  If A is invertible then S is positive definite. For proof we just insert V  V  I into the middle of the SVD A  U  V    UV   V  V   .  The factor S  V  V  is symmetric and semidefinite because  is. The factor Q  UV  is an orthogonal matrix because Q  Q  V U  UV   I . In the complex case S becomes Hermitian instead of symmetric and Q becomes unitary instead of orthogonal.  In the invertible case  is definite and so is S . Example . Polar decomposition A  QS                        . Example . Reverse polar decomposition A  S  Q                          . The exercises show how in the reverse order. S changes but Q remains the same.  Both S and S  are symmetric positive definite because this A is invertible. Application of A  QS  A major use of the polar decomposition is in continuum mechanics and recently in robotics.   In any deformation  it is important to separate stretching from rotation and that is exactly what QS achieves.  The orthogonal matrix Q is a rotation and possibly a reflection.  The material feels no strain.  The symmetric matrix S has eigenvalues   ...     which are the stretching factors or compression factors.   The diagonalization that displays those eigenvalues is the natural choice of axescalled principal axes  as in the ellipses of Section ..  It is S that requires work on the material and stores up elastic energy. W e note that S  is A  A  which is symmetric positive definite when A is invertible. S is the symmetric positive definite square root of A  A  and Q is AS   . In fact A could be rectangular as long as A  A is positive definite . That is the condition we keep meeting   that A must have independent columns.  In the reverse order A  S  Q  the matrix S  is the symmetric positive definite square root of AA  . . Least Squares For a rectangular system Ax  b . the leastsquares solution comes from the normal equations A  A  x  A  b . If A has dependent columns then A  A is not invertible and  x is not determined . Any vector in the nullspace could be added to  x . We can now complete Chapter  by choosing a best  shortest   x for every Ax  b . Ax  b has two possible difficulties Dependent rows or dependent columns .  With dependent  rows Ax  b may  have no  solution.   That  happens  when b is  outside  the column space of A .  Instead of Ax  b .  we solve A  A  x  A  b .  But if A has dependent columns  this  x will not be unique.  We have to choose a particular solution of A  A  x  A  b  and we choose the shortest. The optimal solution of Ax  b is the minimum length solution of A  A  x  A  b . That minimum length solution will be called x  .  It is our preferred choice as the best solution to Ax  b which had no solution  and also to A  A  x  A  b which had too many. We start with a diagonal example. Example . A is diagonal with dependent rows and dependent columns A  x  p is                                 x   x   x   x           b  b      . The columns all end with zero. In the column space the closest vector to b   b   b   b   is p   b   b     .  The best we can do with Ax  b is to solve the first two equations since the third equation is   b  . That error cannot be reduced but the errors in the first two equations will be zero. Then  x   b     and  x   b     . No w we face the second difficulty.  To make  x as short as possible we choose the totally arbitrary  x  and  x  to be zero. The minimum length solution is x   A  is pseudoinverse x   A  b is shortest x        b     b                                            b  b  b     .  This equation finds x   and it also displays the matrix that produces x  from b . That matrix is the pseudoinverse A  of our diagonal A . Based on this example we know    and x  for any diagonal matrix            . . .                    . . .            b       b     . . . b          . The matrix  is m by n  with r nonzero entries   . Its pseudoinverse   is n by m  with r nonzero entries     . All the blank spaces are zeros .  Notice that      is  again. That is like  A       A  but here A is not invertible. No w we find x  in the general case. We claim that the shortest solution x  is always in the row space of A . Remember that any vector  x can be split into a row space compo nent x  and a nullspace component  x  x   x  . There are three important points about that splitting .  The row space component also solves A  A  x   A  b  because Ax   . .  The components are orthogonal and they obey Pythagorass law   x     x      x     so  x is shortest when x   . .  All solutions of A  A  x  A  b have the same x  . That vector is x  . The fundamental theorem of linear algebra was in Figure ..  Every p in the column space comes from one and only one vector x  in the row space. All we are doing is to choose that vector x   x   as the best solution to Ax  b . The pseudoinverse in Figure . starts with b and comes back to x  . It inverts A where A is invertible between row space and column space.  The pseudoinverse knocks out the left nullspace by sending it to zero and it knocks out the nullspace by choosing x  as x  . We have not yet shown that there is a matrix A  that always gives x  but there is. It will be n by m  because it takes b and p in R  back to x  in R  . We look at one more example before finding A  in general. Example . Ax  b is  x    x    x    with a whole plane of solutions. According  to  our  theory  the  shortest  solution  should  be  in  the  row  space  of A          .  The multiple of that row that satisfies the equation is x           .  There are longer solutions like                   or          but they all have nonzero components from the nullspace. The matrix that produces x  from b     is the pseu doinverse A  . Whereas A was  by  this A  is  by  A                            and A                .            The row space of A is the column space of A  . Here is a formula for A   If A  U  V  the SVD then its pseudoinverse is A   V   U  .  Example  had   the square root of the eigenvalue of AA      . Here it is again with  and    A             U  V                                         V   U                                                          A  . The minimum length leastsquares solution is x   A  b  V   U  b . Proof. Multiplication by the orthogonal matrix U  leaves lengths unchanged  Ax  b    U  V  x  b     V  x  U  b  . Introduce the new unknown y  V  x  V   x  which has the same length as x .  Then minimizing  Ax  b  is the same as minimizing   y  U  b  . Now  is diagonal and we know the best y  . It is y     U  b so the best x  is V y   Shortest solution x   V y   V   U  b  A  b . V y  is in the row space and A  Ax   A  b from the SVD .  Problem Set . Problems  compute the SVD of a square singular matrix A . . Compute A  A and its eigenvalues      and unit eigenvectors v   v   A            . . a  Compute AA  and its eigenvalues      and unit eigenvectors u   u  . b  Choose signs so that Av     u  and verify the SVD             u  u         v  v    . c  Which four vectors give orthonormal bases for   A     A     A      A   ? Problems  ask for the SVD of matrices of rank . . Find the SVD from the eigenvectors v   v  of A  A and Av     u   Fibonacci matrix A            . . Use the  SVD  part  of  the MATLAB demo eigshow or  Java  on  the  course  page web.mit.edu.  to find the same vectors v  and v  graphically. . Compute A  A and AA   and their eigenvalues and unit eigenvectors for A                  . Multiply the three matrices U  V  to recover A . Problems  bring out the underlying ideas of the SVD. . Suppose u  ... u  and v  ... v  are orthonormal bases for R  . Construct the matrix A that transforms each v  into u  to give Av   u  ... Av   u  . . Construct  the  matrix  with  rank    that  has Av   u for v             and u           . Its only singular value is    . . Find U  V  if A has orthogonal columns w  ... w  of lengths   ...   . . Explain how U  V  expresses A as a sum of r rank matrices in equation  A    u  v        u  v   .   . Suppose A is a  by  symmetric matrix with unit eigenvectors u  and u  .   If its eigenvalues are     and      what are U    and V  ? . Suppose A is invertible with       . Change A by as small a matrix as possible to produce a singular matrix A  . Hint  U and V do not change Find A  from A   u  u          v  v    . . a  If A changes to  A  what is the change in the SVD? b  What is the SVD for A  and for A   ? . Why doesnt the SVD for A  I just use   I ? . Find the SVD and the pseudoinverse   of the m by n zero matrix . . Find the SVD and the pseudoinverse V   U  of A               B                   and C            . . If an m by n matrix Q has orthonormal columns what is Q  ? . Diagonalize A  A to find its positive definite square root S  V     V  and its polar decomposition A  QS  A           . . What is the minimumlength leastsquares solution x   A  b to the following? Ax                                C D E              . You can compute A   or find the general solution to A  A  x  A  b and choose the solution that is in the row space of A . This problem fits the best plane C  Dt  Ez to b   and also b   at t  z   and b   at t  z  . a  If A has independent columns its leftinverse  A  A    A  is A  . b  If A has independent rows its rightinverse A   AA     is A  . In both cases verify that x   A  b is in the row space. and A  Ax   A  b . . Split A  U  V  into its reverse polar decomposition QS  . . Is  AB    B  A  always true for pseudoinverses? I believe not.  . Removing zero rows of U leaves A  L U  where the r columns or L span the column space of A and the r rows of U span the row space. Then A  has the explicit formula U   U U      L  L    L  . Wh y is A  b in the row space with U  at the front?  Why does A  AA  b  A  b  so that x   A  b satisfies the normal equation as it should? . Explain why AA  and A  A are projection matrices and therefore symmetric. What fundamental subspaces do they project onto? . Minimum Principles In this section we escape for the first time from linear equations. The unknown x will not be given as the solution to Ax  b or Ax   x .  Instead the vector x will be determined by a minimum principle. It is astonishing how many natural laws can be expressed as minimum principles. Just the fact that heavy liquids sink to the bottom is a consequence of minimizing their po tential energy. And when you sit on a chair or lie on a bed the springs adjust themselves so that the energy is minimized.  A straw in a glass of water looks bent because light reaches your eye as quickly as possible.  Certainly there are more highbrow examples The fundamental principle of structural engineering is the minimization of total energy.  We have to say immediately that these energies are nothing but positive definite quadratic functions .   And the derivative of a quadratic is linear.   We get back to the familiar  linear  equations  when  we  set  the  first  derivatives  to  zero.   Our  first  goal  in this  section  is to  find  the  minimum  principle  that  is  equivalent  to Ax  b   and  the minimization equivalent to Ax   x .   We will be doing in finite dimensions exactly what the theory of optimization does in a continuous problem where first derivatives   gives a differential equation.   In every problem  we are free to solve the linear equation or minimize the quadratic. The first step is straightforward We want to find the parabola P  x  whose minimum occurs when Ax  b . If A is just a scalar that is easy to do The graph of P  x     Ax   bx has zero slope when dP d x  Ax  b   . This point x  A   b will be a minimum if A is positive.  Then the parabola P  x  opens upward Figure ..  In more dimensions this parabola turns into a parabolic bowl a paraboloid. To assure a minimum of P  x   not a maximum or a saddle point A must be positive definite!        H If A is symmetric positive definite then P  x     x  Ax  x  b reaches its minimum at the point where Ax  b . At that point P      b  A   b .         Proof. Suppose Ax  b . For any vector y  we show that P  y   P  x   P  y   P  x     y  A y  y  b    x  Ax  x  b    y  A y  y  Ax    x  Ax set b  Ax      y  x   A  y  x  .  This cant be negative since A is positive definiteand it is zero only if y  x  . At all other points P  y  is larger than P  x   so the minimum occurs at x . Example . Minimize P  x   x    x  x   x    b  x   b  x  .  The usual approach by calculus is to set the partial derivatives to zero. This gives Ax  b   P   x    x   x   b     P   x    x    x   b    means         x  x     b  b   .  Linear algebra recognizes this P  x  as   x  Ax  x  b  and knows immediately that Ax  b gives the minimum. Substitute x  A   b into P  x   Minimum value P      A   b   A  A   b    A   b   b     b  A   b .  In applications   x  Ax is the internal energy and  x  b is the external work. The system automatically goes to x  A   b  where the total energy P  x  is a minimum. Minimizing with Constraints Many  applications  add  extra  equations Cx  d on  top  of  the  minimization  problem. These equations are constraints .  We minimize P  x  subject to the extra requirement Cx  d .  Usually x cant satisfy n equations Ax  b and also  extra constraints Cx  d . We have too many equations and we need  more unknowns.  Those  new  unknowns y  ... y  are  called Lagrange  multipliers .   They  build  the constraint into a function L  x  y  . This was the brilliant insight of Lagrange L  x  y   P  x  y   Cx  d     x  Ax  x  b  x  C  y  y  d . That term in L is chosen exactly so that  L   y   brings back Cx  d .  When we set the derivatives of L to zero we have n   equations for n   unknowns x and y  Constrained minimization  L   x     L   y    Ax  C  y  b Cx  d  The first equations involve the mysterious unknowns y .  You might well ask what they represent.  Those dual unknowns y tell how much the constrained minimum P    which only allows x when Cx  d  exceeds the unconstrained P  allowing all x  Sensitivity of minimum P     P     y   C A   b  d   P  .  Example . Suppose P  x   x      x      x   . Its smallest value is certainly P   . This unconstrained problem has n   A  I  and b  .  So the minimizing equation Ax  b just gives x    and x   . Now add one constraint c  x   c  x   d .  This puts x on a line in the x   x  plane. The old minimizer x   x    is not on the line.  The Lagrangian function L  x  y     x      x    y  c  x   c  x   d  has n       partial derivatives  L   x     L   x     L   y   x   c  y   x   c  y   c  x   c  x   d .  Substituting x    c  y and x    c  y into the third equation gives  c   y  c   y  d . Solution y   d c    c   x   c  d c    c   x   c  d c    c   .  The constrained minimum of P    x  x is reached at that solution point P       x      x      c   d   c   d   c    c        d  c    c   .  This equals    yd as predicted in equation  since b   and P   . Figure . shows what problem the linear algebra has solved if the constraint keeps x on a line  x   x   . We are looking for the closest point to      on this line . The solution is x        . We expect this shortest vector x to be perpendicular to the line and we are right.                       Least Squares Again In minimization our big application is least squares. The best  x is the vector that mini mizes the squared error E    Ax  b   . This is a quadratic and it fits our framework! I will highlight the parts that look new Squared error E    Ax  b    Ax  b   x     x   x         .  Compare with   x  Ax  x  b at the start of the section which led to Ax  b   A changes to A  A    b changes to A  b    b  b is added  . The constant b  b raises the whole graphthis has no effect on the best  x .  The other two changes A to A  A and b to A  b  give a new way to reach the leastsquares equation normal equation. The minimizing equation Ax  b changes into the Leastsquares equation A  A  x  A  b .  Optimization needs a whole book. We stop while it is pure linear algebra. The Rayleigh quotient Our second goal is to find a minimization problem equivalent to Ax   x . That is not so easy. The function to minimize cannot be a quadratic or its derivative would be linear and the eigenvalue problem is nonlinear   times x . The trick that succeeds is to divide one quadratic by another one Rayleigh quotient Minimize R  x   x  Ax x  x . I Rayleighs Principle The minimum value of the Rayleigh quotient is the smallest eigenvalue   . R  x  reaches that minimum at the first eigenvector x  of A  Minimum where Ax    x  R  x    x   Ax  x   x   x     x  x   x     .  If we keep x  Ax   then R  x  is a minimum when x  x   x   is as large as possible. We are looking for the point on the ellipsoid x  Ax   farthest from the originthe vector x of greatest length. From our earlier description of the ellipsoid its longest axis points along the first eigenvector. So R  x  is a minimum at x  . Algebraically we can diagonalize the symmetric A by an orthogonal matrix Q  AQ   . Then set x  Qy and the quotient becomes simple R  x    Qy   A  Qy   Qy    Qy   y   y y  y    y        y   y       y   .  The minimum of R is    at the point where y    and y     y    At all points    y    y      y        y      y        y    . The Rayleigh quotient in equation  is never below   and never above   the largest eigenvalue. Its minimum is at the eigenvector x  and its maximum is at x   Maximum where Ax     x  R  x    x   Ax  x   x   x     x  x   x     . One small yet important point The Rayleigh quotient equals a   when the trial vector is x      ...   . So a  on the main diagonal is between   and   . You can see this in Figure . where the horizontal distance to the ellipse where a  x    is between the shortest distance and the longest distance        a       which is    a     . The diagonal entries of any symmetric matrix are between   and   . We drew Figure . for a  by  positive definite matrix to see it clearly. Intertwining of the Eigenvalues The intermediate eigenvectors x  ... x    are saddle points of the Rayleigh quotient zero derivatives but not minima or maxima.  The difficulty with saddle points is that we have no idea whether R  x  is above or below them.  That makes the intermediate eigenvalues   ...     harder to estimate. F or this optional topic the key is to find a constrained minimum or maximum.  The constraints come from the basic property of symmetric matrices x  is perpendicular to the other eigenvectors. J The minimum of R  x  subject to x  x    is   .  The minimum of R  x  subject to any other constraint x  v   is not above       min       R  x  and    min      R  x  .                                                            This maximin principle makes   the maximum over all v of the minimum of R  x  with x  v  . That offers a way to estimate   without knowing   . Example . Throw away the last row and column of any symmetric matrix    A          A       A       A                     becomes B             B       B    . The second eigenvalue    A    is above the lowest eigenvalue    B   . The lowest eigenvalue    A       is below    B  . So    B  is caught between. This example chose v         so the constraint x  v   knocked out the third com ponent of x thereby reducing A to B . The complete picture is an intertwining of eigenvalues    A      B      A      B        B      A  .  This has a natural interpretation for an ellipsoid when it is cut by a plane through the origin. The cross section is an ellipsoid of one lower dimension. The major axis Of this cross section cannot be longer than the major axis of the whole ellipsoid    B      A  . But the major axis of the cross section is at least as long as the second axis of the original ellipsoid    B      A  .  Similarly the minor axis of the cross section is smaller than the original second axis and larger than the original minor axis    A      B      A  . Y ou can see the same thing in mechanics.  When springs and masses are oscillating suppose one mass is held at equilibrium. Then the lowest frequency is increased but not above   . The highest frequency is decreased but not below     . W e close with three remarks I hope your intuition says that they are correct.  Remark  . The maximin principle extends to j dimensional subspaces S   Maximum of minimum      max     min     R  x   .  Remark  . There is also a minimax principle for      Minimum of maximum      min     max     R  x   .  If j   we are maximizing R  x  over one constraint x  v  . That maximum is between the unconstrained     and   . The toughest constraint makes x perpendicular to the top eigenvector v  x  .  Then the best x is the next eigenvector x    .  The minimum of the maximum is     . Remark  . F or the generalized problem Ax   Mx   the same principles hold if M is positive definite. In the Rayleigh quotient x  x becomes x  Mx  Rayleigh quotient Minimizing R  x   x  Ax x  M x gives    M   A  .  Ev en for unequal masses in an oscillating system  M   I  holding one mass at equilib rium will raise the lowest frequency and lower the highest frequency. Pr oblem Set . . Consider the system Ax  b given by                       x  x  x               . Construct the corresponding quadratic P  x   x   x    compute its partial derivatives  P   x   and verify that they vanish exactly at the desired solution. . Complete the square in P    x  Ax  x  b     x  A   b   A  x  A   b   constant. This constant equals P  because the term before it is never negative. Why? . Find the minimum if there is one of P     x   x y  y    y and P     x    y . What matrix A is associated with P  ? . Review Another quadratic that certainly has its minimum at Ax  b is Q  x      Ax  b      x  A  Ax  x  A  b    b  b . Comparing Q with P  and ignoring the constant   b  b  what system of equations do we get at the minimum of Q ?  What are these equations called in the theory of least squares?   . For  any  symmetric  matrix A   compute  the  ratio R  x  for  the  special  choice x    ...   . How is the sum of all entries a  related to   and   ? . W ith A          find a choice of x that gives a smaller R  x  than the bound     that comes from the diagonal entries. What is the minimum value of R  x  ? . If B is positive definite show from the Rayleigh quotient that the smallest eigenvalue of A  B is larger than the smallest eigenvalue of A . . If   and   are the smallest eigenvalues of A and B  show that the smallest eigen value   of A  B is at least as large as      .  Try the corresponding eigenvector x in the Rayleigh quotients. Note . Problems   and    are  perhaps  the  most  typical  and  most  important  results that come easily from Rayleighs principle but only with great difficulty from the eigenvalue equations themselves. . If B is positive definite show from the minimax principle  that the second small est eigenvalue is increased by adding B     A  B      A  . . If you  throw  away two rows  and  columns  of A   what  inequalities  do  you  expect between the smallest eigenvalue  of the new matrix and the original  s? . Find the minimum values of R  x   x    x  x   x   x    x   and R  x   x    x  x   x    x    x   . . Pro ve from equation  that R  x  is never larger than the largest eigenvalue   . . The minimax principle for   involves j dimensional subspaces S   Equivalent to equation     min    max     R  x   . a  If   is positive infer that every S  contains a vector x with R  x   . b  Deduce that S  contains a vector y  C   x with y  c  ACy  y  y  . c  Conclude that the j th eigenvalue of C  AC  from its minimax principle is also positiveproving again the law of inertia in Section .. . Show that the smallest eigenvalue   of Ax   Mx is not larger than the ratio a   m  of the corner entries. . Which particular subspace S  in Problem  gives the minimum value   ?  In other words over which S  is the maximum of R  x  equal to   ?  . Recommended From the zero submatrix decide the signs of the n eigenvalues A                                n      . . Constrained minimum Suppose the unconstrained minimum x  A   b happens to satisfy the constraint Cx  d . Verify that equation  correctly gives P     P   the correction term is zero. . The Finite Element Method There were two main ideas in the preceding section on minimum principles i  Solving Ax  b is equivalent to minimizing P  x     x  Ax  x  b . ii  Solving Ax    x is equivalent to minimizing R  x   x  Ax  x  x . No w we try to explain how these ideas can be applied. The story is a long one because these principles have been known for more than a century. In engineering problems like plate bending or physics problems like the ground state eigenfunction of an atom minimization was used to get a rough approximation to the true solution.  The approximations had to be rough the computers were human. The principles i and ii were there but they could not be implemented. Obviously the computer was going to bring about a revolution.  It was the method of finite differences that jumped ahead because it is easy to discretize a differential equation. Already in Section . derivatives were replaced by differences. The physical region is covered by a mesh  and u   f  x  became u      u   u     h  f  .   The s brought new ways to solve systems Au  f that are very large and very sparse algorithms and hardware are both much faster now. What we did not fully recognize was that even finite differences become incredibly complicated for real engineering problems  like the stresses on an airplane. The real difficulty is not to solve the equations but to set them up .  For an irregular region we piece the mesh together from triangles or quadrilaterals or tetrahedra.  Then we need a systematic way to approximate the underlying physical laws. The computer has to help not only in the solution of Au  f and Ax   x  but in its formulation. Y ou can guess what happened.  The old methods came back with a new idea and a new name.  The new name is the finite element method .  The new ides uses more of the power of the computerin constructing a discrete approximation  solving it  and displaying the resultsthan any other technique in scientific computation  . If the basic      idea is simple the applications can be complicated. For problems on this scale the one undebatable point is their costI am afraid a billion dollars would be a conservative estimate of the expense so far.  I hope some readers will be vigorous enough to master the finite element method and put it to good use. Trial Functions Starting from the classical RayleighRitz principle  I will introduce the new idea of finite elements.  The equation can be  u   f  x  with boundary conditions u     u     .  This problem is infinitedimensional the vector b is replaced by a function f  and the matrix A becomes  d   dx  .  We can write down the energy whose minimum is required replacing inner products v  f by integrals of v  x  f  x   Total energy P  v     v  A v  v  f       v  x   v   x  d x     v  x  f  x  dx .  P  v  is to be minimized over all functions v  x  that satisfy v     v     . The function that gives the minimum will be the solution u  x  .  The differential equation has been converted to a minimum principle and it only remains to integrate by parts    v   v   dx      v    dx   vv         so P  v          v   x    v  x  f  x   d x . The term vv  is zero at both limits because v is. Now   v   x   dx is positive like x  Ax . We are guaranteed a minimum. To compute the minimum exactly is equivalent to solving the differential equation ex actly. The RayleighRitz principle produces an ndimensional problem by choosing only n trial functions V   x  ... V   x  .  From all combinations V  y  V   x     y  V   x   we look for the particular combination call it U  that minimizes P  V  .  This is the key idea to minimize over a subspace of V s instead of over all possible v  x  . The function that gives the minimum is U  x  . We hope and expect that U  x  is near the correct u  x  . Substituting V for v  the quadratic turns into P  V         y  V    x     y  V    x    dx      y  V   x    y  V   x   f  x  dx .  The  trial  functions V are  chosen  in  advance.   That  is  the  key  step!   The  unknowns y  ... y  go  into  a  vector y .   Then P  V     y  A y  y  b is  recognized  as  one  of  the quadratics we are accustomed to.  The matrix entries A  are  V   V   dx  coefficient of y  y  . The components b  are  V  f dx . We can certainly find the minimum of   y  A y  y  b by solving Ay  b . Therefore the RayleighRitz method has three steps . Choose the trial functions V  ... V  . . Compute the coefficients A  and b  .  . Solve Ay  b to find U  x   y  V   x    y  V   x  . Everything depends on step .  Unless the functions V   x  are extremely simple the other steps will be virtually impossible. And unless some combination of the V  is close to the true solution u  x   those steps will be useless.  To combine both computability and accuracy the key idea that makes finite elements successful is the use of piecewise polynomials as the trial functions V  x  . Linear Finite Elements The simplest and most widely used finite element is piecewise linear .  Place nodes at the interior points x   h  x    h ... x   nh  just as for finite differences.  Then V  is the hat function that equals  at the node x   and zero at all the other nodes Figure .a.  It is concentrated in a small interval around its node and it is zero everywhere else including x   and x  . Any combination y  V     y  V  must have the value y  at node j the other V s are zero there so its graph is easy to draw Figure .b.                                     Step  computes the coefficients A    V   V   dx in the stiffness matrix A . The slope V   equals   h in the small interval to the left of x   and    h in the interval to the right. If these double intervals do not overlap the product V   V   is zero and A   .  Each hat function overlaps itself and only two neighbors Diagonal i  j A    V   V   dx     h   d x      h   d x   h . Offdiagonal i  j   A     V   V   dx     h     h  d x    h . Then the stiffness matrix is actually tridiagonal Stiffness matrix A   h                                    .   This looks just like finite differences!   It has led to a thousand discussions about the relation between these two methods. More complicated finite elementspolynomials of higher degree.  defined on triangles or quadrilaterals for partial differential equations also produce sparse matrices A .  You could think of finite elements as a systematic way to construct accurate difference equations on irregular meshes. The essential thing is the simplicity of these piecewise polynomials. Inside every element their slopes arc easy to find and to integrate. The components b  on the right side are new.  Instead of just the value of f at x   as for finite differences they are now an average of f around that point b    V  f dx . Then in step  we solve the tridiagonal system Ay  b  which gives the coefficients in the minimizing trial function U  y  V     y  V  .  Connecting all these heights y  by a broken line we have the approximate solution U  x  . Example . u    with u     u      and solution u  x   x  x  . The approximation will use three intervals and two hat functions with h    . The matrix A is  by . The right side requires integration of the hat function times f  x   . That produces twice the area   under the hat A           and b        . The solution to Ay  b is y         . The best U  x  is   V     V   which equals   at the mesh points. This agrees with the exact solution u  x   x  x        . In a more complicated example the approximation will not be exact at the nodes. But it is remarkably close.  The underlying theory is explained in the authors book An Analysis of the Finite Element Method see www.wellesleycambridge.com  written jointly with George Fix.  Other books give more detailed applications and the subject of finite elements has become an important part of engineering education.  It is treated in Introduction to Applied Mathematics  and also in my new book Applied Mathematics and Scientific Computing .   There we discuss partial differential equations  where the method really comes into its own. Eigenvalue Problems The RayleighRitz ideato minimize over a finitedimensional family of V s in place of all admissible v sis also useful for eigenvalue problems. The true minimum of the Rayleigh quotient is the fundamental frequency   .  Its approximate minimum   will be largerbecause the class of trial functions is restricted to the V s.   This step was completely natural and inevitable  to apply the new finite element ideas to this long established variational form of the eigenvalue problem. The best example of an eigenvalue problem has u  x   sin  x and       Eigenfunction u  x   u    u  with u     u      .  That function sin  x minimizes the Rayleigh quotient v  Av  v  v  Rayleigh quotient R  v      v  x   v   x  dx     v  x   d x      v   x   dx     v  x   d x . This is a ratio of potential to kinetic energy and they are in balance at the eigenvector. Normally this eigenvector would be unknown and to approximate it we admit only the trial candidates V  y  V     y  V   R  V       y  V      y  V     dx     y  V      y  V    dx  y  Ay y  M y . Now we face a matrix problem Minimize y  Ay  y  My . With M  I  this leads to the standard eigenvalue problem Ay   y .  But our matrix M will be tridiagonal because neighboring hat functions overlap. It is exactly this situation that brings in the general ized eigenvalue problem . The minimum value   will be the smallest eigenvalue of Ay   My .  That   will be close to and above   .  The eigenvector y will give the approximation U  y  V     y  V  to the eigenfunction. As in the static problem.  The method can be summarized in three steps   choose the V    compute A and M  and  solve Ay   My .  I dont know why that costs a billion dollars. Pr oblem Set . . Use three hat functions with h     to solve  u    with u     u     . Verify that the approximation U matches u  x  x  at the nodes. . Solve  u   x with u     u     . Then solve approximately with two hat func tions and h    . Where is the largest error? . Suppose  u    with the boundary condition u      changed to u      . This natural condition on u  need not be imposed on the trial functions V . With h     there is  an  extra halfhat V    which  goes  from    to    between x    and x  . Compute A     V     d x and f     V  dx .   Solve Ay  f for the finite element solution y  V   y  V   y  V  . . Solve  u    with a single hat function but place its node at x    instead of x    . Sk etch this function V  .  With boundary conditions u     u      compare the finite element approximation with the true u  x  x  . . Galerkins method starts with the differential equation say  u   f  x   instead of the energy P .  The trial solution is still u  y  V   y  V     y  V   and the y s are chosen to make the difference between  u  and f orthogonal to every V   Galerkin    y  V    y  V    y  V    V  dx   f  x  V   x  dx .   integrate the left side by parts to reach Ay  f  proving that Galerkin gives the same A and  f  as RayleighRitz for symmetric problems . . A basic identity for quadratics shows y  A   b as minimizing P  y     y  A y  y  b     y  A   b   A  y  A   b     b  A   b . The minimum over a subspace of trial functions is at the y nearest to A   b .  That makes the first term on the right as small as possible it is the key to convergence of U to u . If A  I and b          which multiple of V         gives the smallest value of P  y     y  y  y  ? . F or a single hat function V  x  centered at x     compute A    V    dx and M   V  dx . In the  by  eigenvalue problem is   A  M larger or smaller than the true eigenvalue     ? . F or the hat functions V  and V  centered at x  h    and x   h     compute the  by  mass matrix M    V  V  dx  and solve the eigenvalue problem Ax   Mx . . What is the mass matrix M    V  V  dx for n hat functions with h      ?    .    Introduction One aim of this book is to explain the useful parts of matrix theory.   In comparison with older texts in abstract linear algebra the underlying theory has not been radically changed.  One of the best things about the subject is that the theory is really essential for the applications.  What is different is the change in emphasis which comes with a new point of view.  Elimination becomes more than just a way to find a basis for the row space and the GramSchmidt process is not just a proof that every subspace has an orthonormal basis. Instead we really need these algorithms. And we need a convenient description A  LU or A  QR  of what they do. This chapter will take a few more steps in the same direction. I suppose these steps are governed by computational necessity rather than by elegance and I dont know whether to apologize for that it makes them sound very superficial and that is wrong. They deal with the oldest and most fundamental problems of the subject Ax  b and Ax   x  but they are continually changing and improving.  In numerical analysis there is a survival of the fittest and we want to describe some ideas that have survived so far.  They fall into three groups .   Techniques for Solving Ax  b . Elimination is a perfect algorithm  except when the particular problem has special propertiesas almost every problem has. Sec tion . will concentrate on the property of sparseness when most of the entries in A are zero.  We develop iterative rather than direct methods for solving Ax  b .  An iter ative method is selfcorrecting and never reaches the exact answer.  The object is to get close more quickly than elimination.  In some problems that can be done in many others elimination is safer and faster if it takes advantage of the zeros. The competition is far from over and we will identify the spectral radius that controls the speed of con vergence to x  A   b . .  Techniques for Solving Ax   x . The eigenvalue problem is one of the out   standing successes of numerical analysis. It is clearly defined its importance is obvious but until recently no one knew how to solve it.  Dozens of algorithms have been sug gested and everything depends on the size and the properties of A and on the number of eigenvalues that are wanted.  You can ask LAPACK for an eigenvalue subroutine without knowing its contents but it is better to know. We have chosen two or three ideas that have superseded almost all of their predecessors the QR algorithm  the family of  power methods  and the preprocessing of a symmetric matrix to make it tridiagonal . The first two methods are iterative and the last is direct.  It does its job in a finite number of steps but it does not end up with the eigenvalues themselves. This produces a much simpler matrix to use in the iterative steps. .   The  Condition  Number  of  a  Matrix. Section  .  attempts  to  measure  the sensitivity of a problem  If A and b are slightly changed how great is the effect on x  A   b ? Before starting on that question we need a way to measure A and the change  A .  The length of a vector is already defined and now we need the norm of a matrix . Then the condition number  and the sensitivity of A will follow from multiplying the norms of A and A   . The matrices in this chapter are square . . Matrix Norm and Condition Number An error and a blunder are very different things.  An error is a small mistake probably unavoidable even by a perfect mathematician or a perfect computer. A blunder is much more serious and larger by at least an order of magnitude.  When the computer rounds oft a number after  bits  that is an error  But when a problem is so excruciatingly sensitive that this roundoff error completely changes the solution then almost certainly someone has committed a blunder.  Our goal in this section is to analyze the effect of errors so that blunders can be avoided. We are actually continuing a discussion that began in Chapter  with A         .   and B    .        . We claimed that B is wellconditioned and not particularly sensitive to roundoffexcept that if Gaussian elimination is applied in a stupid way the matrix becomes completely vulnerable. It is a blunder to accept .  as the first pivot and we must insist on a larger and safer choice by exchanging the rows of B .  When partial pivoting is built into the elimination algorithm the computer automatically looks for the largest pivot . Then the natural resistance to roundoff error is no longer compromised. How  do  we  measure  this  natural  resistance  and  decide  whether  a  matrix  is  well conditioned  or  illconditioned?   If  there  is  a  small  change  in b or  in A   how  large  a change does that produce in the solution x ?  We begin with a change in the righthand side  from b to b   b .  This error might come from experimental data or from roundoff.  We may suppose that  b is small but its direction is outside our control. The solution is changed from x to x   x  Error equation A  x   x   b   b  so by subtraction A   x    b .  An error  b leads to  x  A    b .  There will be a large change in the solution x when A   is large A is nearly singular. The change in x is especially large when  b points in the direction that is amplified most by A   . Suppose A is symmetric and its eigenvalues are positive         .  Any vector  b is a combination of the corresponding unit eigenvectors x  ... x  . The worst error  x  coming from A    is in the direction of the first eigenvector x   Worst error If  b   x   then  x   b   .  The error   b  is amplified by       which is the largest eigenvalue of A   .   This amplification is greatest when   is near zero and A is nearly singular . Measuring sensitivity entirely by   has a serious drawback. Suppose we multiply all the entries of A by  then   will be multiplied by  and the matrix will look much less singular.  This offends our sense of fair play such a simple rescaling cannot make an illconditioned matrix well. It is true that  x will be  times smaller but so will the solution x  A   b .  The relative error   x    x  will be the same.  Dividing by  x  normalizes the problem against a trivial change of scale.  At the same time there is a normalization for  b  our problem is to compare the relative change   b    b  with the relative error   x    x  . The worst case is when   x  is largewith  b in the direction of the eigenvector x  and when  x  is small. The true solution x should be as small as possible compared to the true b . This means that the original problem Ax  b should be at the other extreme  in the direction of the last eigenvector x   if b  x   then x  A   b  b    . It is this combination b  x  and  b   x   that makes the relative error as large as possible. These are the extreme cases in the following inequalities A F or a positive definite matrix the solution x  A   b and the error  x  A    b always satisfy  x   b    and   x    b    and   x   x         b   b  .  The ratio c       is the condition number of a positive definite matrix A . Example . The eigenvalues of A are approximately         and     A         .   has condition number about c      .   We  must  expect  a  violent  change  in  the  solution  from  ordinary  changes  in  the  data. Chapter  compared the equations Ax  b and Ax   b   u  v   u   .  v   u  v   u   .  v   .  . The righthand sides are changed only by   b   .      . At the same time the solution goes from u   v   to u  v  . This is a relative error of   x   x                       which equals          b   b  . W ithout having made any special choice of the perturbation there was a relatively large change  in  the  solution.   Our x and  b make    angles  with  the  worst  cases  which accounts for the missing  between     and the extreme possibility c      . If A  I or even if A  I   its condition number is c        . By compari son the determinant is a terrible measure of illconditioning . It depends not only on the scaling but also on the order n  if A  I   then the determinant of A is    .  In fact this nearly singular matrix is as wellconditioned as possible. Example . The n by n finite difference matrix A has     and       n   A                                    . The condition number is approximately c  A     n   and this time the dependence on the order n is genuine.  The better we approximate  u   f  by increasing the number of unknowns the harder it is to compute the approximation.  At a certain crossover point an increase in n will actually produce a poorer answer. Fortunately  for  the  engineer  this  crossover  occurs  where  the  accuracy  is  already pretty good. Working in single precision a typical computer might make roundoff errors of order    .  With n   unknowns and c   the error is amplified at most to be of order    which is still more accurate than any ordinary measurements.  But there will be trouble with  unknowns or with a        approximation to d  u  dx   f  x  for which the condition number grows as n  .  Unsymmetric Matrices Our analysis so far has applied to symmetric matrices with positive eigenvalues.  We could  easily  drop  the  positivity  assumption  and  use  absolute  values    .   But  to  go        beyond symmetry as we certainly want to do there will have to be a major change. This is easy to see for the very unsymmetric matrices A          and A           .  The eigenvalues all equal one but the proper condition number is not       . The relative change in x is not bounded by the relative change in b . Compare x      when b       x       when b       . A  change in b has produced a hundredfold change in x  the amplification factor is   . Since c represents an upper bound the condition number must be at least . The difficulty here is that a large offdiagonal entry in A means an equally large entry in A   . Expecting A   to get smaller as A gets bigger is often wrong. For a proper definition of the condition number we look back at equation .  We were trying to make x small and b  Ax large. When A is not symmetric the maximum of  Ax    x  may be found at a vector x that is not one of the eigenvectors . This maximum is an excellent measure of the size of A . It is the norm of A . B The norm of A is the number  A  defined by  A   max      Ax   x  .  In other words  A  bounds the amplifying power of the matrix  Ax  A  x  for all vectors x .  The matrices A and A   in equation  have norms somewhere between  and . They can be calculated exactly but first we want to complete the connection between norms and condition numbers. Because b  Ax and  x  A    b  equation  gives  b  A  x  and   x  A     b  .  This is the replacement for equation  when A is not symmetric. In the symmetric case  A  is the same as    and  A    is the same as     . The correct replacement for      is the product  A  A    which is the condition number. C The condition number of A is c   A  A    . The relative error satisfies  x from  b   x   x   c   b   b  directly from equation .  If we perturb the matrix A instead of the righthand side b  then  x from  A   x   x   x   c   A   A  from equation  below.    What is remarkable is that the same condition number appears in equation  when the matrix itself is perturbed If Ax  b and  A   A  x   x   b  then by subtraction A  x   A  x   x     or  x   A     A  x   x  . Multiplying by  A amplifies a vector by no more than   A   and multiplying by A   amplifies by no more than  A    . Then   x    A     A  x   x   which is   x   x   x   A     A   c   A   A  .  These inequalities  mean  that  roundoff  error  comes  from  two  sources.   One  is  the natural  sensitivity of  the  problem  measured  by c .   The  other  is  the  actual  error  b or  A .   This  was  the  basis  of  Wilkinsons  error  analysis.   Since  elimination  actually produces approximate factors L  and U   it solves the equation with the wrong matrix A   A  L  U  instead  of  the  right  matrix A  LU .   He  proved  that  partial  pivoting controls  A so the burden of the roundoff error is carried by the condition number c . A Formula for the Norm The norm of A measures the largest amount by which any vector eigenvector or not is amplified by matrix multiplication  A   max   Ax    x   . The norm of the identity matrix is . To compute the norm square both sides to reach the symmetric A  A   A    max  Ax    x    max x  A  Ax x  x .  D  A  is the square root of the largest eigenvalue of A  A   A       A  A  . The vector that A amplifies the most is the corresponding eigenvector of A  A  x  A  Ax x  x  x     x  x  x     A  A    A   .  Figure . shows an unsymmetric matrix with eigenvalues        and norm  A    . . In this case A   has the same norm. The farthest and closest points Ax on the ellipse come from eigenvectors x of A  A  not of A . Note  . The norm and condition number are not actually computed in practice  only estimated There is not time to solve an eigenvalue problem for    A  A  . Note  . In the leastsquares equation A  Ax  A  b  the condition number c  A  A  is the square of c  A  .  Forming A  A can turn a healthy problem into a sick one.  It may be necessary to orthogonalize A by GramSchmidt instead of computing with A  A . Note  . The singular values of A in the SVD are the square roots of the eigenvalues of A  A .  By equation  another formula for the norm is  A     .  The orthogonal U and V leave lengths unchanged in  Ax    U  V  x  . So the largest  Ax    x  comes from the largest  in the diagonal matrix  .                                                                                                    Note  . Roundoff error also enters Ax   x . What is the condition number of the eigen value problem? The condition number of the diagonalizing S measures the sensitivity of the eigenvalues .  If  is an eigenvalue of A  E   then its distance from one of the eigenvalues of A is      S  S    E   c  S   E  .  W ith orthonormal eigenvectors and S  Q  the eigenvalue problem is perfectly condi tioned c  Q   .  The change   in the eigenvalues is no greater than the change  A . Therefore the best case is when A is symmetric or more generally when AA   A  A . Then A is a normal matrix its diagonalizing S is an orthogonal Q Section .. If x  is the k th column of S and y  is the k th row of S    then   changes by     y  Ex   terms of order  E   .  In practice y  Ex  is a realistic estimate of   .  The idea in every good algorithm is to keep the error matrix E as small as possibleusually by insisting as in the next section on orthogonal matrices at every step of the computation of  . Pr oblem Set . . For  an  orthogonal  matrix Q   show  that  Q     and  also c  Q   .   Orthogonal matrices and their multiples  Q  are the only perfectly conditioned matrices. . Which famous inequality gives   A  B  x  Ax    Bx   and why does it follow from equation  that  A  B  A    B  ? . Explain why  ABx  A  B  x   and deduce from equation  that  AB  A  B  . Show that this also implies c  AB   c  A  c  B  .   . For the positive definite A          compute  A           A      and c  A        .  Find a righthand side b and a perturbation  b so that the error is the worst possible   x    x   c   b    b  . . Sho w that if  is any eigenvalue of A  Ax   x  then    A  . . The matrices in equation  have norms between  and . Why? . Comparing the eigenvalues of A  A and AA   prove that  A    A   . . For a positive definite A  the Cholesky decomposition is A  LDL   R  R  where R   DL  . Show directly from equation  that the condition number of c  R  is the square root of c  A  .  Elimination without row exchanges cannot hurt a positive definite matrix since c  A   c  R   c  R  . . Show that max    is not a true norm by finding  by  counterexamples to    A  B      A     B  and    AB      A     B  . . Sho w that the eigenvalues of B         are     the singular values of A . Hint  Try B  . . a  Do A and A   ha ve the same condition number c ? b  In parallel with the upper bound  on the error prove a lower bound   x   x    c   b   b  . Consider A   b  x instead of Ax  b . . Find  the  norms   and  condition  numbers      of  these  positive  definite matrices                               . . Find the  norms  and  condition  numbers  from  the  square  roots  of    A  A  and    A  A                                 . . Pro ve that the condition number  A  A    is at least . . Why is I the only symmetric positive definite matrix that has       ? Then the only matrices with  A    and  A      must have A  A  I .  They are matrices. . Orthogonal matrices have norm  Q   . If A  QR  show that  A  R  and also  R  A  . Then  A    Q  R  . Find an example of A  LU with  A    L  U  .  . Suggested by Moler and Van Loan Compute b  Ay and b  Az when b   .  .   A   .  .  .  .   y   .   .   z   .    .   . Is y closer than z to solving Ax  b ?  Answer in two ways  Compare the residual b  Ay to b  Az . Then compare y and z to the true x         Sometimes we want a small residual sometimes a small  x . Pr oblems  are about vector norms other than the usual  x    x  x . . The    norm is  x     x       x   . The    norm is  x    max  x   . Compute  x    x   and  x   for the vectors x             and x   .  .  .  .  .   . . Prove that  x    x  x   .  Show from the Schwarz inequality that the ratios  x    x   and  x     x  are never larger than  n . Which vector  x  ... x   gives ratios equal to  n ? . All vector norms must satisfy the triangle inequality . Prove that  x  y    x     y   and  x  y    x     y   . . Compute the exact inverse of the Hilbert matrix A by elimination.  Then compute A   again by rounding all numbers to three figures In MATLAB  A  hilb                         . . F or the same A  compute b  Ax for x         and x         .   . A small change  b produces a large change  x . . Compute   and   for the  by  Hilbert matrix a      i  j    .  If Ax  b with  b    how large can  x  be?  If b has roundoff error less than     how large an error can this cause in x ? . If you know L  U  Q  and R  is it faster to solve LU x  b or QRx  b ? . Choosing the largest available pivot in each column partial pivoting factor each A into PA  LU  A            and A                             .   . Find the LU factorization of A       .   On your computer  solve by elimination when                               x  x          . The true x is      .  Make a table to show the error for each  .  Exchange the two equations and solve againthe errors should almost disappear. . Computation of Eigenvalues There is no one best way to find the eigenvalues of a matrix.  But there are certainly some terrible ways which should never be tried and also some ideas that do deserve a permanent place. We begin by describing one very rough and ready approach the power method   whose  convergence  properties  are  easy  to  understand.   We  added  a  graphic animation with sound to the course page web.mit.edu.  to show the power method in action. We move steadily toward a more sophisticated algorithm which starts by making a symmetric matrix tridiagonal and ends by making it virtually diagonal. That second step is done by repeating GramSchmidt so it is known as the QR method . The ordinary power method operates on the principle of a difference equation.   It starts  with  an  initial  guess u  and  then  successively  forms u   Au   u   Au    and in  general u     Au  .   Each  step  is  a  matrixvector  multiplication.   After k steps  it produces u   A  u   although the matrix A  will never appear.  The essential thing is that multiplication by A should be easyif the matrix is large it had better be sparse because convergence to the eigenvector is often very slow. Assuming A has a full set of eigenvectors x  ... x   the vector u  will be given by the usual formula Eigenvectors weighted by   u   c     x     c     x  . Suppose the largest eigenvalue   is all by itself there is no other eigenvalue of the same magnitude and               . Then as long as the initial guess u  contained some component of the eigenvector x   so that c     this component will gradually dominate in u   u      c         x     c             x     c  x  .  The vectors u  point more and more accurately toward the direction of x  . Their conver gence factor is the ratio r             . It is just like convergence to a steady state for a Markov matrix except now   may not equal . The scaling factor    in equation  prevents u  from growing very large or very small in case       or      .  Often we can just divide each u  by its first component   before taking the next step. With this simple scaling the power method u     Au     converges to a multiple of x  . The scaling factors   will approach   . Example . The u  approach the eigenvector                when A            is the matrix of population shifts in Section . u        u    .  .    u    .  .    u    .  .    u    .  .   . If r             is close to  then convergence is very slow.  In many applications r  .  which means that more than  iterations are needed to achieve one more digit. The example had r  .  and it was still slow. If r   which means             then convergence will probably not occur at all. That happens in the applet with sound for a complex conjugate pair        . There are several ways to get around this limitation and we shall describe three of them .  The bloc k power method works with several vectors at once in place of u  .  If we multiply p orthonormal vectors by A  and then apply GramSchmidt to orthogonal ize them againthat is a single step of the methodthe convergence ratio becomes r              . We will obtain approximations to p different eigenvalues and their elgenvectors. .  The in verse power method operates with A   instead of A . A single step is v     A   v   which means that we solve the linear system Av     v  and save the factors L and U !.  Now we converge to the smallest eigenvalue   and its eigenvector x   provided          .   Often it is   that is wanted in the applications  and then inverse iteration is an automatic choice. .  The shifted inverse power method is best of all. Replace A by A   I . Each eigen value is shifted by   and the convergence factor for the inverse method will change to r                . If  is a good approximation to    r  will be very small and the convergence is enormously accelerated.  Each step of the method solves  A   I  w     w   w   c  x          c  x            c  x         . When  is close to     the first term dominates after only one or two steps.   If   has already been computed by another algorithm such as QR  then  is this computed value.  One standard procedure is to factor A   I into LU and to solve U x       ...   by backsubstitution. If   is not already approximated the shifted inverse power method has to generate its own choice of  . We can vary     at every step if we want to so  A    I  w     w  .   When A is symmetric a very accurate choice is the Rayleigh quotient  shift by    R  w    w   Aw  w   w  . This quotient R  x  has a minimum at the true eigenvector x  . Its graph is like the bottom of a parabola so the error      is roughly the square of the error in the eigenvector. The convergence factors                are themselves converging to zero.  Then these Rayleigh quotient shifts give cubic convergence of   to   .  Tridiagonal and Hessenberg Forms The power method is reasonable only for a matrix that is large and sparse.  When too many entries are nonzero this method is a mistake.  Therefore we ask whether there is any simple way to create zeros . That is the goal of the following paragraphs. It should be said that after computing a similar matrix Q   AQ with more zeros than A we do not intend to go back to the power method.  There are much more powerful variants and the best of them seems to be the QR algorithm. The shifted inverse power method has its place at the very end in finding the eigenvector. The first step is to pro duce quickly as many zeros as possible using an orthogonal matrix Q . If A is symmetric then so is Q   AQ . No entry can become dangerously large because Q preserves lengths. To go from A to Q   AQ  there are two main possibilities We can produce one zero at every step as in elimination or we can work with a whole column at once. For a single zero it is easy to use a plane rotation as illustrated in equation  found near the end of this section that has cos  and sin  in a  by  block.  Then we could cycle through all the entries below the diagonal choosing at each step a rotation  that will produce a zero this is Jacobis method . It fails to diagonalize A after a finite number of rotations since the zeros from early steps will be destroyed when later zeros are created. T o  preserve  the  zeros  and  stop  we  have  to  settle  for  less  than  a  triangular  form. The Hessenberg form accepts one nonzero diagonal below the main diagonal .  If a Hessenberg matrix is symmetric it only has three nonzero diagonals. A series of rotations in the right planes will produce the required zeros. Householder found a new way to accomplish exactly the same thing. A Householder transformation is a reflection matrix determined by one vector v  Householder matrix H  I   vv   v   . Often v is normalized to become a unit vector u  v   v   and then H becomes I   uu  . In either case H is both symmetric and orthogonal  H  H   I   uu   I   uu    I   uu    uu  uu   I .                                             Thus H  H   H   . Householders plan was to produce zeros with these matrices and its success depends on the following identity Hx    z  E Suppose z is the column vector     ...       x   and v  x   z . Then Hx    z       ...   . The vector Hx ends in zeros as desired. The proof is to compute Hx and reach   z  Hx  x   vv  x  v    x   x   z    x   z   x  x   z    x   z   x   x   z  because x  x        z .  This identity can be used right away on the first column of A .  The final Q   AQ is allowed one nonzero diagonal below the main diagonal Hessenberg form.  Therefore only the entries strictly below the diagonal will be involved  x       a  a  . . . a         z         . . .        Hx          . . .       .  At this point Householders matrix H is only of order n   so it is embedded into the lower righthand corner of a fullsize matrix U   U                           H           U     and U    AU          a                                  . The first stage is complete and U    AU  has the required first column.  At the second stage x consists of the last n   entries in the second column three bold stars.  Then H  is of order n  . When it is embedded in U   it produces U                                    H              U     U     U    AU   U                                              . U  will  take  care  of  the  third  column.   For  a    by    matrix  the  Hessenberg  form  is achieved it has six zeros. In general Q is the product of all the matrices U  U   U     and the number of operations required to compute it is of order n  .   Example . to change a   a  to zero A                              x       v       H          . Embedding H into Q  the result Q   AQ is tridiagonal Q                    Q   AQ                   . Q   AQ is a matrix that is ready to reveal its eigenvaluesthe QR algorithm is ready to beginbut we digress for a moment to mention two other applications of these same Householder matrices H . . The GramSchmidt factorization A  QR .  Remember that R is to be upper trian gular. We no longer have to accept an extra nonzero diagonal below the main one since no matrices are multiplying on the right to spoil the zeros.  The first step in constructing Q is to work with the whole first column of A  x       a  a  . . . a         z         . . .        v  x   x  z  H   I   vv   v   . The first column of H  A equals  x  z .  It is zero below the main diagonal and it is the first column of R .  The second step works with the second column of H  A  from the pivot on down and produces an H  H  A which is zero below that pivot. The whole algorithm is like elimination but slightly slower.  The result of n   steps is an upper triangular R  but the matrix that records the steps is not a lower triangular L . Instead it is the product Q  H  H   H     which can be stored in this factored form keep only the v s and never computed explicitly.  That completes GramSchmidt. . The singular value decomposition U  AV   . The diagonal matrix  has the same shape as A  and its entries the singular values are the square roots of the eigenval ues of A  A . Since Householder transformations can only prepare for the eigenvalue problem we cannot expect them to produce  . Instead they stably produce a bidi agonal matrix  with zeros everywhere except along the main diagonal and the one above. The first step toward the SVD is exactly as in QR above x is the first column of A  and H  x is zero below the pivot.  The next step is to multiply on the right by an H     which will produce zeros as indicated along the first row A  H  A                     H  AH                         .  Then two final Householder transformations quickly achieve the bidiagonal form H  H  AH                           and H  H  AH    H                           . The QR Algorithm for Computing Eigenvalues The algorithm is almost magically simple. It starts with A   factors it by GramSchmidt into Q  R   and then reverses the factors  A   R  Q  .  This new matrix A  is similar to the original one because Q    A  Q   Q     Q  R   Q   A  . So the process continues with no change in the eigenvalues All A  are similar A   Q  R  and then A     R  Q  .  This equation describes the unshifted QR algorithm  and almost always A  approaches a triangular form Its diagonal entries approach its eigenvalues which are also the eigen values of A  . If there was already some processing to obtain a tridiagonal form then A  is connected to the absolutely original A by Q   AQ  A  . As it stands the QR algorithm is good but not very good. To make it special it needs two refinements  We must allow shifts to A     I   and we must ensure that the QR factorization at each step is very quick. .  The Shifted Algorithm. If the number   is close to an eigenvalue the step in equation  should be shifted immediately by   which changes Q  and R   A     I  Q  R  and then A     R  Q     I .  This matrix A    is similar to A  always the same eigenvalues Q    A  Q   Q     Q  R     I  Q   A    . What happens in practice is that the  n  n  entry of A  the one in the lower righthand corneris the first to approach an eigenvalue. That entry is the simplest and most pop ular choice for the shift   .  Normally this produces quadratic convergence and in the symmetric case even cubic convergence to the smallest eigenvalue.  After three or four   steps of the shifted algorithm the matrix A  looks like this A                                with    . W e accept the computed    as a very close approximation to the true   .  To find the next  eigenvalue  the QR algorithm  continues  with  the  smaller  matrix    by    in  the illustration in the upper lefthand corner.  Its subdiagonal elements will be somewhat reduced by the first QR steps and another two steps are sufficient to find   . This gives a systematic procedure for finding all the eigenvalues.  In fact the QR method is now completely described .  It only remains to catch up on the eigenvectorsthat is a single inverse power stepand to use the zeros that Householder created. . When A  is tridiagonal or Hessenberg each QR step is very fast. The GramSchmidt process factoring into QR  takes O  n   operations for a full matrix A . For a Hessenberg matrix this becomes O  n    and for a tridiagonal matrix it is O  n  . Fortunately each new A  is again in Hessenberg or tridiagonal form Q  is Hessenberg Q   A  R                                                                 . Y ou can easily check that this multiplication leaves Q  with the same three zeros as A  . Hessenberg times triangular is Hessenberg . So is triangular times Hessenberg A  is Hessenberg A   R  Q                                                               . The symmetric case is even better since A   Q    A  Q   Q   A  Q  stays symmetric. By the reasoning just completed A  is also Hessenberg.  So A  must be tridiagonal .  The same applies to A   A  ...  and every QR step begins with a tridiagonal matrix . The last point is the factorization itself producing the Q  and R  from each A  or really from A     I .  We may use Householder again but it is simpler to annihilate each subdiagonal element in turn by a plane rotation P  . The first is P   Rotation to kill a  P  A        cos   sin  sin  cos              a     a                    The      entry in this product is a  sin   a  cos   and we choose the angle  that makes this combination zero. The next rotation P  is chosen in a similar way to remove the      entry of P  P  A  . After n   rotations we have R   Triangular factor R   P     P  P  A  .  Books on numerical linear algebra give more information about this remarkable algo rithm in scientific computing.  We mention one more method Arnoldi in ARPACK for large sparse matrices. It orthogonalizes the Krylov sequence x  Ax  A  x ... by Gram Schmidt. If you need the eigenvalues of a large matrix dont use det  A   I  ! Pr oblem Set . . For the matrix A         with eigenvalues     and     apply the power method u     Au  three times to the initial guess u       .  What is the limiting vector u  ? . F or the same A and the initial guess u        compare three inverse power steps to one shifted step with   u   Au   u   u   u     A   u             u  or u   A   I    u  . The limiting vector u  is now a multiple of the other eigenvector      . . Explain why          controls the convergence of the usual power method.  Con struct a matrix A for which this method does not converge . . The Markov matrix A            has    and .  and the power method u   A  u  converges  to       .   Find  the  eigenvectors  of A   .   What  does  the  inverse  power method u    A   u  converge to after you multiply by .   ? . Sho w that for any two different vectors of the same length  x    y   the House holder transformation with v  x  y gives Hx  y and Hy  x . . Compute    x   v  x   z  and H  I   vv   v  v  Verify Hx    z  x      and z      . . Using Problem  find the tridiagonal HAH   that is similar to A                                  . Show that starting from A           the unshifted QR algorithm produces only the modest improvement A             . . Apply to the following matrix A a single QR step with the shift   a  which in this case means without shift since a   .  Show that the offdiagonal entries go from sin  to  sin    which is cubic convergence . A   cos  sin  sin    . . Check that the tridiagonal A      is left unchanged by the QR algorithm. It is one of the rare counterexamples to convergence so we shift. . Show by induction that without shifts  Q  Q   Q   R   R  R   is exactly the QR factorization of A    .  This identity connects QR to the power method and leads to an explanation of its convergence. If                  these eigenvalues will gradually appear on the main diagonal. . Choose sin  and cos  in the rotation P to triangularize A  and find R  P  A   cos   sin  sin  cos                 R . . Choose sin  and cos  to make P  AP    triangular same A .  What are the eigen values? . When A is multiplied by P  plane rotation which entries are changed? When P  A is multiplied on the right by P     which entries are changed now? . How many multiplications and how many additions are used to compute PA ?  A careful organization of all the rotations gives   n  multiplications and additions the same as for QR by reflectors and twice as many as for LU . . Turning a robot hand A robot produces any  by  rotation A from plane rotations around  the x  y   and z axes.   If P  P  P  A  I   the  three  robot  turns  are  in A  P    P    P    . The three angles are Euler angles . Choose the first  so that P  A     cos   sin   sin  cos                             is zero in the      position. . Iterative Methods for Ax  b In contrast to eigenvalues for which there was no choice we do not absolutely need an iterative method to solve Ax  b .  Gaussian elimination will reach the solution x in  a finite number of steps  n    for a full matrix less than that for the large matrices we actually meet Often that number is reasonable.  When it is enormous we may have to settle for an approximate x that can be obtained more quicklyand it is no use to go part way through elimination and then stop. Our goal is to describe methods that start from any initial guess x   and produce an improved approximation x    from the previous x  . We can stop when we want to. An iterative method is easy to invent by splitting the matrix A .  If A  S  T  then the equation Ax  b is the same as Sx  T x  b . Therefore we can try Iteration from x  to x    Sx     T x   b .  There is no guarantee that this method is any good. A successful splitting S  T satisfies two different requirements .  The new vector x    should be easy to compute .  Therefore S should be a simple and invertible! matrix it may be diagonal or triangular. .  The sequence x  should converge to the true solution x . If we subtract the iteration in equation  from the true equation Sx  T x  b  the result is a formula involving only the errors e   x  x   Error equation Se     Te  .  This is just a difference equation. It starts with the initial error e   and after k steps it produces the new error e    S   T   e  .  The question of convergence is exactly the same as the question of stability x   x exactly when e   . F The iterative method in equation  is convergent if and only if every eigenvalue of S   T satisfies     .  Its rate of convergence depends on the maximum size of     Spectral radius rho   S   T   max      .  Remember that a typical solution to e     S   Te  is a combination of eigenvectors Error after k steps e   c     x     c     x  .  The largest     will eventually be dominant so the spectral radius       will govern the rate at which e  converges to zero. We certainly need   . Requirements  and  above are conflicting.  We could achieve immediate conver gence with S  A and T   the first and only step of the iteration would be Ax   b . In that case the error matrix S   T is zero its eigenvalues and spectral radius are zero and the rate of convergence usually defined as  log   is infinite. But Ax   b may be hard to solve that was the reason for a splitting. A simple choice of S can often succeed and we start with three possibilities      . S  diagonal part of A  Jacobis method . . S  triangular pail of A  GaussSeidel method . . S  combination of  and   successive overrelaxation or SOR. S is also called a preconditioner  and its choice is crucial in numerical analysis. Example  Jacobi . Here S is the diagonal part of A  A           S       T             S   T          . If the components of x are v and w  the Jacobi step Sx     T x   b is  v     w   b   w     v   b   or  v w               v w     b    b     . The decisive matrix S   T has eigenvalues     which means that the error is cut in half one more binary digit becomes correct at every step.  In this example which is much too small to be typical the convergence is fast. For a larger matrix A  there is a very practical difficulty. The Jacobi iteration re quires us to keep all components of x  until the calculation of x    is complete .  A much more natural idea which requires only half as much storage is to start using each component of the new x    as soon as it is computed x    takes the place of x  a com ponent at a time. Then x  can be destroyed as fast as x    is created The first component remains as before New x  a   x         a  x   a  x   a   x     b  . The next step operates immediately with this new value of x   to find  x       New x  a   x        a   x        a  x   a   x     b  . And the last equation in the iteration step will use new values exclusively New x  a   x         a   x   a   x   a    x         b  . This  is  called  the GaussSeidel  method   even  though  it  was  apparently  unknown  to Gauss and not recommended by Seidel.  That is a surprising bit of history because it is not a bad method.  When the terms in x    are moved to the lefthand side S is seen as the lower triangular part of A . On the righthand side T is strictly upper triangular. Example  GaussSeidel . Here S   T has smaller eigenvalues A           S            T             S   T          .  A single GaussSeidel step takes the components v  and w  into  v     w   b   w     v   b   or          x               x   b . The eigenvalues of S   T are   and .  The error is divided by  every time so a sin gle GaussSeidel step is worth two Jacobi steps .  Since both methods require the same number of operationswe just use the new value instead of the old and actually save on storagethe GaussSeidel method is better. This rule holds in many applications even though there are examples in which Jacobi converges and GaussSeidel fails or conversely.   The symmetric case is straightfor ward When all a    GaussSeidel converges if and only if A is positive definite. It was discovered during the years of hand computation probably by accident that convergence is faster if we go beyond the GaussSeidel correction x     x  .  Roughly speaking those approximations stay on the same side of the solution x .  An overrelax ation factor  moves us closer to the solution.  With    we recover GaussSeidel with     the  method  is  known  as successive  overrelaxation SOR.  The  optimal choice of  never exceeds . It is often in the neighborhood of .. T o describe overrelaxation let D  L  and U be the parts of A on below and above the diagonal respectively.  This splitting has nothing to do with the A  LDU of elim ination.  In fact we now have A  L  D  U .  The Jacobi method has S  D on the lefthand side and T   L  U on the righthand side.  GaussSeidel chose S  D  L and T   U . To accelerate the convergence we move to Overrelaxation  D   L  x          D   U  x    b .  Re gardless of   the matrix on the left is lower triangular and the one on the right is upper triangular. Therefore x    can still replace x   component by component as soon as it is computed. A typical step is a   x       a   x        a   x   a    x          a  x   a  x     b   . If the old guess x  happened to coincide with the true solution x  then the new guess x    would stay the same and the quantity in brackets would vanish. Example  SOR . For the same A          each overrelaxation step is        x                     x    b . If we divide by   these two matrices are the S and T in the splitting AS  T  the iteration is back to Sx     T x   b . The crucial matrix L  S   T is L                                                    .      The optimal  makes the largest eigenvalue of L its spectral radius as small as possible. The whole point of overrelaxation is to discover this optimal  .   The product of the eigenvalues equals det L  det T  det S       det L        . Al ways det S  det D because L lies below the diagonal and det T  det      D be cause U lies above the diagonal. Their product is det L        . This explains why we never go as far as   .  The product of the eigenvalues would be too large and the iteration could not converge. We also get a clue to the behavior of the eigenvalues At the optimal  the two eigenvalues are equal.  They must both equal    so their product will match det L .  This value of  is easy to compute because the sum of the eigenvalues always agrees with the sum of the diagonal entries the trace of L  Optimal                               .  This quadratic equation gives             . . The two equal eigenvalues are approximately      .  which is a major reduction from the GaussSeidel value     at   .  In this example the right choice of  has again doubled the rate of convergence because       . . If  is further increased the eigenvalues become a complex conjugate pairboth have        which is now increasing with  . The discovery that such an improvement could be produced so easily almost as if by magic was the starting point for  years of enormous activity in numerical analysis. The first problem was solved in Youngs  thesisa simple formula for the optimal  .   The key step was to connect the eigenvalues  of L to the eigenvalues  of the original Jacobi matrix D     L  U  . That connection is expressed by Formula for                .  This is valid for a wide class of finite difference matrices and if we take    Gauss Seidel it yields       . Therefore    and     as in Example  where      and        . All the matrices in Youngs class have eigenvalues  that occur in plusminus pairs and the corresponding  are  and   .  So GaussSeidel doubles the Jacobi rate of convergence. The important problem is to choose  so that   will be minimized.  Fortunately Youngs equation  is exactly our  by  example! The best  makes the two roots  both equal to                        or               . F or a large matrix this pattern will be repeated for a number of different pairs    and we can only make a single choice of  .  The largest  gives the largest value of  and  of     .  Since our goal is to make   as small as possible that extremal pair specifies the best choice    Optimal                   and        .  G The splittings of the      matrix of order n yield these eigenvalues of B  Jacobi  S     matrix S   T has      cos  n   GaussSeidel S      matrix S   T has       cos  n     SOR with the best         cos  n         sin  n     . This can only be appreciated by an example. Suppose A is of order  which is very moderate.  Then h      cos  h  .   and the Jacobi method is slow  cos   h  .  means that even GaussSeidel will require a great many iterations.  But since sin  h   .   .  the optimal overrelaxation method will have the convergence factor    .   .   .   with          .  . The error is reduced by  at every step and a single SOR step is the equivalent of  Jacobi steps   .     . . That is a striking result from such a simple idea.  Its real applications are not in one dimensional problems like  u   f . A tridiagonal system Ax  b is already easy. It is for partial differential equations that overrelaxation and other ideas will be important. Changing to  u   u   f leads to the fivepoint scheme.  The entries      in the x direction combine with      in the y direction to give a main diagonal of   and four offdiagonal entries of  . The matrix A does not have a small bandwidth! There  is  no  way  to  number  the N  mesh  points  in  a  square  so  that  each  point  stays close to all four of its neighbors.  That is the true curse of dimensionality  and parallel computers will partly relieve it. If the ordering goes a row at a time every point must wait a whole row for the neigh bor above it to turn up.  The fivepoint matrix has bandwidth N   This matrix has had more attention and been attacked in more different ways than any other linear equa tion Ax  b .  The trend now is back to direct methods based on an idea of Golub and Hockney certain special matrices will fall apart when they are dropped the right way. It is comparable to the Fast Fourier Transform. Before that came the iterative methods of alternating direction  in which the splitting separated the tridiagonal matrix in the x direction from the one in the y direction A recent choice is S  L  U   in which small                         entries of the true L and U are set to zero while factoring A . It is called incomplete LU and it can be terrific. We cannot close without mentioning the conjugate gradient method  which looked dead hut is suddenly very much alive Problem  gives the steps.  It is direct rather than iterative but unlike elimination it can be stopped part way.  And needless to say a completely new idea may still appear and win.  But it seems fair to say that it was the change from .  to .  that revolutionized the solution of Ax  b . Pr oblem Set . . This matrix has eigenvalues      and     A                     . Find the Jacobi matrix D     L  U  and the GaussSeidel matrix  D  L      U  and their eigenvalues and the numbers   and   for SOR. . F or this n by n matrix describe the Jacobi matrix J  D     L  U   A                            . Show that the vector x    sin  h  sin   h ... sin n  h  is an eigenvector of J with eigenvalue    cos  h  cos    n    . . In Problem  show that x    sin k  h  sin  k  h ... sin nk  h  is an eigenvector of A . Multiply x  by A to find the corresponding eigenvalue   .  Verify that in the  by  case these eigenvalues are         . Note . The eigenvalues of the Jacobi matrix J      L  U   I    A are           cos k  h . They occur in plusminus pairs and   is cos  h .  Problems  require Gershgorins circle theorem Every eigenvalue of A lies in at least one of the circles C  ... C   where C  has its center at the diagonal entry a  .  Its radius r         a   is equal to the absolute sum along the rest of the row . Proof. Suppose x  is the largest component of x . Then Ax   x leads to    a   x        a  x   or    a         a    x    x          a     r  . . The matrix A                             is called diagonally dominant because every  a    r  .  Show that zero cannot lie in any of the circles and conclude that A is nonsingular. . Write the Jacobi matrix J for the diagonally dominant A of Problem  and find the three Gershgorin circles for J .  Show that all the radii satisfy r     and that the Jacobi iteration converges. . The  true  solution  to Ax  b is  slightly  different  from  the  elimination  solution  to LU x   b  A  LU misses zero because of roundoff. One strategy is to do everything in double precision but a better and faster way is iterative refinement  Compute only one vector r  b  Ax  in double precision solve LU y  r  and add the correction y to x  . Problem Multiply x   x   y by LU  write the result as a splitting Sx   T x   b  and explain why T is extremely small. This single step brings us almost exactly to x . . For a general  by  matrix A   a   b c   d   find the Jacobi iteration matrix S   T   D    L  U  and its eigenvalues   .  Find also the GaussSeidel matrix   D  L    U and its eigenvalues    and decide whether       . . Change Ax  b to x   I  A  x  b . What are S and T for this splitting? What matrix S   T controls the convergence of x        A  x   b ? . If  is an eigenvalue of A  then is an eigenvalue of B  I  A .  The real eigen values of B have absolute value less than  if the real eigenvalues of A lie between and . . Sho w why the iteration x      I  A  x   b does not converge for A         .      . Why is the norm of B  never larger than  B   ?  Then  B    guarantees that the powers B  approach zero convergence.  This is no surprise since     is below  B  . . If A is singular  then all splittings A  S  T must fail.   From Ax    show that S   T x  x . So this matrix B  S   T has    and fails. . Change the s to s and find the eigenvalues of S   T for both methods  J            x               x   b  GS           x               x   b . Does     for GaussSeidel equal      for Jacobi? . Write a computer code  MATLAB or other for GaussSeidel.  You can define S and T from A  or set up the iteration loop directly from the entries a  . Test it on the      matrices A of order    with b      ...   . . The SOR splitting matrix S is the same as for GaussSeidel except that the diagonal is divided by  . Write a program for SOR on an n by n matrix. Apply it with    . . . when A is the      matrix of order . . When A  A   the ArnoldiLanczos  method finds  orthonormal q s  so  that Aq   b    q     a  q   b  q    with q   . Multiply by q   to find a formula for a  . The equation says that AQ  QT where T is a matrix. . What bound on     does Gershgorin give For these matrices see Problem ? What are the three Gershgorin circles that contain all the eigenvalues? A     .  .  .  .  .  .  .  .  .     A                     . The key point for large matrices is that matrixvector multiplication is much faster than matrixmatrix multiplication . A crucial construction starts with a vec tor b and computes Ab  A  b ... but never A  !.   The first N vectors span the N th Krylov subspace . They are the columns of the Krylov matrix K   K    b   Ab   A  b  A    b  . The ArnoldiLanczos iteration orthogonalizes the columns of K   and the conjugate gradient iteration solves Ax  b when A is symmetric positive definite.  Arnoldi Iteration    Conjugate Gradient Iteration q   b   b  x     r   b  p   r  for n   to N   for n   to N v  Aq      r     r       p     A p     step length x    to x  for j   to n x   x       p    approximate solution h   q   v r   r       A p    new residual b  Ax  v  v  h  q      r   r     r     r     improvement this step h        v  p   r     p    next search direction q     v  h      Note Only  matrix vector multiplication Aq and A p . In Arnoldi show that q  is orthogonal to q  .  The Arnoldi method is GramSchmidt orthogonalization applied to the Krylov matrix K   Q  R  .  The eigenvalues of Q   AQ  are often very close to those of A  even for N  n . The Lanczos iteration is Arnoldi for symmetric matrices all coded in ARPACK. . In conjugate gradients show that r  is orthogonal to r  orthogonal residuals and p  Ap     search  directions  are A orthogonal.   The  iteration  solves Ax  b by minimizing the error e  Ae in the Krylov subspace. It is a fantastic algorithm.    .    Linear Inequalities Algebra is about equations and analysis is often about inequalities.  The line between them has always seemed clear.  But I have realized that this chapter is a counterexam ple linear programming is about inequalities  but it is unquestionably a part of linear algebra. It is also extremely usefulbusiness decisions are more likely to involve linear programming than determinants or eigenvalues. There are three ways to approach the underlying mathematics  intuitively through the  geometry  computationally  through  the  simplex  method  or  algebraically  through duality.  These approaches are developed in Sections . . and ..  Then Section . is about problems like marriage in which the solution is an integer.  Section . discusses poker and other matrix games. The MIT students in Bringing Down the House counted high cards to win at blackjack Las Vegas follows fixed rules and a true matrix game involves random strategies. Section . has something new in this fourth edition.  The simplex method is now in a lively competition with a completely different way to do the computations called an interior  point  method .   The  excitement  began  when  Karmarkar  claimed  that  his version was  times faster than the simplex method.  His algorithm outlined in . was one of the first to be patentedsomething we then believed impossible  and not really desirable.   That claim brought a burst of research into methods that approach the solution from the interior where all inequalities are strict x   becomes x  . The result is now a great way to get help from the dual problem in solving the primal problem. One key to this chapter is to see the geometric meaning of linear inequalities .  An inequality divides n dimensional space into a halfspace in which the inequality is satis fied and a halfspace in which it is not.  A typical example is x   y  .  The boundary between the two halfspaces is the line x   y   where the inequality is tight. Figure . would look almost the same in three dimensions.  The boundary becomes a plane like x   y  z   and above it is the halfspace x   y  z  .  In n dimensions the  plane has dimension n  .                        Another constraint is fundamental to linear programming x and y are required to be nonnegative .  This pair of inequalities x   and y   produces two more halfspaces. Figure . is bounded by the coordinate axes x   admits all points to the right of x   and y   is the halfspace above y  . The Feasible Set and the Cost Function The important step is to impose all three inequalities at once. They combine to give the shaded region in Figure ..  This feasible set is the intersection of the three halfspaces x   y   x   and y  .  A feasible set is composed of the solutions to a family of linear inequalities like Ax  b the intersection of m halfspaces. When we also require that every component of x is nonnegative the vector inequality x   this adds n more halfspaces. The more constraints we impose the smaller the feasible set. It can easily happen that a feasible set is bounded or even empty.  If we switch our example to the halfspace x   y   keeping x   and y   we get the small triangle OAB . By combining both inequalities x   y   and x   y   the set shrinks to a line where x   y  . If we add a contradictory constraint like x   y   the feasible set is empty. The algebra of linear inequalities or feasible sets is one part of our subject.  But linear programming has another essential ingredient It looks for the feasible point that maximizes or minimizes  a certain cost function like  x   y .   The problem  in linear programming is to find the point that lies in the feasible set and minimizes the cost . The problem is illustrated by the geometry of Figure . The family of costs  x   y   gives a family of parallel lines.  The minimum cost comes when the first line intersects the feasible set.  That intersection occurs at B  where x    and y    the minimum cost is  x    y   . The vector      is feasible because it lies in the feasible set it is optimal because it minimizes the cost function and the minimum cost  is the value of the program. We denote optimal vectors by an asterisk.                                       The optimal vector occurs at a corner of the feasible set .  This is guaranteed by the geometry because the lines that give the cost function or the planes when we get to more unknowns move steadily up until they intersect the feasible set. The first contact must occur along its boundary!  The simplex method will go from one corner of the feasible set to the next until it finds the corner with lowest cost.  In contrast interior point methods approach that optimal solution from inside the feasible set. Note . With a different cost function the intersection might not be just a single point. If the cost happened to be x   y  the whole edge between B and A would be optimal. The minimum cost is x    y   which equals  for all these optimal vectors. On our feasible set the maximum problem would have no solution! The cost can go arbitrarily high and the maximum cost is infinite. Every linear programming problem falls into one of three possible categories .  The feasible set is empty . .  The cost function is unbounded on the feasible set. .  The cost reaches its minimum or maximum on the feasible set the good case . The empty and unbounded cases should be very uncommon for a genuine problem in economics or engineering. We expect a solution.  Slack Variables There is a simple way to change the inequality x   y   to an equation. Just introduce the difference as a slack variable w  x   y  .  This is our equation!  The old con straint x   y   is converted into w   which matches perfectly the other inequality constraints x   y  .  Then we have only equations and simple nonnegativity con straints on x  y  w . The variables w that take up the slack are now included in the vector unknown x  Primal problem Minimize cx subject to Ax  b and x   . The row vector c contains the costs in our example c         .  The condition x   puts the problem into the nonnegative part of R  .  Those inequalities cut back on the solutions to Ax  b . Elimination is in danger and a completely new idea is needed. The Diet Problem and Its Dual Our example with cost  x   y can be put into words.  It illustrates the diet problem in linear programming with two sources of proteinsay steak and peanut butter. Each pound of peanut butter gives a unit of protein and each steak gives two units.  At least four units are required in the diet. Therefore a diet containing x pounds of peanut butter and y steaks is constrained by x   y   as well as by x   and y  . We cannot have negative steak or peanut butter. This is the feasible set and me pcm is to minimize the cost. If a pound of peanut butter costs  and a steak is . then the cost of the whole diet is  x   y . Fortunately the optimal diet is two steaks x    and y   . Every linear program including this one has a dual .  If the original prohev a min imization its dual is a maximization. The minimum in the given primal problem equals the maximum in its dual .  This is the key to linear programming and it will be explained in Section .. Here we stay with the diet problem and try to interpret its dual. In place of the shopper who buys enough protein at minimal cost the dual problem is faced by a druggist. Protein pills compete with steak and peanut butter. Immediately we meet the two ingredients of a typical linear program  The druggist maximizes the pill price p  but that price is subject to linear constraints.  Synthetic protein must not cost more than the protein in peanut butter  a unit or the protein in steak  for two units.  The price must be nonnegative or the druggist will not sell.  Since four units of protein are required the income to the druggist will be  p  Dual problem Maximize  p  subject to p     p    and p   . In this example the dual is easier to solve than the primal  it has only one unknown p .  The constraint  p   is the tight one that is really active and the maximum price of synthetic protein is p   . .  The maximum revenue is  p   and the shopper ends up paying the same for natural and synthetic protein.  That is the duality theorem maximum equals minimum .   Typical Applications The next section will concentrate on solving linear programs. This is the time to describe two practical situations in which we minimize or maximize a linear cost function subject to linear constraints . .  Production Planning. Suppose General Motors makes a profit of  on each Chevrolet  on each Buick and  on each Cadillac.  These get   and  miles per gallon respectively and Congress insists that the average car must get . The plant can assemble a Chevrolet in  minute a Buick in  minutes and a Cadillac in  minutes. What is the maximum profit in  hours  minutes? Problem Maximize the profit  x   y   z subject to  x   y   z    x  y  z   x   y   z    x  y  z   . . Portfolio Selection. Federal bonds pay  municipals pay  and junk bonds pay .  We can buy amounts x  y  z not exceeding a total of .  The problem is to maximize the interest with two constraints i  no more than  can be invested in junk bonds and ii  the portfolios average quality must be no lower than municipals so x  z . Problem Maximize  x   y   z subject to x  y  z      z      z  x  x  y  z   . The three inequalities give three slack variables with new equations like w  x  z and inequalities w  . Pr oblem Set . . Sketch the feasible set with constraints x   y    x  y   x   y  .  What points lie at the three corners of this set? . Recommended On the preceding feasible set what is the minimum value of the cost function x  y ? Draw the line x  y  constant that first touches the feasible set. What points minimize the cost functions  x  y and x  y ? . Show that the feasible set constrained by  x   y     x   y   x   y   is empty.  . Show that the following problem is feasible but unbounded  so it has no optimal solution Maximize x  y  subject to x   y     x   y   x  y  . . Add a single inequality constraint to x   y   such that the feasible set contains only one point. . What shape is the feasible set x   y   z   x  y  z   and what is the maximum of x   y   z ? . Solve the portfolio problem at the end of the preceding section. . In the feasible set for the General Motors problem the nonnegativity x  y  z   leaves an eighth of threedimensional space the positive octant. How is this cut by the two planes from the constraints and what shape is the feasible set?  How do its corners show that with only these two constraints there will be only two kinds of cars in the optimal solution? . Transportation problem Suppose Texas California and Alaska each produce a mil lion barrels of oil  barrels are needed in Chicago at a distance of   and  miles from the three producers  respectively  and  barrels are needed in New England   and  miles away.  If shipments cost one unit for each barrelmile what linear program with five equality constraints must be solved to minimize the shipping cost? . The Simplex Method This  section  is  about  linear  programming  with n unknowns x    and m constraints Ax  b . In the previous section we had two variables and one constraint x   y  . The full problem is not hard to explain and not easy to solve. The best approach is to put the problem into matrix form. We are given A  b  and c  .  an m by n matrix A . .  a column vector b with m components and .  a row vector c  cost vector  with n components. To be feasible the vector x must satisfy x   and Ax  b . The optimal vector x  is the feasible vector of least cost and the cost is cx  c  x     c  x  . Minimum problem Minimize the cost cx subject to x   and Ax  b. The condition x   restricts x to the positive quadrant in n dimensional space.  In R  it is a quarter of the plane it is an eighth of R  .  A random vector has one chance in   of being nonnegative. Ax  b produces m additional halfspaces and the feasible   vectors meet all of the m  n conditions.  In other words x lies in the intersection of m  n halfspaces.  This feasible set has flat sides it may be unbounded.  and it may be empty. The cost function cx brings to the problem a family of parallel planes.  One plane cx   goes through the origin.  The planes cx  constant give all possible costs.  As the cost varies these planes sweep out the whole n dimensional space. The optimal x  lowest cost occurs at the point where the planes first touch the feasible set . Our aim is to compute x  .  We could do it in principle by finding all the corners of  the  feasible  set  and  computing  their  costs.   In  practice  this  is  impossible.   There could be billions of corners and we cannot compute them all.  Instead we turn to the simplex method  one of the most celebrated ideas in computational mathematics. It was developed by Dantzig as a systematic way to solve linear programs and either by luck or genius it is an astonishing success. The steps of the simplex method are summarized later and first we try to explain them. The Geometry Movement Along Edges I think it is the geometric explanation that gives the method away. Phase I simply locates one corner of the feasible set. The heart of the method goes from corner to corner along the edges of the feasible set . At a typical corner there are n edges to choose from. Some  edges  lead  away  from  the  optimal  but  unknown x    and  others  lead  gradually toward it.  Dantzig chose an edge that leads to a new corner with a lower cost .  There is no possibility of returning to anything more expensive. Eventually a special corner is reached from which all edges go the wrong way  The cost has been minimized.  That corner is the optimal vector x   and the method stops. The next problem is to turn the ideas of corner and edge into linear algebra. A corner is the meeting point of n different planes .  Each plane is given by one equationjust as three planes front wall side wall and floor produce a corner in three dimensions. Each corner of the feasible set comes from turning n of the n  m inequalities Ax  b and x   into equations and finding the intersection of these n planes. One possibility is to choose the n equations x    ... x     and end up at the origin. Like all the other possible choices this intersection point will only be a genuine corner if it also satisfies the m remaining inequality constraints . Otherwise it is not even in the feasible set and is a complete fake. Our example with n   variables and m   constraints has six intersections illustrated in Figure ..  Three of them are actually corners P  Q  R of the feasible set.  They are the vectors             and       One of them must be the optimal vector unless the minimum cost is   .  The other three including the origin are fakes. In general there are  n  m  !  n ! m ! possible intersections. That counts the number of ways to choose n plane equations out of n  m .  The size of that binomial coefficient makes computing all corners totally impractical for large m and n . It is the task of Phase                                  I either to find one genuine corner or to establish that the feasible set is empty.   We continue on the assumption that a corner has been found. Suppose one of the n intersecting planes is removed. The points that satisfy the remaining n   equations form an edge that comes out of the corner .  This edge is the intersection of the n   planes.  To stay in the feasible set only one direction is allowed along each edge.  But we do have a choice of n different edges and Phase II must make that choice. To describe this phase rewrite Ax  b in a form completely parallel to the n simple constraints x   .  This is the role of the slack variables w  Ax  b .  The constraints Ax  b are translated into w    ... w    with one slack variable for every row of A . The equation w  Ax  b  or Ax  w  b  goes into matrix form Slack variables give m equations  A  I   x w   b . The feasible set is governed by these m equations and the n  m simple inequalities x   w  . We now have equality constraints and nonnegativity . The simplex method notices no difference between x and w  so we simplify  A  I  is renamed A  x w  is renamed x  c   is renamed c . The equality constraints are now Ax  b .   The n  m inequalities become just x  . The only trace left of the slack variable w is in the fact that the new matrix A is m by n  m  and the new x has n  m components. We keep this much of the original notation leaving m and n unchanged as a reminder of what happened. The problem has become Minimize cx  subject to x   and Ax  b .   Example . The problem in Figure . has constraints x   y    x  y   and cost x  y . The new system has four unknowns  x  y  and two slack variables A                  b      c              . The Simplex Algorithm With equality constraints the simplex method can begin. A corner is now a point where n components of the new vector x the old x and w  are zero . These n components of x are the free variables in Ax  b . The remaining m components are the basic variables or pivot variables .  Setting the n free variables to zero the m equations Ax  b determine the m basic variables.  This basic solution x will be a genuine corner if its m nonzero components are positive . Then x belongs to the feasible set. A The corners of the feasible set are the basic feasible solutions of Ax  b . A solution is basic when n of its m  n components are zero and it is feasible when it satisfies x  . Phase I of the simplex method finds one basic feasible solution. Phase II moves step by step to the optimal x  . The corner point P in Figure . is the intersection of x   with  x  y    . Corner Basic Feasible  two zeros positive nonzeros Ax                                      b . Which corner  do  we  go  to  next? We want to  move along  an  edge  to  an  adjacent corner.  Since the two corners are neighbors m   basic variables will remain basic. Only one of the  s will become free zero. At the same time one variable will move up from zero to become basic . The other m   basic components in this case the other  will change but stay positive. The choice of edge see Example  below decides which variable leaves the basis and which one enters.  The basic variables are computed by solving Ax  b . The free components of x are set to zero. Example . An entering variable and a leaving variable move us to a new corner. Minimize     x   x    x  subject to x   x    x    x    x   x    x    . Start from the corner at which x    and x    are the basic variables.  At that corner x   x   x   .   This is feasible  but the zero cost may not be minimal.   It would be foolish to make x  positive because its cost coefficient is   and we are trying to lower the cost. We choose x  because it has the most negative cost coefficient  . The entering variable will be x  .  With x  entering the basis x  or x  must leave.  In the first equation increase x  and decrease x  while keeping x    x   .  Then x  will be down to zero when x  reaches .  The second equation keeps x    x   .  Here x  can only increase as far as .  To go further would make x  negative so the leaving variable is x  .  The new corner has x             . The cost is down to  . Quick  Way In Ax  b   the  right  sides  divided  by  the  coefficients  of  the  entering variable are   and   . The smallest ratio   tells which variable hits zero first and must leave.  We consider only positive ratios because if the coefficient of x  were   then increasing x  would actually increase x  .  At x    the second equation would give x   . The ratio   says that the second variable leaves . It also gives x   . If all coefficients of x  had been negative this would be an unbounded case we can make x  arbitrarily large and bring the cost down toward   . The current step ends at the new corner x             .  The next step will only be easy if the basic variables x  and x  stand by themselves as x  and x  originally did. Therefore we pivot by substituting x        x   x   into the cost function and the first equation. The new problem starting from the new corner is Minimize the cost  x   x      x   x    x    x   x    with constraints x     x     x    x      x     x   x    . The next step is now easy.  The only negative coefficient   in the cost makes x  the entering variable. The ratios of   and    the right sides divided by the x  column make x  the leaving variable.  The new corner is x               . The new cost     is the minimum. In a large problem a departing variable might reenter the basis later on. But the cost keeps going downexcept in a degenerate caseso the m basic variables cant be the same as before. No corner is ever revisited! The simplex method must end at the optimal corner or at   if the cost turns out to be unbounded. What is remarkable is the speed at which x  is found. Summary The cost coefficients      at the first corner and      at the second corner decided the entering variables. These numbers go into r  the crucial vec tor defined below.  When they are all positive we stop.  The ratios decided the leaving variables. Remark  on  Degeneracy A  corner  is degenerate if  more  than  the  usual n com ponents of x are zero.  More than n planes pass through the corner so a basic variable happens to vanish. The ratios that determine the leaving variable will include zeros and the basis might change without actually moving from the corner.  In theory we could stay at a corner and cycle forever in the choice of basis.   Fortunately  cycling does not occur.   It is so rare that commercial codes ignore it. Unfortunately degeneracy is extremely common in applicationsif you print the cost after each simplex step you see it repeat several times before the simplex method finds a good edge. Then the cost decreases again. The Tableau Each  simplex  step  involves  decisions  followed  by  row  operationsthe  entering  and leaving variables have to be chosen and they have to be made to come and go.  One way to organize the step is to fit A  b  c into a large matrix or tableau  Tableau is m   by m  n   T   A   b c   . At the start the basic variables may be mixed with the free variables.  Renumbering if necessary suppose that x  ... x  are the basic nonzero variables at the current corner . The first m columns of A form a square matrix B the basis matrix for that corner. The last n columns give an m by n matrix N .  The cost vector c splits into  c  c    and the unknown x into  x   x   . At the corner the free variables are x   . There Ax  b turns into Bx   b  Tableau at corner T   B N    b c  c    x    x   B   b cost  c  B   b . The basic variables will stand alone when elimination multiplies by B    Reduced tableau T    I B   N  B   b c  c    . T o reach the fully reduced row echelon form R  rref  T    subtract c  times the top block row from the bottom row Fully reduced R   I B   N B   b            c  B   b  . Let me review the meaning of each entry in this tableau and also call attention to Ex ample  following with numbers. Here is the algebra Constraints x   B   Nx   B   b Corner x   B   b  x    .  The cost c  x   c  x  has been turned into Cost cx   c   c  B   N  x   c  B   b Cost at this corner  c  B   b .  Every important quantity appears in the fully reduced tableau R . We can decide whether the corner is optimal by looking at r  c   c  B   N in the middle of the bottom row. If  any entry in r is negative the cost can still be reduced .  We can make rx  negative at the start of equation  by increasing a component of x  . That will be our next step. But if r   the best corner has been found.  This is the stopping test  or optimality condition  B The corner is optimal when r  c   c  B   N  .  Its cost is c  B   b . Negative components of r correspond to edges on which the cost goes down. The entering variable x  corresponds to the most negative component of r . The components of r are the reduced costs the cost in c  to use a free variable minus what it saves .  Computing r is called pricing out the variables.  If the direct cost in c   is less than the saving from reducing basic variables then r    and it will pay to increase that free variable. Suppose the most negative reduced cost is r  .  Then the i th component of x  is the entering variable  which increases from zero to a positive value  at the next corner the end of the edge. As x  is increased other components of x may decrease to maintain Ax  b . The x  that reaches zero first becomes the leaving variable it changes from basic to free. We reach the next corner when a component of x  drops to zero . That new corner is feasible because we still have x  . It is basic because we again have n zero components.  The i th component of x  went from zero to  .  The k th com ponent of x  dropped to zero the other components of x  remain positive. The leaving x  that drops to zero is the one that gives the minimum ratio in equation  C Suppose x  is the entering variable and u is column i of N  At new corner x     smallest ratio  B   b    B   u     B   b    B   u   .  This minimum is taken only over positive components of B   u .  The k th col umn of the old B leaves the basis  x  becomes  and the new column u enters. B   u is the column of B   N in the reduced tableau R  above the most negative entry in the bottom row r  If B   u   the next corner is infinitely far away and the minimal cost is   this doesnt happen here. Our example will go from the corner P to Q  and begin again at Q . Example . The original cost function x  y and constraints Ax  b       give  A   b c                                . At the corner P in Figure . x   intersects  x  y  . To be organized we exchange   columns  and  to put basic variables before free variables Tableau at P T                           . Then elimination multiplies the first row by   to give a unit pivot and uses the second row to produce zeros in the second column Fully reduced at P R                                    . Look first at r        in the bottom row.  It has a negative entry in column  so the third variable will enter the basis. The current corner P and its cost   are not optimal. The column above that negative entry is B   u        its ratios with the last column are   and   . Since the first ratio is smaller the first unknown w and the first column of the tableau is pushed out of the basis. We move along the feasible set from corner P to corner Q in Figure .. The new tableau exchanges columns  and  and pivoting by elimination gives                                                                   . In that new tableau at Q  r        is positive. The stopping test is passed . The corner x  y   and its cost   are optimal. The Organization of a Simplex Step The geometry of the simplex method is now expressed in algebracorners are basic feasible solutions.  The vector r and the ratio  are decisive.  Their calculation is the heart of the simplex method and it can be organized in three different ways .  In a tableau as above. .  By updating B   when column u taken from N replaces column k of B . .  By computing B  LU  and updating these LU factors instead of B   . This  list  is  really  a  brief  history  of  the  simplex  method  In  some  ways  the  most fascinating stage was the firstthe tableau which dominated the subject for so many years.   For  most  of  us  it  brought  an  aura  of  mystery  to  linear  programming  chiefly because it managed to avoid matrix notation almost completely by the skillful device of  writing out all matrices in full !. For computational purposes except for small problems in textbooks the day of the tableau is over. To see why remember that after the most negative coefficient in r indicates which column u will enter the basis none of the other columns above r will be used. It was a waste of time to compute them .  In a larger problem hundreds of columns would be computed time and time again just waiting for their turn to enter the basis.  It makes the theory clear to do the eliminations so completely and reach R .  But in practice this cannot be justified. It is quicker and in the end simpler to see what calculations are really necessary. Each simplex step exchanges a column of N for a column of B .   Those columns are decided by r and  .  This step begins with the current basis matrix B and the current solution x   B   b . A Step of the Simplex Method .  Compute the row vector   c  B   and the reduced costs r  c    N . .  If r   stop the current solution is optimal.  Otherwise if r  is the most negative component choose u  column i of N to enter the basis. .  Compute the ratios of B   b to B   u  admitting only positive components of B   u .  If B   u   the minimal cost is   .  When the smallest ratio occurs at component k  the k th column of the current B will leave. .  Update B  B    or LU  and the solution x   B   b . Return to step . This is sometimes called the revised simplex method to distinguish it from the oper ations on a tableau. It is really the simplex method itself boiled down. This discussion is finished once we decide how to compute steps   and    c  B    v  B   u  and x   B   b .  The most popular way is to work directly with B    calculating it explicitly at the first corner. At succeeding corners the pivoting step is simple. When column k of the identity matrix is replaced by u  column k of B   is replaced by v  B   u . To recover the identity matrix elimination will multiply the old B   by E            v    v    v                      v   v       v      v   v           Many simplex codes use the product form of the inverse  which saves these simple matrices E   instead of directly updating B   . When needed they are applied to b and c  . At regular intervals maybe every  simplex steps B   is recomputed and the E   are erased. Equation  is checked in Problem  at the end of this section.   A newer approach uses the ordinary methods of numerical linear algebra regarding equation  as three equations sharing the same matrix B   B  c   Bv  u  Bx   b .  The usual factorization B  LU or PB  LU  with row exchanges for stability leads to the three solutions. L and U can be updated instead of recomputed. One question remains How many simplex steps do we have to take ?  This is impos sible to answer in advance. Experience shows that the method touches only about  m   different corners which means an operation count of about m  n . That is comparable to ordinary elimination for Ax  b  and is the reason for the simplex methods success. But mathematics shows that the path length cannot always be bounded by any fixed multiple or power of m .  The worst feasible sets Klee and Minty invented a lopsided cube can force the simplex method to try every cornerat exponential cost. It was Khachians method that showed that linear programming could be solved in polynomial time.  His algorithm stayed inside the feasible set and captured x  in a series of shrinking ellipsoids.  Linear programming is in the nice class P  not in the dreaded class NP like the traveling salesman problem. For NP problems it is believed but not proved that all deterministic algorithms must take exponentially long to finish in the worst case. All this time the simplex method was doing the jobin an average time that is now proved for variants of the usual method to be polynomial. For some reason hidden in the geometry of manydimensional polyhedra bad feasible sets are rare and the simplex method is lucky. Karmarkars Method We come now to the most sensational event in the recent history of linear programming. Karmarkar  proposed  a  method  based  on  two  simple  ideas  and  in  his  experiments  it defeated the simplex method. The choice of problem and the details of the code are both crucial and the debate is still going on.  But Karmarkars ideas were so natural and fit so perfectly into the framework of applied linear algebra that they can be explained in a few paragraphs. The first idea is to start from a point inside the feasible set we will suppose it is x       ...   .  Since the cost is cx  the best costreducing direction is toward  c . Normally that takes us off the feasible set moving in that direction does not maintain Ax  b . If Ax   b and Ax   b  then  x  x   x  has to satisfy A  x  . The step  x must lie in the nullspace of A .  Therefore we project  c onto the nullspace to find the feasible direction closest to the best direction.  This is the natural but expensive step in Karmarkars method.                The step  x is a multiple of the projection  Pc .  The longer the step the more the cost is reducedbut we cannot go out of the feasible set. The multiple of  Pc is chosen so that x  is close to but a little inside  the boundary at which a component of x reaches zero. That completes the first ideathe projection that gives the steepest feasible descent . The second step needs a new idea. since to continue in the same direction is useless. Karmarkars suggestion is to transform x  back to     ...   at the center .   His change of variables was nonlinear but the simplest transformation is just a rescaling by a diagonal matrix D .  Then we have room to move.  The rescaling from x to X  D   x changes the constraint and the cost Ax  b becomes ADX  b c  x becomes c  DX . Therefore the matrix AD takes the place of A and the vector c  D takes the place of c  . The second step projects the new c onto the nullspace of the new A .  All the work is in this projection to solve the weighted normal equations  AD  A   y  AD  c .  The normal way to compute y is by elimination.  GramSchmidt will orthogonalize the columns of DA   which can be expensive although it makes the rest of the calculation easy.  The favorite for large sparse problems is the conjugate gradient method  which gives the exact answer y more slowly than elimination but you can go part way and then stop. In the middle of elimination you cannot stop. Like other new ideas in scientific computing Karmarkars method succeeded on some problems and not on others.  The underlying idea was analyzed and improved.  Newer interior point methods staying inside the feasible set are a major successmentioned in the next section.  And the simplex method remains tremendously valuable.  like the whole subject of linear programmingwhich was discovered centuries after Ax  b  but shares the fundamental ideas of linear algebra.  The most farreaching of those ideas is duality which comes next. Pr oblem Set . . Minimize x   x   x   subject to  x    x   x   x     x    x   x   x    . Which of x   x   x  should enter the basis and which of x   x  should leave? Compute the new pair of basic variables and find the cost at the new corner. . After the preceding simplex step prepare for and decide on the next step.   . In Example   suppose the cost is  x  y .   With rearrangement  the cost vector is c           . Show that r   and therefore that corner P is optimal. . Suppose the cost function in Example  is x  y   so that after rearrangement c            at the corner P . Compute r and decide which column u should enter the basis.  Then compute B   u and show from its sign that you will never meet another corner. We are climbing the y axis in Figure . and x  y goes to   . . Again in Example  change the cost to x   y . Verify that the simplex method takes you from P to Q to R  and that the corner R is optimal. . Phase I finds a basic feasible solution to Ax  b a corner.  After changing signs to make b   consider the auxiliary problem of minimizing w   w     w   subject to x   w   Ax  w  b .  Whenever Ax  b has a nonnegative solution the minimum cost in this problem will be zerowith w   . a  Show that for this new problem the corner x   w  b is both basic and fea sible.  Therefore its Phase I is already set and the simplex method can proceed to find the optimal pair x   w  .  If w    then x  is the required corner in the original problem. b  With A       and b      write out the auxiliary problem its Phase I vector x   w  b  and its optimal vector. Find the corner of the feasible set x   x    x   x    and draw a picture of this set. . If we wanted to maximize instead of minimize the cost with Ax  b and x   what would be the stopping test on r  and what rules would choose the column of N to make basic and the column of B to make free? . Minimize  x   x   subject to x   x    x    x    x   x    x  . . Verify the inverse in equation  and show that BE has Bv  u in its k th column. Then BE is the correct basis matrix for the next stop E   B   is its inverse and E   updates the basis matrix correctly. . Suppose we want to minimize cx  x   x   subject to  x    x   x     x    x   x     all x   x   x   x     . Starting from x            should x  or x  be increased from its current value of zero? How far can it be increased until the equations force x  or x  down to zero? At that point what is the new x ? . For the matrix P  I  A   AA     A   show that if x is in the nullspace of A   then Px  x . The nullspace stays unchanged under this projection. . a  Minimize the cost c  x   x    x    x  on the plane x   x   x    by testing the vertices P  Q  R  where the triangle is cut off by the requirement x  .  b  Project c         onto the nullspace of A          and find the maximum step s that keeps e  sPc nonnegative. . The Dual Problem Elimination can solve Ax  b  but the four fundamental subspaces showed that a different and deeper understanding is possible. It is exactly the same for linear programming. The mechanics of the simplex method will solve a linear program but duality is really at the center of the underlying theory.  Introducing the dual problem is an elegant idea and at  the  same  time  fundamental  for  the  applications.   We  shall  explain  as  much  as  we understand. The theory begins with the given primal problem  Primal P Minimize cx subject to x   and Ax  b. The dual  problem starts  from the  same  A  b  and  c  and  reverses everything .   In  the primal c is  in  the  cost  function  and b is  in  the  constraint  In  the  dual b and c are switched The dual unknown y is a row vector with m components and the feasible set has yA  c instead of Ax  b . In short the dual of a minimum problem is a maximum problem. Now y   Dual D Maximize yb subject to y   and yA  c. The dual of this problem is the original minimum problem. There is complete symmetry between the primal and dual problems.  The simplex method applies equally well to a maximizationanyway both problems get solved at once. I have to give you some interpretation of all these reversals. They conceal a competi tion between the minimizer and the maximizer. In the diet problem the minimizer has n foods peanut butter and steak in Section .. They enter the diet in the nonnegative amounts x  ... x  .  The constraints represent m required vitamins  in place of the one earlier constraint of sufficient protein.  The entry a  measures the i th vitamin in the j th food and the i th row of Ax  b forces the diet to include at least b  of that vitamin. If c  is the cost of the j th food then c  x     c  x   cx is the cost of the diet. That cost is to be minimized. In the dual  the druggist is selling vitamin pills at prices y   .   Since food j contains  vitamins  in  the  amounts a    the  druggists  price  for  the  vitamin  equivalent cannot exceed the grocers price c  . That is the j th constraint in yA  c . Working within this constraint on vitamin prices the druggist can sell the required amount b  of each vitamin for a total income of y  b     y  b   yb to be maximized. The feasible sets for the primal and dual problems look completely different.  The first is a subset of R   marked out by x   and Ax  b .  The second is a subset of R     determined by y   and A  and c .  The whole theory of linear programming hinges on the relation between primal and dual. Here is the fundamental result D Duality Theorem When both problems have feasible vectors they have optimal x  and y  . The minimum cost cx  equals the maximum income y  b . If optimal vectors do not exist there are two possibilities  Either both feasible sets are empty or one is empty and the other problem is unbounded the maximum is   or the minimum is   . The duality theorem settles the competition between the grocer and the druggist. The result is always a tie .  We will find a similar minimax theorem in game theory.  The customer has no economic reason to prefer vitamins over food even though the druggist guarantees to match the grocer on every foodand even undercuts on expensive foods like peanut butter. We will show that expensive foods are kept out of the optimal diet so the outcome can be and is a tie. This may seem like a total stalemate but I hope you will not be fooled. The optimal vectors contain the crucial information.  In the primal problem x  tells the purchaser what to buy. In the dual y  fixes the natural prices  shadow prices  at which the economy should run.  Insofar as our linear model reflects the true economy. x  and y  represent the essential decisions to be made. We want to prove that c  x  y  b .  It may seem obvious that the druggist can raise the vitamin prices y  to meet the grocer hut only one thing is truly clear  Since each food can be replaced by its vitamin equivalent with no increase in cost all adequate food diets must cost at least as much as vitamins.  This is only a onesided inequality druggists price  grocers price .  It is called weak duality  and it is easy to prove for any linear program and its dual E If x and y are feasible in the primal and dual problems then yb  cx . Proof. Since the vectors are feasible they satisfy Ax  b and yA  c .  Because feasi bility also includes x   and y   we can take inner products without spoiling those inequalities multiplying by negative numbers would reverse them yAx  yb and yAx  cx .  Since the lefthand sides are identical we have weak duality yb  cx . This onesided inequality prohibits the possibility that both problems are unbounded. If yb is arbitrarily large a feasible x would contradict yb  cx .  Similarly if cx can go down to    the dual cannot admit a feasible y . Equally important any vectors that achieve yb  cx must be optimal.  At that point the grocers price equals the druggists price.  We recognize an optimal food diet and optimal vitamin prices by the fact that the consumer has nothing to choose F If the vectors x and y are feasible and cx  yb  then x and y are optimal.  Since no feasible y can make yb larger than cx  our y that achieves this value is opti mal. Similarly any x that achieves the cost cx  yb must be an optimal x  . We give an example with two foods and two vitamins.  Note how A  appears when we write out the dual since yA  c for row vectors means A  y   c  for columns. Primal Minimize x    x  Dual Maximize     y    y  subject to x     x    subject to y     y     x   x     y    y     x    x   . y    x   . Solution x    and x    are feasible with cost x    x   . In the dual y     and y    gi ve the same value  y    y   . These vectors must be optimal. Please look closely to see what actually happens at the moment when yb  cx . Some of the inequality constraints are tight  meaning that equality holds. Other constraints are loose and the key rule makes economic sense i  The diet has x     when food j is priced above its vitamin equivalent. ii  The price is y     when vitamin i is oversupplied in the diet x  . In the example x    because the second food is too expensive.  Its price exceeds the druggists price since y    y    is a strict inequality      . Similarly the diet required seven units of the second vitamin but actually supplied  x    x   . So we found y    and that vitamin is a free good .  You can see how the duality has become complete. These optimality conditions are easy to understand in matrix terms.  From equation  we want y  Ax   y  b at the optimum. Feasibility requires Ax   b  and we look for any components in which equality fails .  This corresponds to a vitamin that is oversup plied so its price is y    . At the same time we have y  A  c .  All strict inequalities expensive foods corre spond to x     omission from the diet.  That is the key to y  Ax   cx    which we need.  These are the complementary slackness conditions of linear programming and the KuhnTucker conditions of nonlinear programming G The optimal vectors x  and y  satisfy complementary slackness  If  Ax     b  then y     If  y  A    c  then x     .  Let me repeat the proof. Any feasible vectors x and y satisfy weak duality yb  y  Ax    yA  x  cx .  We need equality and there is only one way in which y  b can equal y   Ax   . Any time b    Ax     the factor y   that multiplies these components must be zero .   Similarly feasibility gives yAx  cx . We get equality only when the second slackness condition is fulfilled. If there is an overpricing  y  A    c   it must be canceled through multiplication by x    .  This leaves us with y  b  cx  in equation .  This equality guarantees the optimality of x  and y  . The Proof of Duality The onesided inequality yb  cx was easy to prove  it gave a quick test for optimal vectors they turn it into an equality and now it has given the slackness conditions in equation . The only thing it has not done is to show that y  b  cx  is really possible. Until those optimal vectors are actually produced the duality theorem is not complete. To produce y  we return to the simplex methodwhich has already computed x  . Our problem is to show that the method stopped in the right place for the dual problem even though it was constructed to solve the primal.  Recall that the m inequalities Ax  b were changed to equations by introducing the slack variables w  Ax  b  Primal feasibility  A  I   x w   b and  x w    .  Every simplex step picked m columns of the long matrix  A  I  to be basic and shifted them theoretically to the front.  This produced  B  N  .  The same shift reordered the long cost vector  c   into  c  c   .  The stopping condition which brought the simplex method to an end was r  c   c  B   N  . This condition r   was finally met  since the number of corners is finite.  At that moment the cost was as low as possible Minimum cost cx    c  c    B   b    c  B   b .  If we can choose y   c  B   in the dual we certainly have y  b  cx  .  The minimum and maximum will be equal.  We have to show that this y  satisfies the dual constraints yA  c and y   Dual feasibility y  A  I    c   .  When the simplex method reshuffles the long matrix and vector to put the basic variables first this rearranges the constraints in equation  into y  B   N    c  c   .  For y   c  B    the first half is an equality and the second half is c  B   N  c  . This is the stopping condition r   that we know to be satisfied!  Therefore our y  is feasible and the duality theorem is proved .  By locating the critical m by m matrix B  which is nonsingular as long as degeneracy is forbidden the simplex method has produced the optimal y  as well as x  .  Shadow Prices In calculus  everybody knows the condition for a maximum or a minimum The first derivatives are zero . But this is completely changed by constraints. The simplest exam ple is the line y  x .  Its derivative is never zero calculus looks useless and the largest y is certain to occur at the end of the interval. That is exactly the situation in linear pro gramming!  There are more variables and an interval is replaced by a feasible set but still the maximum is always found at a corner of the feasible set with only m nonzero components. The problem in linear programming is to locate that cornet For this calculus is not completely helpless.  Far from it because Lagrange multipliers will bring back zero derivatives at the maximum and minimum. The dual variables y are exactly the La grange multipliers .  And they answer the key question How does the minimum cost cx   y  b change if we change b or c ? This is a question in sensitivity analysis . It allows us to squeeze extra information out of the dual problem. For an economist or an executive these questions about marginal cost are the most important. If we allow large changes in b or c  the solution behaves in a very jumpy way.  As the price of eggs increases there will be a point at which they disappear from the diet. The variable x  will jump from basic to free.  To follow it properly we would have to introduce parametric programming.  But if the changes are small the corner that was optimal remains optimal .  The choice of basic variables does not change B and N stay the same. Geometrically we shifted the feasible set a little by changing b  and we tilted the planes that come up to meet it by changing c . When these changes are small contact occurs at the same slightly moved corner. At  the  end  of  the  simplex  method  when  the  right  basic  variables  are  known  the corresponding m columns of A make up the basis matrix B . At that corner a shift of size  b changes the minimum cost by y   b . The dual solution y  gives the rate of change of minimum cost its derivative with respect to changes in b .  The components of y  are the shadow prices . If the requirement for a vitamin goes up by   and the druggists price is y    then the diet cost from druggist or grocer will go up by y    .  In the case that y   is zero that vitamin is a free good and the small change has no effect.  The diet already contained more than b  . We now ask a different question.  Suppose we insist that the diet contain some small edible amount of egg.  The condition x    is changed to x    .  How does this change the cost? If eggs were in the diet x   there is no change. But if x     it will cost extra to add in the amount  . The increase will not be the full price c    since we can cut down on other foods. The reduced cost of eggs is their own price minus the price we are paying for the equivalent in cheaper foods.  To compute it we return to equation  of Section   . cost   c   c  B   N  x   c  B   b  rx   c  B   b . If egg is the first free variable then increasing the first component of x  to  will increase the cost by r   . The real cost of egg is r  . This is the change in diet cost as the zero lower bound nonnegativity constraint moves upwards.  We know that r   and economics tells us the same thing The reduced cost of eggs cannot be negative or they would have entered the diet. Interior Point Methods The simplex method moves along edges of the feasible set eventually reaching the opti mal corner x  . Interior point methods start inside the feasible set where the constraints are all in equalities. These methods hope to move more directly to x  and also find y  . When they are very close to the answer they stop. One way to stay inside is to put a barrier at the boundary.  Add an extra cost in the form of a logarithm that blows up when any variable x or any slack variable w  Ax  b touches zero. The number  is a small parameter to be chosen Barrier problem P    Minimize cx       ln x      ln w   .  This cost is nonlinear but linear programming is already nonlinear from inequalities. The notation is simpler if the long vector  x  w  is renamed x and  A  I  is renamed A . The primal constraints are now x   and Ax  b .  The sum of ln x  in the barrier now goes to m  n . The dual constraints are yA  c .   We dont need y   when we have Ax  b in the primal.  The slack variable is s  c  yA  with s  .  What are the KuhnTucker conditions for x and y to be the optimal x  and y  ? Along with the constraints we require duality cx   y  b . Including the barrier gives an approximate problem P    .  For its KuhnTucker op timality conditions the derivative of ln x  gives   x  .  If we create a diagonal matrix X from those positive numbers x   and use e       for the row vector of n  m ones then optimality in P    is as follows Primal column vectors Ax  b with x   a Dual row vectors yA   eX    c b As    we expect those optimal x and y to approach x  and y  for the original no barrier problem and  eX   will stay nonnegative.  The plan is to solve equations a b with smaller and smaller barriers given by the size of  . In reality those nonlinear equations are approximately solved by Newtons method which means they are linearized.  The nonlinear term is s   eX   .  To avoid   x    rewrite that as sX   e . Creating the diagonal matrix S from s  this is eSX   e . If we change e  y  c  and s to column vectors and transpose optimality now has three parts Primal Ax  b  x   . a Dual A  y  s  c . b Nonlinear X Se   e   . c Ne wtons method takes a step  x   y   s from the current x  y  s .  Those solve equa tions a and b but not c.  By ignoring the secondorder term  X  Se  the corrections come from linear equations! A  x   . a Newton step A   y   s   . b S  x  X  s   e  X Se . c Robert Freunds notes for his MIT class pin down the quadratic convergence rate and the computational complexity of this algorithm. Regardless of the dimensions m and n  the duality gap sx is generally below    after  Newton steps.  This algorithm is used almost as is in commercial interiorpoint software  and for a large class of nonlinear optimization problems as well. The Theory of Inequalities There is more than one way to study duality.   We quickly proved yb  cx   and then used the simplex method to get equality. This was a constructive proof  x  and y  were actually computed. Now we look briefly at a different approach which omits the simplex algorithm and looks more directly at the geometry.  I think the key ideas will be just as clear in fact probably clearer if we omit some of the details. The best illustration of this approach came in the Fundamental Theorem of Linear Algebra. The problem in Chapter  was to find b in the column space of A . After elim ination and the four subspaces this solvability question was answered in a completely different way by Problem  in Section . H Ax  b has a solution or there is a y such that yA   and yb   . This is the theorem of the alternative  because to find both x and y is impossible If Ax  b then yAx  yb    and this contradicts yAx   x  .  In the language of subspaces either b is in the column space or it has a component sticking into the left nullspace. That component is the required y . For inequalities we want to find a theorem of exactly the same kind.  Start with the same system Ax  b  but add the constraint x  . When does there exist a nonnegative solution to Ax  b ? In Chapter  b was anywhere in the column space. Now we allow only nonnegative combinations and the b s no longer fill out a subspace. Instead they fill a coneshaped                                   region. For n columns in R   the cone becomes an openended pyramid. Figure . has four vectors in R   and A is  by . If b lies in this cone there is a nonnegative solution to Ax  b  otherwise not. What is the alternative if b lies outside the cone ? Figure . also shows a separating hyperplane which has the vector b on one side and the whole cone on the other side. The plane consists of all vectors perpendicular to a fixed vector y .  The angle between y and b is greater than    so yb  .  The angle between y and every column of A is less than    so yA  .  This is the alternative we are looking for.  This theorem of the separating hyperplane is fundamental to mathematical economics. I Ax  b has a nonnegative solution or there is a y with yA   and yb  . Example . The nonnegative combinations of the columns of A  I fill the positive quadrant b  . For every other b  the alternative must hold for some y  Not in cone If b        then y        gives yI      but yb    . The x axis perpendicular to y        separates b from the cone  quadrant. Here is a curious pair of alternatives.  It is impossible for a subspace S and its or thogonal complement S  both to contain positive vectors.  Their inner product would be positive not zero.  But S might be the x axis and S  the y axis in which case they contain the semipositive vectors      and      . This slightly weaker alternative does work Either S contains a positive vector x    or S  contains a nonzero y  . When S and S  are perpendicular lines in the plane one or the other must enter the first quadrant. I cant see this clearly in three or four dimensions.  For linear programming  the important alternatives come when the constraints are inequalities. When is the feasible set empty no x ? J Ax  b has a solution x   or there is a y   with yA   and yb  . Proof. The slack variables w  Ax  b change Ax  b into an equation. Use I First alternative  A  I   x w   b for some  x w    . Second alternative y  A  I         for some y with yb   . It is this result that leads to a nonconstructive proof of the duality theorem. Pr oblem Set . . What is the  dual of the  following problem  Minimize x   x    subject to x    x     x    x    x   ?  Find the solution to both this problem and its dual and verify that minimum equals maximum. . What is the dual of the following problem  Maximize y  subject to y    y    y   y   ? Solve both this problem and its dual. . Suppose A is the identity matrix so that m  n  and the vectors b and c are nonnega tive. Explain why x   b is optimal in the minimum problem find y  in the maximum problem and verify that the two values are the same.  If the first component of b is negative what are x  and y  ? . Construct a  by  example in which Ax  b  x   is unfeasible and the dual problem is unbounded. . Starting with the  by  matrix A         choose b and c so that both of the feasible sets Ax  b  x   and yA  c  y   are empty. . If all entries of A  b  and c are positive show that both the primal and the dual are feasible. . Show that x           and y           are feasible in the primal and dual with A                                                     b                 c                . Then after computing cx and yb  explain how you know they are optimal.   . Verify that the vectors in the previous exercise satisfy the complementary slackness conditions in equation  and find the one slack inequality in both the primal and the dual. . Suppose that A       b        and c      . Find the optimal x and y  and verify the complementary slackness conditions as well as yb  cx . . If the primal problem is constrained by equations instead of inequalities Minimize cx subject to Ax  b and x  then the requirement y   is left out of the dual Maximize  yb  subject  to  yA  c .   Show  that  the  onesided  inequality yb  cx still holds. Why was y   needed in equation  but not here? This weak duality can be completed to full duality. . a  Without the simplex method minimize the cost  x    x    x   subject to x   x   x    x    x    x   . b  What is the shape of the feasible set? c  What is the dual problem and what is its solution y ? . If the primal has a unique optimal solution x   and then c is changed a little explain why x  still remains the optimal solution. . Write the dual of the following problem Maximize x   x   x  subject to  x   x    x   . What are the optimal x  and y  if they exist!? . If A       describe the cone of nonnegative combinations of the columns. If b lies inside that cone say b        what is the feasible vector x ?  If b lies outside say b        what vector y will satisfy the alternative? . In three dimensions  can you find a set of six vectors whose cone of nonnegative combinations fills the whole space? What about four vectors? . Use H to show that the following equation has no solution because the alternative holds           x      . . Use I to show that there is no solution x   the alternative holds            x      . . Show that the alternatives in J  Ax  b  x   yA   yb   y   cannot both hold. Hint  yAx .  .    Network Models Some linear problems have a structure that makes their solution very quick.  Band ma trices have all nonzeros close to the main diagonal and Ax  b is easy to solve. In linear programming we are interested in the special class for which A is an incidence matrix . Its entries are   or   or mostly zero  and pivot steps involve only additions and subtractions. Much larger problems than usual can be solved. Networks  enter  all  kinds  of  applications.   Traffic  through  an  intersection  satisfies Kirchhoffs current law flow in equals flow out. For gas and oil network programming has designed pipeline systems that are millions of dollars cheaper than the intuitive not optimized designs. Scheduling pilots and crews and airplanes has become a significant problem in applied mathematics!  We even solve the marriage problem to maximize the number of marriages when brides have a veto. That may not be the real problem but it is the one that network programming solves. The problem in Figure . is to maximize the flow from the source to the sink . The flows cannot exceed the capacities marked on the edges  and the directions given by the arrows cannot be reversed.  The flow on the two edges into the sink cannot exceed     . Is this total of  achievable? What is the maximal flow from left to right? The unknowns are the flows x  from node i to node j .  The capacity constraints are x   c  . The flows are nonnegative x    going with the arrows. By maximizing the return flow x  dotted line we maximize the total flow into the sink.    Another constraint is still to be heard from. It is the conservation law that the flow into each node equals the flow out . That is Kirchhoffs current law Current law   x     x       for j     ...  .  The flows x  enter node j from earlier nodes i .   The flows x  leave node j to later nodes k .  The balance in equation  can be written as Ax   where A is a nodeedge incidence matrix the transpose of Section .. A has a row for every node and a       column for every edge Incidence Matrix A                                                   node       edge                                  Maximal Flow Maximize x  subject to Ax   and   x   c  . A flow of  can go on the path .   A flow of  can go along .   An additional flow of  can take the lowest path .  The total is  and no more is possible . How do you prove that the maximal flow is  and not ? Trial and error is convincing but mathematics is conclusive The key is to find a cut in the network across which all capacities are filled. That cut separates nodes  and  from the others. The edges that go forward across the cut have total capacity        and no more can get across! Weak duality says that every cut gives a bound to the total flow and full duality says that the cut of smallest capacity  the minimal cut  is filled by the maximal flow. K Max flowmin cut theorem. The maximal flow in a network equals the total capacity across the minimal cut. A cut splits the nodes into two groups S and T source in S and sink in T . Its capacity is the sum of the capacities of all edges crossing the cut from S to T .  Several cuts might have the same capacity. Certainly the total flow can never be greater than the total capacity across the minimal cut. The problem here and in all of duality is to show that equality is achieved by the right flow and the right cut. Proof that max flow  min cut. Suppose a flow is maximal. Some nodes might still be reached from the source by additional flow without exceeding any capacities.  Those nodes go with the source into the set S .  The sink must lie in the remaining set T  or it could have received more flow!  Every edge across the cut must he filled or extra flow could have gone further forward to a node in T . Thus the maximal flow does fill this cut to capacity. and equality has been achieved. This suggests  a  way  to  construct  the  maximal  flow  Check  whether  any  path  has unused capacity.  If so add flow along that augmenting path.  Then compute the re maining capacities and decide whether the sink is cut off from the source or additional flow is possible. If you label each node in S by the previous node that flow could come from you can backtrack to find the path for extra flow.  The Marriage Problem Suppose we have four women and four men. Some of those sixteen couples are compat ible others regrettably are not.  When is it possible to find a complete matching  with everyone married?  If linear algebra can work in dimensional space it can certainly handle the trivial problem of marriage. There are two ways to present the problemin a matrix or on a graph.  The matrix contains a    if the i th woman and j th man are not compatible and a    if they are willing to try. Thus row i gives the choices of the i th woman and column j corresponds to the j th man Compatibility matrix A                                                    has  compatible pairs. The left graph in Figure . shows two possible marriages.  Ignoring the source s and sink t  it has four women on the left and four men on the right. The edges correspond to the  s in the matrix  and the capacities are  marriage. There is no edge between the first woman and fourth man because the matrix has a   .     It might seem that node M  cant be reached by more flowbut that is not so!  The extra flow on the right goes backward to cancel an existing marriage.  This extra flow makes  marriages which is maximal. The minimal cut is crossed by  edges. A complete matching if it is possible is a set of four is in the matrix.  They would come from four different rows and four different columns since bigamy is not allowed. It is like finding a permutation matrix within the nonzero entries of A . On the graph this means four edges with no nodes in common.  The maximal flow is less than  exactly when a complete matching is impossible. In our example the maximal flow is  not . The marriages    are allowed   and several other sets of three marriages but there is no way to reach four. The minimal cut on the right separates the two women at the bottom from the three men at the top. The two women have only one man left to choosenot enough. The capacity across the cut is only . Whenev er there is a subset of k women who among them like fewer than k men a complete matching is impossible. That test is decisive. The same impossibility can be expressed in different ways . For Chess It is impossible to put four rooks on squares with s in A  so that no rook can take any other rook. . For Marriage Matrices The s in the matrix can be covered by three horizontal or vertical lines. That equals the maximum number of marriages. . For Linear Algebra Every matrix with the same zeros as A is singular. Remember that the determinant is a sum of !   terms. Each term uses all four rows and columns. The zeros in A make all  terms zero. A block of zeros is preventing a complete matching! The  by  submatrix in rows   and columns    of A is entirely zero. The general rule for an n by n matrix is that a p by q block of zeros prevents a matching if p  q  n . Here women   could marry only the man . If p women can marry only n  q men and p  n  q which is the same as a zero block with p  q  n  then a complete matching is impossible. The mathematical problem is to prove the following If every set of p women does like at least p men a complete matching is possible. That is Halls condition . No block of zeros is too large.  Each woman must like at least one man each two women must between them like at least two men and so on to p  n . L A complete matching is possible if and only if Halls condition holds. The proof is simplest if the capacities are n   instead of   on all edges across the middle. The capacities out of the source and into the sink are still . If the maximal flow is n  all those edges from the source and into the sink are filledand the flow produces n marriages.  When a complete matching is impossible and the maximal flow is below n  some cut must be responsible. That cut will have capacity below n  so no middle edges cross it. Suppose p nodes on the left and r nodes on the right are in the set S with the source. The capacity across that cut is n  p from the source to the remaining women and r from these men to the sink. Since the cut capacity is below n  the p women like only the r men and no others.  But the capacity n  p  r is below n exactly when p  r  and Halls condition fails.  Spanning Trees and the Greedy Algorithm A fundamental network model is the shortest path problem in which the edges have lengths instead of capacities. We want the shortest path from source to sink. If the edges are telephone lines and the lengths are delay times we are finding the quickest route for a call If the nodes are computers we are looking for the perfect messagepassing protocol. A  closely  related  problem  finds  the shortest  spanning  tree a  set  of n    edges connecting all the nodes of the network.  Instead of getting quickly between a source and a sink we are now minimizing the cost of connecting all the nodes.  There are no loops  because the cost to close a loop is unnecessary. A spanning tree connects the nodes without loops  and we want the shortest one. Here is one possible algorithm . Start from any node s and repeat the following step Add the shortest edge that connects the current tree to a new node. In Figure . the edge lengths would come in the order      .  The last step skips the edge of length  which closes a loop. The total length is but is it minimal? We accepted the edge of length  very early and the second algorithm holds out longer.    . Accept edges in increasing order of length rejecting edges that complete a loop. Now the edges come in the order      again rejecting  and . They are the same edgesalthough that will not always happen. Their total length is the sameand that does always happen. The spanning tree problem is exceptional because it can be solved in one pass . In the language of linear programming we are finding the optimal corner first.  The spanning tree problem is being solved like backsubstitution with no false steps .  This general approach is called the greedy algorithm . Here is another greedy idea . Build trees from all n nodes by repeating the following step Select any tree and add the minimumlength edge going out from that tree. The  steps  depend  on  the  selection  order  of  the  trees.   To  stay  with  the  same  tree  is algorithm .  To take the lengths in order is algorithm .  To sweep through all the trees   in turn is a new algorithm.  It sounds so easy but for a large problem the data structure becomes critical With a thousand nodes there might be nearly a million edges and you dont want to go through that list a thousand times. Further Network Models There are important problems related to matching that are almost as easy .  The optimal assignment problem  a  measures the value of applicant i in job j . Assign jobs to maximize the total valuethe sum of the a  on assigned jobs.  If all a  are  or  this is the marriage problem. .  The transportation problem  Given supplies at n points and demands at n markets choose shipments x  from suppliers to markets that minimize the total cost  C  x  . If all supplies and demands are  this is the optimal assignment problemsending one person to each job. . Minimum costflow  Now the routes have capacities c  as well as costs C   mixing the maximal flow problem with the transportation problem.  What is the cheapest flow subject to capacity constraints? A  fascinating  part  of  this  subject  is  the  development  of  algorithms.   Instead  of  a theoretical  proof  of  duality  we  use breadthfirst  search  or  depthfirst  search to  find the optimal assignment or the cheapest flow.  It is like the simplex method in starting from a feasible flow a corner and adding a new flow to move to the next corner The algorithms are special because network problems involve incidence matrices. The technique of dynamic programming rests on a simple idea If a path from source to sink is optimal  then each part of the path must be optimal .   The solution is built backwards from the sink with a multistage decision process. At each stage the distance to the sink is the minimum of a new distance plus an old distance Bellman equation xt distance  minimum over y of  xy  yt distances. I wish there were space for more about networks. They are simple but beautiful. Pr oblem Set . . In Figure .  add  to every capacity.   Find by inspection the maximal flow and minimal cut. . Find a maximal flow and minimal cut for the following network  . If you  could  increase  the  capacity  of  any  one  pipe  in  the  network  above  which change would produce the largest increase in the maximal flow? . Draw a node network with capacity  i  j  between node i and node j .  Find the largest possible flow from node  to node . . In a graph the maximum number of paths from s to t with no common edges equals the minimum number of edges whose removal disconnects s from t .  Relate this to the max flowmin cut theorem. . Find a maximal set of marriages a complete matching if possible for A                                                                                 and B                                                                                 . Sketch the network for B  with heavier lines on the edges in your matching. . For the matrix A in Problem  which rows violate Halls conditionby having all their s in too few columns? Which p by q submatrix of zeros has p  q  n ? . How many lines horizontal and vertical are needed to cover all the s in A in Prob lem ? For any matrix explain why weak duality is true If k marriages are possible then it takes at least k lines to cover all the s. . a  Suppose every row and every column contains exactly two s. Prove that a com plete matching is possible.  Show that the s cannot be covered by less than n lines b  Find an example with two or more is in each row and column for which a com plete matching is impossible. . If a  by  matrix has  s prove that it allows at least  marriages. . For infinite sets a complete matching may be impossible even if Hails condition is passed. If the first row is all s and then every a      show that any p rows have s in at least p columnsand yet there is no complete matching.   . If Figure . shows lengths instead of capacities find the shortest path from s to t  and a minimal spanning tree. . Apply algorithms  and  to find a shortest spanning tree for the network of Problem . . a  Why does the greedy algorithm work for the spanning tree problem? b  Show by example that the greedy algorithm could fail to find the shortest path from s to t  by starting with the shortest edge. . If A is the  by  matrix with is just above and just below the main diagonal find a  a set of rows with s in too few columns. b  a set of columns with is in too few rows. c  a p by q submatrix of zeros with p  q  . d  four lines that cover all the s. . The maximal flow problem has slack variables w   c   x  for the difference be tween capacities and flows. State the problem of Figure . as a linear program. . Game Theory The best way to explain a twoperson zerosum game is to give an example. It has two players X and Y  and the rules are the same for every turn X holds up one hand or two and so does Y . If they make the same decision Y wins .  If they make opposite decisions X wins  for one hand and  for two Payoff matrix payments to X  A          one hand by Y two hands by Y one hand by X two hands by X If X does the same thing every time Y will copy him and win. Similarly Y cannot stick to a single strategy or X will do the opposite.  Both players must use a mixed strategy  and the choice at every turn must be independent of the previous turns. If there is some historical pattern the opponent can take advantage of it.  Even the strategy stay with the same choice until you lose is obviously fatal.  After enough plays your opponent would know exactly what to expect. In a mixed strategy X can put up one hand with frequency x  and both hands with frequency x     x  .   At every turn this decision is random.   Similarly Y can pick  probabilities y  and y     y  . None of these probabilities should be  or  otherwise the opponent adjusts and wins.  If they equal    Y w ould be losing  too often.  He would lose  a quarter of the time  another quarter of the time and win  half the timean average loss of .. This is more than necessary. But the more Y moves toward a pure twohand strategy the more X will move toward one hand. The fundamental problem is to find the best mixed strategies . Can X choose probabil ities x  and x  that present Y with no reason to move his own strategy and vice versa? Then the average payoff will have reached a saddle point  It is a maximum as far as X is concerned and a minimum as far as Y is concerned. To find such a saddle point is to solve the game. X is combining the two columns with weights x  and   x  to produce a new mixed column. Weights   and   w ould produce this column Mixed column                     . Against this  mixed  strategy Y will  always  lose .   This  does  not  mean  that  all strategies are optimal for Y !  If Y is lazy and stays with one hand X will change and start winning . Then Y will change and then X again. Finally since we assume they are both intelligent they settle down to optimal mixtures. Y will combine the rows with weights y  and   y   trying to produce a new row which is as small as possible Mixed row y            y             y      y   . The right mixture makes the two components equal at y     . Then both components equal  the mixed row becomes      . With this strategy Y cannot lose more than  . Y has minimized the maximum loss and that minimax agrees with the maximin found by X . The value of the game is minimax  maximin  . The  optimal  mixture  of  rows  might  not  always  have  equal  entries!   Suppose X is allowed a third strategy of holding up three hands to win  when Y puts up one hand and  when Y puts up two. The payoff matrix becomes A              . X will choose the threehand strategy column  every time and win at least .  At the same time Y always chooses the first row the maximum loss is .  We still have maximin  minimax   but the saddle point is over in the corner. In Y s optimal mixture of rows  which was purely row    appears only in the column actually used by X .  In X s optimal mixture of columns which was column   appears in the row that enters Y s best strategy. This rule corresponds exactly to the complementary slackness condition of linear programming.   Matrix Games The most general  m by n matrix game is exactly like our example. X has n possible moves columns of A . Y chooses from the m rows.  The entry a  is the payment when X chooses column j and Y chooses row i . A negative entry means a payment to Y . This is a zerosum game . Whatever one player loses the other wins. X is free to choose any mixed strategy x   x  ... x   . These x  give the frequencies for the n columns and they add to .  At every turn X uses a random device to produce strategy i with frequency x  . Y chooses a vector y   y  ... y    also with y    and  y    which gives the frequencies for selecting rows. A single play of the game is random.  On the average the combination of column j for X and row i for Y will turn up with probability x  y  . When it does come up the payoff is a  . The expected payoff to X from this combination is a  x  y   and the total expected payoff from each play of the same game is   a  x jy   yAx  yAx   y   y      a  a   a   . . . . . . . . . a   a    a          x  x  . . . x        a  x  y     a  x  y   average payoff . It is this payoff yAx that X wants to maximize and Y wants to minimize. Example . Suppose A is the n by n identity matrix A  I .  The expected payoff be comes yIx  x  y     x  y  . X is hoping to hit on the same choice as Y  to win a   . Y is trying to evade X  to pay a   . If X chooses any column more often than another Y can escape more often. The optimal mixture is x      n    n ...   n  .  Similarly Y cannot overemphasize any rowthe optimal mixture is y      n    n ...   n  .  The probability that both will choose strategy i is    n    and the sum over i is the expected payoff to X . The total value of the game is n times    n    or   n  y  Ax      n    n      . . .          n . . .   n       n         n     n . As n increases Y has a better chance to escape. The value   n goes down. The symmetric matrix A  I did not make the game fair. A skewsymmetric matrix  A    A  means a completely fair game .  Then a choice of strategy j by X and i by Y wins a  for X  and a choice of j by Y and i by X wins the same amount for Y because a    a  . The optimal strategies x  and y  must be the same and the expected payoff must be y  Ax   .  The value of the game when A    A  is zero.  But the strategy is still to be found.  Example . Fair game A                    . In words X and Y both choose a number between  and . The smaller choice wins . If X chooses  and Y chooses  the payoff is a    if they choose the same number we are on the diagonal and nobody wins. Neither player can choose a strategy involving  or . The pure strategies x   y          are optimalboth players choose  every time. The value is y  Ax   a   . The matrix that leaves all decisions unchanged has mn equal entries  say  .   This simply means that X wins an additional amount  at every turn. The value of the game is increased by   but there is no reason to change x  and y  . The Minimax Theorem Put yourself in the place of X  who chooses the mixed strategy x   x  ... x   . Y will eventually recognize that strategy and choose y to minimize the payment yAx . An intel ligent player X will select x  to maximize this minimum  X wins at least min  yAx   max  min  yAx .  Player Y does the opposite.  For any chosen strategy y  X will maximize yAx .  There fore Y will choose the mixture y  that minimizes this maximum  Y loses no more than max  y  Ax  min  max  yAx .  I  hope  you  see  what  the  key  result  will  be  if  it  is  true.   We  want  the  amount  in equation  that X is guaranteed to win to equal the amount in equation  that Y must be satisfied to lose.  Then the game will be solved X can only lose by moving from x  and Y can only lose by moving from y   The existence of this saddle point was proved by von Neumann M For any matrix A  the minimax over all strategies equals the maximin Minimax theorem max  min  yAx  min  max  yAx  value of the game.  If the maximum on the left is attained at x   and the minimum on the right is attained at y   this is a saddle point from which nobody wants to move y  Ax  y  Ax   yAx  for all x and y .  At this saddle point x  is at least as good as any other x since y  Ax  y  Ax  . And the second player Y could only pay more by leaving y  .   As in duality theory maximin  minimax is easy. We combine the definition in equa tion  of x  and the definition in equation  of y   max  min  yAx  min  yAx   y  Ax   max  y  Ax  min  max  yAx .  This only says that if X can guarantee to win at least   and Y can guarantee to lose no more than   then    .  The achievement of von Neumann was to prove that    . The minimax theorem means that equality must hold throughout equation . F or us the striking thing about the proof is that it uses exactly the same mathematics as the theory of linear programming . X and Y are playing dual roles.  They are both choosing strategies from the feasible set of probability vectors x     x    y     y   . What is amazing is that even von Neumann did not immediately recognize the two theories as the same. He proved the minimax theorem in  linear programming began  before    and  Gale  Kuhn  and  Tucker  published  the  first  proof  of  duality in based on von Neumanns notes!  We are reversing history by deducing the minimax theorem from duality. Briefly the minimax theorem can be proved as follows.  Let b be the column vector of m s and c be the row vector of n s. These linear programs are dual P minimize cx D maximize yb subject to Ax  b  x   subject to yA  c  y  . To make sure that both problems are feasible  add a large number  to all entries of A .  This cannot affect the optimal strategies since every payoff goes up by  .  For the resulting matrix which we still denote by A  y   is feasible in the dual and any large x is feasible in the primal. The duality theorem of linear programming guarantees optimal x  and y  with cx   y  b . Because of the s in b and c  this means that  x     y    S . Division by S changes the sums to and the resulting mixed strategies x   S and y   S are optimal .  For any other strategies x and y  Ax   b implies yAx   yb      and y  A  c implies y  Ax  cx   . The main point is that y  Ax    yAx  .  Dividing by S  this says that player X cannot win more than   S against the strategy y   S   and player Y cannot lose less than   S against x   S . Those strategies give maximin  minimax    S . Real Games This completes the theory but it leaves a natural question  Which ordinary games are actually equivalent to matrix games? Do chess and bridge and poker fit into von Neumanns theory? I think chess does not fit very well for two reasons. A strategy for black must include a decision on how to respond to whites first play and second play and so on to the end  of the game. X and Y have billions of pure strategies.  I do not see much of a role for chance.  If white can find a winning strategy or if black can find a drawing strategy neither has ever been foundthat would effectively end the game of chess.  You could play it like tictactoe but the excitement would go away. Bridge does contain some deceptionas in a finesse. It counts as a matrix game but m and n are again fantastically big.  Perhaps separate parts of bridge could be analyzed for an optimal strategy.  The same is true in baseball where the pitcher and batter try to outguess each other on the choice of pitch.  Or the catcher tries to guess when the runner will steal. A pitchout every time will walk the batter so there must be an optimal frequencydepending on the base runner and on the situation.  Again a small part of the game could be isolated and analyzed. On the other hand blackjack is not a matrix game in a casino because the house follows fixed rules.   My friend Ed Thorp found a winning strategy by counting high cardsforcing more shuffling and more decks at Las Vegas.  There was no element of chance and no mixed strategy x  .  The bestseller Bringing Down the House tells how MIT students made a lot of money while not doing their homework. There is also the Prisoners Dilemma  in which two accomplices are separately of fered the same deal Confess and you are free provided your accomplice does not con fess the accomplice then gets  years.  If both confess each gets  years.  If neither confesses only a minor crime  years each can be proved. What to do? The temptation to confess is very great although if they could depend on each other they would hold out. This is not a zerosum game both can lose. One example of a matrix game is poker . Bluffing is essential and to be effective it has to be unpredictable.  If your opponent finds a pattern you lose.  The probabilities for and against bluffing will depend on the cards that are seen and on the bets.  In fact the number of alternatives again makes it impractical to find an absolutely optimal strategy x  . A good poker player must come pretty close to x   and we can compute it exactly if we accept the following enormous simplification of the game X is dealt a jack or a king with equal probability and Y always gets a queen. X can fold and lose the  ante or bet an additional .  If X bets Y can fold and lose  or match the extra  and see if X is bluffing.  Then the higher card wins the  from the opponent. So Y has two possibilities reacting to X who has four strategies Strategies Row  If X bets Y folds. for Y Row  If X bets Y matches the extra .  Bet the extra  on a king and fold on a jack. Strategies  Bet the extra  in either case bluffing. for X  Fold in either case and lose  foolish.  Fold on a king and bet on a jack foolish.   The payoff matrix A requires a little patience to compute a    X loses  half the time on a jack and wins on a king  Y folds. a       Both bets X loses  half the time and wins  half the time. a    X bets and Y folds the bluff succeeds. a    X wins  with the king and loses  with the jack the bluff fails. Poker payoff matrix A                   . The optimal strategy for X is to bluff half the time x              . The underdog Y must choose y          . The value of the game is fifty cents to X . That is a strange way to end this book by teaching you how to play watereddown poker blackjack pays a lot better.  But I guess even poker has its place within linear algebra and its applications. I hope you have enjoyed the book. Pr oblem Set . . How will the optimal strategies in the game that opens this section be affected if the  is increased to ? What is the value the average win for X  of this new game? . With payoff matrix A       explain the calculation by X of the maximin and by Y of the minimax. What strategies x  and y  are optimal? . If a  is the largest entry in its row and the smallest in its column why will X always choose column j and Y always choose row i regardless of the rest of the matrix? Show that the preceding problem had such an entry and then construct an A without one. . Compute Y s best strategy by weighting the rows of A      with y and   y . X will concentrate on the largest of the components  y      y    y  and y      y  . Find the largest of those three depending on y  and then find the y  between  and  that makes this largest component as small as possible. . With the same A as in Problem  find the best strategy for X . Show that X uses only the two columns the first and third that meet at the minimax point in the graph. . Find both optimal strategies and the value if A             . . Suppose A      .   What weights x  and   x  will give a column of the form  u  u    and what weights y  and   y  on the two rows will give a new row  v  v  ? Show that u  v .  . Find x   y  and the value v for A                             . . Compute min            max             x  y   x  y   . . Explain each of the inequalities in equation .  Then once the minimax theorem has turned them into equalities derive again in words the saddle point equations . . Show that x              and y          are optimal strategies in our simplified version of poker by computing yAx  and y  Ax and verifying the conditions  for a saddle point. . Has it been proved that no chess strategy always wins for black?  This is certainly true when the players are given two moves at a time if black had a winning strategy white could move a knight out and back and then follow that strategy leading to the impossible conclusion that both would win. . If X chooses a prime number and simultaneously Y guesses whether it is odd or even with gain or loss of  who has the advantage? . If X is a quarterback with the choice of run or pass and Y can defend against a run or a pass suppose the payoff in yards is A         defense against run defense against pass. run    pass What are the optimal strategies and the average gain on each play?    A.    The Intersection of Two Vector Spaces New questions arise from considering two subspaces V and W  not just one.  We look first at the vectors that belong to both subspaces. This intersection V  W is a subspace of those subspaces If V and W are subspaces of one vector space so is their intersection V  W . The vectors belonging to both V and W form a subspace . Suppose x and y are vectors in V and also in W . Because V and W are vector spaces in their own right x  y and cx are in V and in W . The results of addition and scalar multiplication stay within the intersection . Two planes through the origin or two hyperplanes in R   meet in a subspace. The intersection of several subspaces or infinitely many is again a subspace. Example . The intersection of two orthogonal subspaces V and W is the onepoint subspace V  W     . Only the zero vector is orthogonal to itself. Example . Suppose V and W are the spaces of n by n upper and lower triangular matrices.  The intersection V  W is the set of diagonal matrices belonging to both triangular subspaces.  Adding diagonal matrices or multiplying by c  leaves a diagonal matrix. Example . Suppose V is the nullspace of A  and W is the null space of B . Then V  W is the smaller nullspace of the larger matrix C  Intersection of nullspaces   A     B  is the nullspace of C   A B  . Cx   requires both Ax   and Bx  . So x has to be in both nullspaces.  A.    The Sum of Two Vector Spaces Usually after discussing the intersection of two sets it is natural to look at their Union. With vector spaces this is not natural. The union V  W of two subspaces will not in general be a subspace .  If V and W are the x axis and the y axis in the plane the two axes together are not a subspace. The sum of      and      is not on either axis. We  do  want  to  combine V and W .   In  place  of  their  union  we  turn  to  their  sum. Definition. If V and W are both subspaces of a given space so is their sum . V  W contains all combinations v  w  where v is in V and w is in W . V  W is the smallest vector space that contains both V and W .   The sum of the x axis and the y axis is the whole x  y plane.  So is the sum of any two different lines perpendicular or not . If V is the x axis and W is the   line x  y  then any vector like      can be split into v  w           . Thus V  W is all of R  . Example . Suppose V and W are orthogonal complements in R  .  Then their sum is V  W  R  . Every x is the sum of its projections in V and W . Example . If V is the space of upper triangular matrices and W is the space of lower triangular matrices then V  W is the space of all matrices. Every n by n matrix can be written as the sum of an upper and a lower triangular matrixin many ways because the diagonals are not uniquely determined. These triangular subspaces have dimension n  n     . The space V  W of all matri ces has dimension n  . The space V  W of diagonal matrices has dimension n . Formula  below becomes n   n  n  n       n  n     . Example . If V is the column space of A  and W is the column space of B  then V  W is the column space of the larger matrix  A  B  .  The dimension of V  W may be less than the combined dimensions of V and W because these two spaces might overlap Sum of column spaces dim  V  W   rank of  A  B  .  The computation of V  W is more subtle .  For the intersection of column spaces a good method is to put bases for V and W in the columns of A and B .  The nullspace of  A  B  leads to V  W see Problem . Those spaces have the same dimension the nullity of  A  B  . Combining with dim  V  W  gives dim  V  W  dim  V  W   rank of  A  B  nullity of  A  B  .  We know that the rank plus the nullity counting pivot columns plus free columns al ways equals the total number of columns. When  A B  has k   columns with k  dim V and   dim W  we reach a neat conclusion Dimension formula dim  V  W  dim  V  W   dim  V  dim  W  .  Not a bad formula. The overlap of V and W is in V  W .   A.    The Cartesian Product of Two Vector Spaces If V has dimension n  and W has dimension q  their Cartesian product V  W has di mension n  q . Definition. V  W contains all pairs of vectors x   v  w  . Adding  v  w  to  v   w   in this product space gives  v  v   w  w   .  Multiplying by c gives  cv  cw  . All operations in V  W are a component at a time. Example . The Cartesian product of R  and R  is very much like R  . A typical vector x in R   R  is               one vector from R  and one from R  .  That looks like            in R  . Cartesian products go naturally with block matrices . From R  to R   we have ordinary  by  matrices. On the product space R   R   the natural form of a matrix is a  by  block matrix M  M   R  to R  R  to R  R  to R  R  to R      by     by   by     by     A   B C   D  . Matrixvector multiplication produces  Av  Bw  Cv  Dw  . Not too fascinating. A.    The Tensor Product of Two Vector Spaces Somehow we want a product space that has dimension n times q . The vectors in this tensor product denoted   will look like n by q matrices .  For the tensor product R   R   the vectors will look like  by  matrices. The dimension of R   R  is  but the dimension of R   R  is going to be . Start with v       and w         in R  and R  .  The Cartesian product just puts them next to each other as  v  w  . The tensor product combines v and w into the rank  matrix vw   Column times row v  w  vw                            . All the special matrices vw  belong to the tensor product R   R  .  The product space is spanned by those vectors v  w .   Combinations of rank matrices give all  by  matrices so the dimension of R   R  is .  Abstractly  The tensor product V  W is identified with the space of linear transformations from V to W . If V is only a line in R   and W is only a line in R   then V  W is only a line in matrix space.  The dimensions are now     .  All the rank matrices vw  will be multiples of one matrix.  Basis for the Tensor Product. When V is R  and W is R   we have a standard basis for all  by  matrices a sixdimensional space Basis                                                                                                 . That basis for R   R  was constructed in a natural way.   I started with the standard basis v        and v        for R  .  Those were combined with the basis vectors w           w           and w          in R  .  Each pair v   w  corresponds to one of the six basis vectors  by  matrices above in the tensor product V  W .  This construction succeeds for subspaces too Basis Suppose V and W are subspaces of R  and R  with bases v  ... v  and w  ... w  . Then the nq rank matrices v  w   are a basis for V  W . V  W is an nq dimensional subspace of m by p matrices An algebraist would match this matrix construction to the abstract definition of V  W .  Then tensor products can go beyond the specific case of column vectors. A.    The Kronecker Product A  B of Two Matrices An m by n matrix A transforms any vector v in R  to a vector Av in R   Similarly a p by q matrix B transforms w to Bw . The two matrices together transform vw  to Avw  B  . This is a linear transformation of tensor products and it must come from a matrix. What is the size of that matrix A  B ?  It takes the nq dimensional space R   R  to the m p dimensional space R   R  . Therefore the matrix has shape mp by nq . We will write this Kronecker product also called tensor product as a block matrix Kronecker product m p rows nq columns A  B       a  B    a  B  a   B a  B    a  B  a   B         a   B   a   B  a  B      .  Notice the special structure of this matrix!  A lot of important block matrices have that Kronecker form.   They often come from twodimensional applications  where A is a matrix in the x direction and B is acting in the y direction examples below. If A and B are square so m  n and p  q  then the big matrix A  B is also square. Example . Finite differences in the x and y directions Laplaces partial differen tial equation    u   x     u   y    is replaced by finite differences to find values for u on a twodimensional grid. Differences in the x direction add to differences in the y direction connecting five neighboring values of u                                                                                 A point equation is centered at each of the nine meshpoints. The  by  matrix call it A   is constructed from the  by  D matrix for differences along a line Difference matrix in one direction A                     Identity matrix in other direction I                             . Kronecker products produce three D differences along three lines up or across One direction A  I      I  I   I  I  I   I  I    . Other direction I  A     A       A       A    . Both directions A    A  I  I  A      A   I  I   I A   I  I   I A   I    . The sum  A  I  I  A  is the  by  matrix for Laplaces fivepoint difference equation Section . was for D and Section . mentioned D. The middle row of this  by  matrix shows all five nonzeros from the fivepoint molecule Away from boundary Row  of A                        . Example . The Fourier matrix in D The onedimensional Fourier matrix F is the most important complex matrix in the world. The Fast Fourier Transform in Section . is a quick way to multiply by that matrix F . So the FFT transforms time domain to frequency domain for a D audio signal. For images we need the D transform  Fourier matrix in D F   F  F  Transform along each row then down each column The image is a twodimensional array of pixel values. It is transformed by F  into a two dimensional array of Fourier coefficients. That array can be compressed and transmitted  and stored. Then the inverse transform brings us back from Fourier coefficients to pixel values. We need to know the inverse rule for Kronecker products The inverse of the matrix A  B is the matrix A    B   . The FFT also speeds up the D inverse transform!  We just invert in one direction fol lowed by the other direction. We are adding   c   e  e    over k and then  . The Laplace difference matrix A    A  I  I  A  has no simple inverse formula. That is why the equation A  u  b has been studied so carefully.  One of the fastest methods is to diagonalize A  by using its eigenvector matrix which is the Fourier sine matrix S  S  very similar to F  .  The eigenvalues of A  come immediately from the eigenvalues of A   The n  eigenvalues of  A  I  I  B  are all the sums    A     B  . The n  eigen values of A  B are all the products    A     B  . If A and B are n by n  the  determinant  of A  B the  product  of  its  eigenvalues  is  det A    det B   . The trace of A  B is trace A trace B . This appendix illustrates both pure linear algebra and its crucial applications! Pr oblem Set A . Suppose S and T are subspaces of R   with dim S   and dim T  . a  What is the largest possible dimension of S  T ? b  What is the smallest possible dimension of S  T ? c  What is the smallest possible dimension of S  T ? d  What is the largest possible dimension of S  T ? . What are the intersections of the following pairs of subspaces? a  The x  y plane and the y  z plane in R   b  The line through        and the plane through        and        . c  The zero vector and the whole space R  . d  The plane S perpendicular to        and perpendicular to        in R  . What are the sums of those pairs of subspaces? . Within the space of all  by  matrices let V be the subspace of tridiagonal matrices and W the subspace of upper triangular matrices.  Describe the subspace V  W  whose members are the upper Hessenberg matrices. What is V  W ? Verify formula .   . If V  W contains only the zero vector then equation  becomes dim  V  W   dim V  dim W .  Check this when V is the row space of A  W is the nullspace of A  and the matrix A is m by n of rank r . What are the dimensions? . Give an example in R  for which V  W contains only the zero vector but V is not orthogonal to W . . If V  W      then V  W is called the direct sum of V and W  with the special notation V  W .  If V is spanned by        and         choose a subspace W so that V  W  R   Explain why any vector x in the direct sum V  W can be written in one and only one way as x  v  w with v in V and w in W . . Find a basis for the sum V  W of the space V spanned by v             v            and the space W spanned by w             w            .  Find also the dimension of V  W and a basis for it. . Prove from equation  that rank  A  B   rank  A  rank  B  . .   The intersection   A     B  matches the nullspace of  A B  . Each y  Ax   Bx  in the column spaces of both A and B matches x   x    x   in the nullspace because  A  B  x  Ax   Bx   . Check that y         matches x              and find the intersection   A     B   for A                    B                    . . Multiply A  B times A    B   to get AA    BB    I  I  I  . . What is the  by  Fourier matrix F   F  F for F        ? . Suppose Ax    A  x and By    B  y . Form a long column vector z with n  compo nents x  y  then x  y  and eventually x  y . Show that z is an eigenvector for  A  I  z    A  z and  A  B  z    A    B  z . . What would be the sevenpoint Laplace matrix for  u   u   u   ? This three dimensional matrix is built from Kronecker products using I and A  .    Given a square matrix A  we want to choose M so that M   AM is as nearly diagonal as possible.  In the simplest case A has a complete set of eigenvectors and they become the columns of M otherwise known as S .  The Jordan form is J  M   AM    it is constructed entirely from  by  blocks J       and the goal of a diagonal matrix is completely achieved. In the more general and more difficult case some eigenvectors are missing and a diagonal form is impossible. That case is now our main concern. W e repeat the theorem that is to be proved If a matrix A has s linearly independent eigenvectors  then it is similar to a matrix J that is in Jordan form  with s square blocks on the diagonal J  M   AM     J  . . . J     . Each block has one eigenvector one eigenvalue and is just above the diagonal J                      . An example of such a Jordan matrix is J                                                                                                                                   J  J  J         . The double eigenvalue    has only a single eigenvector in the first coordinate direc tion e               as a result    appears only in a single block J  .  The triple   eigenvalue    has two eigenvectors e  and e   which correspond to the two Jordan blocks J  and J  .  If A had  eigenvectors all blocks would be  by  and J would be diagonal. The key question is this If A is some other  by  matrix under what conditions will its Jordan form be this same J? When will there exist an M such that M   AM  J ? As a first requirement any similar matrix A must share the same eigenvalues     . But the diagonal matrix with these eigenvalues is not similar to J and our question really concerns the eigenvectors. To answer it we rewrite M   AM  J in the simpler form AM  MJ  A        x  x  x  x  x                 x  x  x  x  x                                        . Carrying out the multiplications a column at a time Ax    x  and Ax    x   x   Ax    x  and Ax    x   x  and Ax    x  .  Now we can recognize the conditions on A .  It must have three genuine eigenvectors just as J has. The one with    will go into the first column of M  exactly as it would have gone into the first column of S  Ax    x   The other two which will be named x  and x   go into the third and fifth columns of M  Ax   Ax   .  Finally there must be two other special vectors the generalized eigenvectors x  and x  .  We think of x  as belonging to a string of vectors  headed by x  and described by equation .  In fact x  is the only other vector in the string and the corresponding block J  is of order . Equation  describes two different strings  one in which x  follows x   and another in which x  is alone the blocks J  and J  are  by  and  by . The search for the Jordan form of A becomes a search for these strings of vectors each one headed by an eigenvector For every i either Ax     x  or Ax     x   x    .  The vectors x  go into the columns of M  and each string produces a single block in J . Essentially we have to show how these strings can be constructed for every matrix A . Then if the strings match the particular equations  and  our J will be the Jordan form of A . I think that Filippovs idea makes the construction as clear and simple as possible  . It proceeds by mathematical induction starting from the fact that every  by  matrix      is already in its Jordan form.  We may assume that the construction is achieved for all matrices of order less than n this is the induction hypothesisand then explain the steps for a matrix of order n .  There are three steps and after a general description we apply them to a specific example. Step . If we assume A is singular then its column space has dimension r  n . Looking only within this smaller space the induction hypothesis guarantees that a Jordan form is possiblethere must be r independent vectors w  in the column space such that either Aw     w  or Aw     w   w    .  Step . Suppose the nullspace and the column space of A have an intersection of dimen sion p . Of course every vector in the nullspace is an eigenvector corresponding to   .  Therefore there must have been p strings in step  that started from this eigenvalue and we are interested in the vectors w  that come at the end of these strings.  Each of these p vectors is in the column space so each one is a combination of the columns of A  w   Ay  for some y  . Step . The  nullspace  always  has  dimension n  r .   Therefore  independent  from  its p dimensional  intersection  with  the  column  space  it  must  contain n  r  p additional basis vectors z  lying outside that intersection. Now we put these steps together to give Jordans theorem The r vectors w    the p vectors y    and the n  r  p vectors z  form Jordan strings for the matrix A  and these vectors are linearly independent.  They go into the columns of M  and J  M   AM is in Jordan form. If  we  want  to  renumber  these  vectors  as x  ... x    and  match  them  to  equation   then each y  should be inserted immediately after the w  it came from  it completes a string in which    .  The z s come at the very end each one alone in its own string again the eigenvalue is zero since the z s lie in the nullspace.  The blocks with nonzero eigenvalues are already finished at step  the blocks with zero eigenvalues grow by one row and column at step  and step  contributes any  by  blocks J      . No w we try an example and to stay close to the previous pages we take the eigenval ues to be      A                                                                                 . Step . The column space has dimension r   and is spanned by the coordinate vectors e   e   e  .   To look within this space we ignore the third and fourth rows and   columns of A  what is left has eigenvalues    and its Jordan form comes from the vectors w                       w                       w                      . The w  are in the column space they complete the string for    and they start the string for    Aw    w   Aw    w   w   Aw    w  .  Step . The nullspace of A contains e  and e   so its intersection with the column space is spanned by e  . Therefore p   and as expected there is one string in equa tion  corresponding to   . The vector w  comes at the end as well as the beginning of that string and w   A  e   e   . Therefore y  e   e  . Step . The example has n  r  p         and z  e  is in the nullspace but outside the column space. It will be this z that produces a  by  block in J . If we assemble all five vectors the full strings are Aw    w   Aw    w   w   Aw    w   Ay   y  w   Az   z . Comparing with equations  and  we have a perfect matchthe Jordan form of our example will be exactly the J we wrote earlier. Putting the five vectors into the columns of M must give AM  MJ  or M   AM  J  M                                                                . We  are  sufficiently  trustful  of  mathematics  or  sufficiently  lazy  not  to  multiply  out M   AM . In Filippovs construction the only technical point is to verify the independence of the whole collection w   y   and z  . Therefore we assume that some combination is zero  c  w    d  y    g  z    .  Multiplying by A  and using equations  for the w  as well as Az     c       w  or   w   w         d  Ay    .   The Ay  are the special w  at the ends of strings corresponding to     so they cannot appear in the first sum.   They are multiplied by zero in   w  .   Since equation  is some combination of the w   which were independent by the induction hypothesisthey supplied the Jordan form within the column spacewe conclude that each d  must be zero . Returning to equation  this leaves  c  w     g  z   and the lefthand side is in the column space.  Since the z s were independent of that space each g  must be zero. Finally  c  w    and the independence of the w  produces c   . If the original A had not been singular the three steps would have been applied instead to A   A  cI . The constant c is chosen to make A  singular and it can be any one of the eigenvalues of A . The algorithm puts A  into its Jordan form M   A  M  J  by producing the strings x  from the w   y  and z  .  Then the Jordan form for A uses the same strings and the same M  M   AM  M   A  M  M   cM  J   cI  J . This completes the proof that every A is similar to some Jordan matrix J .  Except for a reordering of the blocks it is similar to only one such J  there is a unique Jordan form for A . Thus the set of all matrices is split into a number of families with the following property All the matrices in the same family have the same Jordan form and they are all similar to each other and to J   but no matrices in different families are similar .  In every family J is the most beautifulif you like matrices to be nearly diagonal.  With this classification into families we stop. Example . A                             with        . This matrix has rank r   and only one eigenvector. Within the column space there is a single string w   w   which happens to coincide with the last two columns A            and A                     or Aw    and Aw    w   w  . The nullspace lies entirely within the column space and it is spanned by w  . Therefore p   in step  and the vector y comes from the equation Ay  w             where solution is y           .   Finally the string w   w   y goes into the matrix M  M                              and M   AM                              J . Application to du  dt  Au As always we simplify the problem by uncoupling the unknowns.  This uncoupling is complete only when there is a full set of eigenvectors and u  Sv  the best change of variables in the present case is u  Mv . This produces the new equation Mdv  dt  AMv  or dv  dt  Jv  which is as simple as the circumstances allow.  It is coupled only by the offdiagonal s within each Jordan block. In the preceding example which has a single block du  dt  Au becomes dv d t                             v or da  dt  b db  dt  c dc  dt   or a  a   b  t  c  t    b  b   c  t c  c  . The system is solved by working upward from the last equation and a new power of t enters at every step.  An  by  block has powers as high as t    .  The exponentials of J  in this case and in the earlier  by  example are e       t    t        t         and        e   te           e             t                           . You can see how the coefficients of a  b  and c appear in the first exponential. And in the second example you can identify all five of the special solutions to du  dt  Au . Three of them are the pure exponentials u   e   x   u   e   x   and u   e   x   formed as usual from the three eigenvectors of A . The other two involve the generalized eigenvectors x  and x   u   e    tx   x   and u   e    tx   x   .  The most general solution to du  dt  Au is a combination c  u     c  u   and the combination that matches u  at time t   is again u   c  x     c  x   or u   Mc  or c  M   u  . This only means that u  Me  M   u   and that the S and  in the old formula Se   S   u  have been replaced by M and J .  Problem Set B . Find the Jordan forms in three steps! of A            and B                             . . Show that the special solution u  in equation  does satisfy du  dt  Au  exactly because of the string Ax    x   Ax    x   x  . . For the matrix B in Problem  use Me  M   to compute the exponential e   and compare it with the power series I  Bt  Bt    !   . . Show that each Jordan block J  is similar to its transpose J    P   J  P  using the permutation matrix P with s along the crossdiagonal lower left to upper right. Deduce that every matrix is similar to its transpose. . Find by inspection the Jordan forms of A                             and B          . . Find the Jordan form J and the matrix M for A and B  B has eigenvalues     . What is the solution to du  dt  Au  and what is e  ? A                                                                                 and B                                 . . Suppose that A   A . Show that its Jordan form J  M   AM satisfies J   J . Since the diagonal blocks stay separate this means J    J  for each block show by direct computation that J  can only be a  by  block J      or J      . Thus A is similar to a diagonal matrix of s and s. Note . This is a typical case of our closing theorem The matrix A can be diagonalized if and only if the product  A    I  A    I    A    I   without including any repetitions of the  s is zero .  One extreme case is a matrix with distinct eigenvalues the Cayley Hamilton theorem says that with n factors A   I we always get zero. The other extreme is the identity matrix also diagonalizable  p   and A  I  . The nondiagonalizable matrix A      satisfies not  A  I    but only  A  I    an equation with a repeated root.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            . A  LU   lower triangular L s on the diagonal  upper triangular U pivots on the diagonal  Requirements No row exchanges as Gaussian elimination reduces A to U . . A  LDU   lower triangular L s on the diagonal  pivot matrix D is diagonal  upper triangular U s on the diagonal  Requirements No row exchanges.  The pivots in D are divided out to leave s in U . If A is symmetric then U is L  and A  LDL  . . PA  LU permutation matrix P to avoid zeros in the pivot positions. Requirements A is invertible.  Then P  L  U are invertible. P does the row ex changes in advance. Alternative A  L  P  U  . . EA  R  m  m invertible E  any A   rref  A  . Requirements None! The reduced row echelon form R has r pivot rows and pivot columns. The only nonzero in a pivot column is the unit pivot. The Last m  r rows of E are a basis for the left nullspace of A . and the first r columns of E   are a basis for the column space of A . . A  CC    lower triangular matrix C  transpose is upper triangular  Requirements A is symmetric and positive definite all n pivots in D are positive. This Cholesky factorization has C  L  D . . A  QR   orthonormal columns in Q  upper triangular R  Requirements A has independent columns. Those are orthogonalized in Q by the GramSchmidt process. If A is square then Q    Q  . . A  S  S     eigenvectors in S  eigenvalues in   left eigenvectors in S    . Requirements A must have n linearly independent eigenvectors.  . A  Q  Q    orthogonal matrix Q  real eigenvalue matrix   Q  is Q    . Requirements A is symmetric . This is the Spectral Theorem. . A  MJM     generalized eigenvectors in M  Jordan blocks in J  M    . Requirements A is any square matrix. Jordan form J has a block for each inde pendent eigenvector of A . Each block has one eigenvalue. . A  U  V    orthogonal U is m  m  m  n matrix    ...   on diagonal  orthogonal V is n  n  . Requir ements None. This singular value decomposition SVD has the eigenvec tors of AA  in U and of A  A in V         A  A       AA   . . A   V   U    orthogonal n  n   diagonal n  m     ...      orthogonal m  m  . Requir ements None. The pseudoinverse has A  A  projection onto row space of A and AA   projection onto column space.  The shortest leastsquares solution to Ax  b is  x  A  b . This solves A  A  x  A  b . . A  QH   orthogonal matrix Q  symmetric positive definite matrix H  . Requirements A is  invertible.   This polar  decomposition has H   A  A .   The factor H is semidefinite if A is singular.  The reverse polar decomposition A  KQ has K   AA  . Both have Q  UV  from the SVD. . A  U  U     unitary U  eigenvalue matrix   U    U   U   . Requir ements A is normal  A  A  AA  . Its orthonormal and possibly complex eigenvectors are the columns of U . Complex  s unless A  A  . . A  U T U     unitary U  triangular T with  s on diagonal  U    U   . Requir ements Schur triangularization of any square A . There is a matrix U with orthonormal columns that makes U   AU triangular. . F    I D I  D  F    F     evenodd permutation   one step of the FFT . Requirements F   Fourier matrix with entries w  where w    w  e      . Then F  F   nI . D has   w  w   ... on its diagonal.   For n    the Fast Fourier Transform has   n  multiplications from  stages of D s.    Adjacency matrix of a graph Square matrix with a    when there is an edge from node i to node j  otherwise a   . A  A  for an undirected graph. Affine transformation T  v   Av  v   linear transformation plus shift. Associative Law  AB  C  A  BC  Parentheses can be removed to leave ABC . Augmented matrix  A  b  Ax  b is solvable when b is in the column space of A  then  A  b  has the same rank as A . Elimination on  A  b  keeps equations correct. Back substitution Upper triangular systems are solved in reverse order x  to x  . Basis for V Independent vectors v  ... v  whose linear combinations give every v in V . A vector space has many bases! Big formula for n by n determinants det  A  is a sum of n ! terms one term for each permutation P of the columns. That term is the product a    a   down the diagonal of the reordered matrix times det  P    . Block matrix A matrix can be partitioned into matrix blocks by cuts between rows andor between columns. Block multiplication of AB is allowed if the block shapes permit the columns of A and rows of B must be in matching blocks. CayleyHamilton Theorem p     det  A   I  has p  A   zero matrix . Change of basis matrix M The old basis vectors v  are combinations  m  w  of the new basis vectors. The coordinates of c  v     c  v   d  w     d  w  are related by d  Mc . For n   set v   m  w   m  w   v   m  w   m  w  . Characteristic equation det  A   I       The n roots are the eigenvalues of A . Cholesk y factorization A  CC    L  D  L  D   for positive definite A .  Circulant matrix C Constant diagonals wrap around as in cyclic shift S . Every circulant is c  I  c  S    c    S    . Cx  convolution c  x . Eigenvectors in F . Cofactor C  Remove row i and column j  multiply the determinant by        . Column picture of Ax  b The vector b becomes a combination of the columns of A . The system is solvable only when b is in the column space   A  . Column space   A  Space of all combinations of the columns of A . Commuting matrices AB  BA If diagonalizable they share n eigenvectors. Companion matrix Put c  ... c  in row n and put n   s along diagonal . Then det  A   I     c   c    c       . Complete solution x  x   x  to Ax  b Particular x     x  in nullspace. Complex conjugate z  a  ib for any complex number z  a  ib .Then z z   z   . Condition number cond  A     A    A  A          In Ax  b  the relative change   x    x  is less than cond  A  times the relative change   b    b  . Condition numbers measure the sensitivity of the output to change in the input. Conjugate Gradient Method A sequence of steps to solve positive definite Ax  b by minimizing   x  Ax  x  b o ver growing Krylov subspaces. Covariance matrix  When random variables x  have mean  average value   their covariances   are the averages of x  x  . With means x   the matrix   mean of  x  x  x  x   is positive semidefinite it is diagonal if the x  are independent. Cramers Rule for Ax  b    B  has b replacing column j of A  and x    B     A  . Cross product u  v in R  Vector perpendicular to u and v  length  u  v  sin    parallelogram area computed as the determinant of  i  j  k  u  u  u   v  v  v   . Cyclic shift S Permutation with s    s    ...  finally s    . Its eigenvalues are n th roots e      of  eigenvectors are columns of the Fourier matrix F . Determinant  A   det  A  Defined by det I   sign reversal for row exchange and linearity in each row. Then  A    when A is singular. Also  AB    A  B    A        A   and  A     A  . The big formula for det  A  has a sum of n ! terms the cofactor formula uses determinants of size n   volume of box   det  A   . Diagonal matrix D    d    if i   j . Blockdiagonal  zero outside square blocks D  . Diagonalizable matrix A Must have n independent eigenvectors in the columns of S  automatic with n different eigenvalues. Then S   AS    eigenvalue matrix.   Diagonalization   S   AS   eigenvalue matrix and S  eigenvector matrix. A must have n independent eigenvectors to make S invertible. All A   S   S   . Dimension of vector space dim  V   number of vectors in any basis for V . Distributive Law A  B  C   AB  AC Add then multiply or multiply then add. Dot product x  y  x  y     x  y  Complex dot product is x  y . Perpendicular vectors have zero dot product.  AB    row i of A   column j of B . Echelon matrix U The first nonzero entry the pivot in each row comes after the pivot in the previous row. All zero rows come last. Eigenvalue  and eigenvector x Ax   x with x    so det  A   I   . Eigsho w Graphical  by  eigenvalues and singular values  MATLAB or Java. Elimination A sequence of row operations that reduces A to an upper triangular U or to the reduced form R  rref  A  . Then A  LU with multipliers   in L  or PA  LU with row exchanges in P  or EA  R with an invertible E . Elimination matrix  Elementary matrix E  The identity matrix with an extra    in the i  j entry  i   j . Then E  A subtracts   times row j of A from row i . Ellipse or ellipsoid x  Ax   A must be positive definite the axes of the ellipse are eigenvectors of A  with lengths     . For  x    the vectors y  Ax lie on the ellipse  A   y    y   AA     y   displayed by eigshow  axis lengths   . Exponential e    I  At  At    !   has derivative Ae   e  u    solves u   Au . Factorization A  LU If elimination takes A to U without row exchanges  then the lower triangular L with multipliers   and     brings U back to A . Fast Fourier Transform FFT A factorization of the Fourier matrix F  into   log  n matrices S  times a permutation. Each S  needs only n   multiplications so F  x and F    c can be computed with n   multiplications. Revolutionary. Fibonacci numbers       ... satisfy F   F     F                      . Growth rate            the largest eigenvalue of the Fibonacci matrix     . Four fundamental subspaces of A   A     A     A      A   . Fourier matrix F Entries F   e      give orthogonal columns F  F  nI . Then y  F c is the inverse Discrete Fourier Transform y    c  e      . Fr ee columns of A Columns without pivots combinations of earlier columns.  Free variable x  Column i has no pivot in elimination. We can give the n  r free variables any values then Ax  b determines the r pivot variables if solvable!. Full column rank r  n Independent columns   A       no free variables. Full row rank r  m Independent rows at least one solution to Ax  b  column space is all of R  . Full rank means full column rank or full row rank. Fundamental Theorem The nullspace   A  and row space   A   are orthogonal complements perpendicular subspaces of R  with dimensions r and n  r  from Ax  . Applied to A   the column space   A  is the orthogonal complement of   A   . GaussJordan method Invert A by row operations on  A  I  to reach  I  A    . GramSchmidt orthogonalization A  QR Independent columns in A  orthonormal columns in Q . Each column q  of Q is a combination of the first j columns of A and conversely so R is upper triangular. Convention diag  R   . Graph G Set of n nodes connected pairwise by m edges. A complete graph has all n  n      edges between nodes. A tree has only n   edges and no closed loops. A directed graph has a direction arrow specified on each edge. Hankel matrix H Constant along each antidiagonal h  depends on i  j . Hermitian matrix A   A   A Comple x analog of a symmetric matrix a    a  . Hessenberg matrix H Triangular matrix with one extra nonzero adjacent diagonal. Hilbert matrix hilb  n  Entries H      i  j        x    x    dx . Positive definite but extremely small   and large condition number. Hyper cube matrix P   Row n   counts corners edges faces ...  of a cube in R  . Identity matrix I or I   Diagonal entries   offdiagonal entries  . Incidence matrix of a directed graph The m by n edgenode incidence matrix has a row for each edge node i to node j  with entries   and  in columns i and j . Indefinite matrix A symmetric matrix with eigenvalues of both signs   and  . Independent vectors v  ... v  No combination c  v     c  v   zero vector unless all c   . If the v s are the columns of A  the only solution to Ax   is x  . Inverse matrix A   Square matrix with A   A  I and AA    I . No inverse if det A   and rank  A   n  and Ax   for a nonzero vector x . The inverses of AB and A  are B   A   and  A     Cofactor formula  A      C   det A .   Iterative method A sequence of steps intended to approach the desired solution. Jordan form J  M   AM If A has s independent eigenvectors its generalized eigenvector matrix M gives J  diag  J  ... J   . The block J  is   I   N  where N  has s on diagonal . Each block has one eigenvalue   and one eigenvector     ...   . Kir chhoffs Laws Current law  net current in minus out is zero at each node. Voltage law  Potential differences voltage drops add to zero around any closed loop. Kronecker product tensor product A  B Blocks a  B  eigenvalues    A     B  . Krylo v subspace K   A  b  The subspace spanned by b  Ab ... A    b . Numerical methods approximate A   b by x  with residual b  Ax  in this subspace. A good basis for K  requires only multiplication by A at each step. Leastsquares solution  x The vector  x that minimizes the error  e   solves A  A  x  A  b . Then e  b  A  x is orthogonal to all columns of A . Left inverse A  If A has full column rank n  then A    A  A    A  has A  A  I  . Left nullspace   A   Nullspace of A   left nullspace of A because y  A    . Length  x  Square root of x  x Pythagoras in n dimensions. Linear combination cv  dw or  c  v  Vector addition and scalar multiplication. Linear transformation T Each vector v in the input space transforms to T  v  in the output space and linearity requires T  cv  dw   cT  v  dT  w  . Examples Matrix multiplication Av  differentiation in function space. Linearly dependent v  ... v  A combination other than all c    gives  c  v   . Lucas numbers L         ...  satisfy L   L     L             with eigenvalues               of the Fibonacci matrix     . Compare L    with Fibonacci. Markov matrix M All m    and each column sum is . Largest eigenvalue   . If m    the columns of M  approach the steadystate eigenvector Ms  s  . Matrix multiplication AB The i  j entry of AB is row i of A   column j of B    a  b  . By columns column j of AB  A times column j of B . By rows row i of A multiplies B . Columns times rows AB  sum of column k row k . All these equivalent definitions come from the rule that AB times x equals A times Bx . Minimal polynomial of A The lowestdegree polynomial with m  A   zero matrix. The roots of m are eigenvalues and m    divides det  A   I  . Multiplication Ax  x   column     x   column n   combination of columns.  Multiplicities AM and GM The algebraic multiplicity AM of an eigenvalue  is the number of times  appears as a root of det  A   I   . The geometric multiplicity GM is the number of independent eigenvectors   dimension of the eigenspace for  . Multiplier    The pivot row j is multiplied by   and subtracted from row i to eliminate the i  j entry    entry to eliminate   j th pivot. Network A directed graph that has constants c  ... c  associated with the edges. Nilpotent matrix N Some power of N is the zero matrix N   . The only eigenvalue is    repeated n times. Examples triangular matrices with zero diagonal. Norm  A  of a matrix The    norm is the maximum ratio  Ax    x     . Then  Ax  A  x    AB  A  B   and  A  B  A    B  . Frobenius norm  A       a      and   norms are largest column and row sums of  a   . Normal equation A  A  x  A  b Gives the leastsquares solution to Ax  b if A has full rank n . The equation says that  columns of A    b  A  x   . Normal matrix N NN   N  N  leads to orthonormal complex eigenvectors. Nullspace matrix N The columns of N are the n  r special solutions to As  . Nullspace   A  Solutions to Ax  . Dimension n  r   columns  rank. Orthogonal matrix Q Square matrix with orthonormal columns so Q  Q  I implies Q   Q   . Preserves length and angles  Qx    x  and  Qx    Qy   x  y . All      with orthogonal eigenvectors. Examples Rotation reflection permutation. Orthogonal subspaces Every v in V is orthogonal to every w in W . Orthonormal vectors q  ... q  Dot products are q   q    if i   j and q   q   . The matrix Q with these orthonormal columns has Q  Q  I . If m  n  then Q   Q   and q  ... q  is an orthonormal basis for R   every v    v  q   q  . Outer product is uv  column times row  rank matrix. Partial pivoting In elimination the j th pivot is chosen as the largest available entry in absolute value in column j . Then all multipliers have     . Roundoff error is controlled depending on the condition number of A . Particular solution x  Any solution to Ax  b  often x  has free variables  . Pascal matrix P   pascal  n  The symmetric matrix with binomial entries           . P   P  P  all contain Pascals triangle with det   see index for more properties.   Permutation matrix P There are n ! orders of  ... n  the n ! P s have the rows of I in those orders. PA puts the rows of A in the same order. P is a product of row exchanges P   P is even or odd det P   or   based on the number of exchanges. Pivot columns of A Columns that contain pivots after row reduction not combinations of earlier columns. The pivot columns are a basis for the column space. Pivot d The first nonzero entry when a row is used in elimination. Plane or hyperplane in R  Solutions to a  x   give the plane dimension n   perpendicular to a   . Polar decomposition A  QH Orthogonal Q  positive semidefinite H . Positive definite matrix A Symmetric matrix with positive eigenvalues and positive pivots. Definition x  Ax   unless x  . Projection matrix P onto subspace S Projection p  Pb is the closest point to b in S  error e  b  Pb is perpendicular to S . P   P  P   eigenvalues are  or  eigenvectors are in S or S  . If columns of A  basis for S  then P  A  A  A    A  . Projection p  a  a  b  a  a  onto the line through a    P  aa   a  a has rank . Pseudoinverse A  MoorePenrose inverse The n by m matrix that inverts A from column space back to row space with   A      A   . A  A and AA  are the projection matrices onto the row space and column space. rank  A    rank  A  . Random matrix rand  n  or randn  n  MATLAB creates a matrix with random entries uniformly distributed on      for rand  and standard normal distribution for randn . Rank  matrix A  uv        Column and row spaces  lines cu and cv . Rank r  A  Equals number of pivots  dimension of column space  dimension of row space. Rayleigh quotient q  x   x  Ax  x  x For A  A      q  x     . Those extremes are reached at the eigenvectors x for    A  and    A  . Reduced row echelon form R  rref  A  Pivots   zeros above and below pivots r nonzero rows of R give a basis for the row space of A . Reflection matrix Q  I   uu  The unit vector u is reflected to Qu   u . All vectors x in the plane u  x   are unchanged because Qx  x . The Householder matrix has Q   Q    Q . Right inverse A  If A has full row rank m  then A   A   AA     has AA   I  .  Rotation matrix R             rotates the plane by   and R    R  rotates back by   . Orthogonal matrix eigenvalues e   and e     eigenvectors     i  . Ro w picture of Ax  b Each equation gives a plane in R  planes intersect at x . Row space   A   All combinations of rows of A . Column vectors by convention. Saddle point of f  x  ... x   A point where the first derivatives of f are zero and the second derivative matrix    f   x   x   Hessian matrix  is indefinite. Schur complement S  D  CA   B Appears in block elimination on     . Schwarz inequality  v  w  v  w  Then  v  Aw     v  Av  w  Aw  if A  C  C . Semidefinite matrix A Positive semidefinite means symmetric with x  Ax   for all vectors x . Then all eigenvalues    no negative pivots. Similar matrices A and B    B  M   AM has the same eigenvalues as A . Simplex method for linear programming The minimum cost vector x  is found by moving from corner to lowercost corner along the edges of the feasible set where the constraints Ax  b and x   are satisfied. Minimum cost at a corner! Singular matrix A A square matrix that has no inverse det  A   . Singular Value Decomposition SVD A  U  V   orthogonal U  times diagonal   times orthogonal V   First r columns of U and V are orthonormal bases of   A  and   A    with Av     u  and singular value    . Last columns of U and V are orthonormal bases of the nullspaces of A  and A . Sk ewsymmetric matrix K The transpose is  K  since K    K  . Eigenvalues are pure imaginary eigenvectors are orthogonal e  is an orthogonal matrix. Solvable system Ax  b The right side b is in the column space of A . Spanning set v  ... v   for V Every vector in V is a combination of v  ... v  . Special solutions to As      One free variable is s    other free variables  . Spectral theorem A  Q  Q  Real symmetric A has real   and orthonormal q   with Aq     q  . In mechanics the q  give the principal axes . Spectrum of A The set of eigenvalues    ...    . Spectral radius      . Standard basis for R  Columns of n by n identity matrix written i  j  k in R  . Stiffness matrix K When x gives the movements of the nodes in a discrete structure Kx gives the internal forces. Often K  A  CA  where C contains spring constants from Hookes Law and Ax  stretching strains from the movements x .   Subspace S of V Any vector space inside V  including V and Z   zero vector  . Sum V  W of subspaces Space of all  v in V    w in W . Direct sum  dim  V  W   dim V  dim W  when V and W share only the zero vector. Symmetric factorizations A  LDL  and A  Q  Q  The number of positive pivots in D and positive eigenvalues in  is the same. Symmetric matrix A The transpose is A   A  and a   a  . A   is also symmetric. All matrices of the form R  R and LDL  and Q  Q  are symmetric. Symmetric matrices have real eigenvalues in  and orthonormal eigenvectors in Q . Toeplitz matrix T Constantdiagonal matrix so t  depends only on j  i . Toeplitz matrices represent linear timeinvariant filters in signal processing. Trace of A Sum of diagonal entries  sum of eigenvalues of A . Tr AB  Tr BA . Transpose matrix A  Entries A    A  . A  is n by m  A  A is square symmetric positive semidefinite. The transposes of AB and A   are B  A  and  A     . Triangle inequality  u  v  u    v  For matrix norms  A  B  A    B  . Tridiagonal matrix T t    if  i  j   . T   has rank  above and below diagonal. Unitary matrix U   U   U   Orthonormal columns complex analog of Q . Vandermonde matrix V V c  b gives the polynomial p  x   c     c    x    with p  x    b  at n points. V    x       and det V  product of  x   x   for k  i . Vector addition v  w   v   w  ... v   w    diagonal of parallelogram. Vector space V Set of vectors such that all combinations cv  dw remain in V . Eight required rules are given in Section . for cv  dw . Vector v in R  Sequence of n real numbers v   v  ... v    point in R  . Volume of box The rows or columns of A generate a box with volume  det  A   . Wavelets w   t  or vectors w  Rescale and shift the time axis to create w   t   w     t  k  . Vectors from w              would be           and           .    cofactor Compute the n by n matrix of cofactors. cramer Solve the system Ax  b by Cramers Rule. deter Matrix determinant computed from the pivots in PA  LU . eigen Eigenvalues eigenvectors and det  A   I  for  by  matrices. eigsho w Graphical demonstration of eigenvalues and singular values. eigval Eigenvalues and their multiplicity as roots of det  A   I   . eigv ec Compute as many linearly independent eigenvectors as possible. elim Reduction of A to row echelon form R by an invertible E . findpiv Find a pivot for Gaussian elimination used by plu . fourbase Construct bases for all four fundamental subspaces. grams GramSchmidt orthogonalization of the columns of A . house  by  matrix giving corner coordinates of a house. inverse Matrix inverse if it exists by GaussJordan elimination. leftnull Compute a basis for the left nullspace. linefit Plot the least squares fit to m given points by a line. lsq Leastsquares solution to Ax  b from A  A  A  b . normal Eigenvalues and orthonormal eigenvectors when A  A  AA  . nulbasis Matrix of special solutions to Ax   basis for null space. orthcomp Find a basis for the orthogonal complement of a subspace. partic Particular solution of Ax  b  with all free variables zero.   plotd Twodimensional plot for the house figures. plu Rectangular PA  LU factorization with row exchanges. polystr Express a polynomial as a string. project Project a vector b onto the column space of A . projmat Construct the projection matrix onto the column space of A . randperm Construct a random permutation. rowbasis Compute a basis for the row space from the pivot rows of R . samespan Test whether two matrices have the same column space. signperm Determinant of the permutation matrix with rows ordered by p . slu LU factorization of a square matrix using no row exchanges . slv Apply slu to solve the system Ax  b allowing no row exchanges. splu Square PA  LU factorization with row exchanges . splv The solution to a square invertible system Ax  b . symmeig Compute the eigenvalues and eigenvectors of a symmetric matrix. tridiag Construct a tridiagonal matrix with constant diagonals a  b  c . These Teaching Codes are directly available from the Linear Algebra Home Page httpweb.mit.edu.www. They were written in MATLAB  and translated into Maple and Mathematica.           Nonsingular Singular A is invertible. A is not invertible. The columns are independent. The columns are dependent. The rows are independent. The rows are dependent. The determinant is not zero. The determinant is zero. Ax   has one solution x  . Ax   has infinitely many solutions. Ax  b has one solution x  A   b . Ax  b has no solution or infinitely many. A has n nonzero pivots. A has r  n pivots. A has full rank r  n . A has rank r  n . The reduced row echelon form is R  I . R has at least one zero row. The column space is all of R  . The column space has dimension r  n . The row space is all of R  . The row space has dimension r  n . All eigenvalues are nonzero. Zero is an eigenvalue of A . A  A is symmetric positive definite. A  A is only semidefinite. A has n positive singular values. A has r  n singular values. Each line of the singular column can be made quantitative using r .